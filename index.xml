<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>CohleM</title>
    <link>https://cohlem.github.io/</link>
    <description>Recent content on CohleM</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 30 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://cohlem.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>skip-connections</title>
      <link>https://cohlem.github.io/sub-notes/skip-connections/</link>
      <pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/skip-connections/</guid>
      <description>Skip connections are simply skipping the layers by adding the identity of input to it&amp;rsquo;s output as shown in the figure below.
Why add the identity of input x to the output ? We calculate the gradients of parameters using chain rule, as shown in figure above. For deeper layers the gradient start to become close to 0 and the gradient stops propagating, which is a vanishing gradient problem in a deep neural networks.</description>
    </item>
    
    <item>
      <title>Optimization Algorithms (SGD with momentum, RMSProp, Adam)</title>
      <link>https://cohlem.github.io/sub-notes/optimization-algorithms/</link>
      <pubDate>Fri, 27 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/optimization-algorithms/</guid>
      <description>The simplest algorithm is the gradient descent in which we simply calculate loss over all the training data and then update our parameters, but it would be too slow and would consume too much resources. A faster approach is to use SGD where we calculate loss over every single training data and then do the parameter update, but the gradient update could be fuzzy. A more robust approach is to do mini batch SGD.</description>
    </item>
    
    <item>
      <title>manual-backpropagation-on-tensors</title>
      <link>https://cohlem.github.io/sub-notes/manual-backpropagation-on-tensors/</link>
      <pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/manual-backpropagation-on-tensors/</guid>
      <description>Main code n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 64 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) # Layer 1 W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) b1 = torch.randn(n_hidden, generator=g) * 0.1 # using b1 just for fun, it&amp;#39;s useless because of BN # Layer 2 W2 = torch.</description>
    </item>
    
    <item>
      <title>Matrix Visualization</title>
      <link>https://cohlem.github.io/sub-notes/matrix-visualization/</link>
      <pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/matrix-visualization/</guid>
      <description>In deep learning, it&amp;rsquo;s important to visualize a matrix and how it is represented in a dimension space because the operations that we perform on those matrix becomes very much intuitive afterwards.
Visualizing two dimensional matrix. This has to be the most intuitive visualization.
[ [12, 63, 10, 42, 70, 31, 34, 8, 34, 5], [10, 97, 100, 39, 64, 25, 86, 22, 31, 25], [28, 44, 82, 61, 70, 94, 22, 88, 89, 56] ] We can simply imagine rows are some examples and columns as those examples&amp;rsquo; features.</description>
    </item>
    
    <item>
      <title>Diagnostic-tool-while-training-nn</title>
      <link>https://cohlem.github.io/sub-notes/diagnostic-tool-while-training-nn/</link>
      <pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/diagnostic-tool-while-training-nn/</guid>
      <description>source: Building makemore Part 3: Activations &amp;amp; Gradients, BatchNorm
Things to look out for while training NN Take a look at previous notes to understand this note better
consider we have this simple 6 layer NN
# Linear Layer g = torch.Generator().manual_seed(2147483647) # for reproducibility class Layer: def __init__(self,fan_in, fan_out, bias=False): self.w = torch.randn((fan_in, fan_out),generator = g) / (fan_in)**(0.5) # applying kaiming init self.bias = bias if bias: self.b = torch.</description>
    </item>
    
    <item>
      <title>BatchNormalization</title>
      <link>https://cohlem.github.io/sub-notes/batchnormalization/</link>
      <pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/batchnormalization/</guid>
      <description>As we saw in our previous note how important it is to have the pre-activation values to be roughly gaussian (0 mean, and unit std). We saw how we can initialize our weights that make our pre-activation roughly gaussian by using Kaiming init. But, how do we always maintain our pre-activations to be roughly gaussian?
Answer: BatchNormalization
Benefits
stable training preserves vanishing gradients BatchNormalization As the name suggests, batches are normalized (across batches), by normalizing across batches we preserve the gaussian property of our pre-activations.</description>
    </item>
    
    <item>
      <title>Maximum likelihood estimate as loss function</title>
      <link>https://cohlem.github.io/sub-notes/maximum-likelihood-estimate-as-loss/</link>
      <pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/maximum-likelihood-estimate-as-loss/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>Backpropagation from scratch</title>
      <link>https://cohlem.github.io/sub-notes/backpropagation-from-scratch/</link>
      <pubDate>Sun, 08 Dec 2024 21:57:23 +0545</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/backpropagation-from-scratch/</guid>
      <description>Source: The spelled-out intro to neural networks and backpropagation: building micrograd
Backpropagation on paper It implements backpropagation for two arithmetic operation (multiplication and addition) which are quite straightforward.
Implementation is for this equation.
a = Value(2.0, label=&amp;#39;a&amp;#39;) b = Value(-3.0, label=&amp;#39;b&amp;#39;) c = Value(10.0, label=&amp;#39;c&amp;#39;) e = a*b; e.label = &amp;#39;e&amp;#39; d = e + c; d.label = &amp;#39;d&amp;#39; f = Value(-2.0, label=&amp;#39;f&amp;#39;) L = d * f; L.label = &amp;#39;L&amp;#39; L The most important thing to note here is the gradient accumulation step (shown at the bottom-left).</description>
    </item>
    
    <item>
      <title>Deep Learning Notes</title>
      <link>https://cohlem.github.io/notes/deep-learning-notes/</link>
      <pubDate>Sun, 08 Dec 2024 21:57:23 +0545</pubDate>
      
      <guid>https://cohlem.github.io/notes/deep-learning-notes/</guid>
      <description>Backpropagation Backpropagation on scalars from scratch Manual Backpropagation on tensor Loss function Maximum likelihood estimate as loss function Why we add regularization to loss functio√± Optimization Optimization Algorithms (SGD with momentum, RMSProp, Adam) Optimizing loss with weight initialization BatchNormalization Diagnostic tool to look out for while training NN Skip Connections Misc Matrix Visualization </description>
    </item>
    
    <item>
      <title>Why we add regularization in loss function</title>
      <link>https://cohlem.github.io/sub-notes/why-we-need-regularization/</link>
      <pubDate>Sun, 08 Dec 2024 21:57:23 +0545</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/why-we-need-regularization/</guid>
      <description>it penalizes the weights, and prioritizes uniformity in weights. How does it penalize the weights? Now when we do the backprop and gradient descent.
The gradient of loss w.r.t some weights become as we can see it penalizes the weight by reducing the weights&amp;rsquo;s value by some higher amount compared to the some minimial weight update when we only used loss function.
So overall, the model tries to balance the Loss (L) as well as keep the weights small.</description>
    </item>
    
    <item>
      <title>Backpropagation from scratch</title>
      <link>https://cohlem.github.io/posts/backpropogation/</link>
      <pubDate>Sat, 12 Aug 2023 21:57:23 +0545</pubDate>
      
      <guid>https://cohlem.github.io/posts/backpropogation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Using Genetic Algorithm for Weights Optimization</title>
      <link>https://cohlem.github.io/posts/using-genetic-algorithm-for-weights-optimization/</link>
      <pubDate>Sat, 12 Aug 2023 21:57:23 +0545</pubDate>
      
      <guid>https://cohlem.github.io/posts/using-genetic-algorithm-for-weights-optimization/</guid>
      <description>BackStory This is a simple fun little project that I did almost a year ago, At that time I used to see a lot of CodeBullet&amp;rsquo;s videos and wanted to learn the gist behind these evolutionary algorithms, and I can&amp;rsquo;t get into my head if I don&amp;rsquo;t do it from scratch so I wanted to implement this project from scratch. At that time, I wanted to document the learning and the implementation process, I even thought of making a youtube video about this topic but could not complete it, at that time, I had made some video animations about this process but could not complete it because I started doing something else and when I can back it was already too much mess and could not complete the video animations, but I&amp;rsquo;ll include the video animations that I had made earlier.</description>
    </item>
    
    <item>
      <title>Fine-tuning LLM vs In-context learning</title>
      <link>https://cohlem.github.io/posts/llmvsincontext/</link>
      <pubDate>Sat, 22 Jul 2023 20:40:19 +0545</pubDate>
      
      <guid>https://cohlem.github.io/posts/llmvsincontext/</guid>
      <description>This is my experience and experimentation that I did while building a product for the use case of using LLMs for our own data for question answering. If you&amp;rsquo;re doing something similar, this could be of some help.
The most commonly used methods while using LLMs with our own data were typically
Fine-tuning the model with your own data Using Retrieval Augmented Generation (RAG) techniques Fine-tuning the model with your own data This is the initial method and follows the general structure of training a model</description>
    </item>
    
    
    <item>
      <title>optimizing-loss-with-weight-initialization</title>
      <link>https://cohlem.github.io/sub-notes/optimizing-loss/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/optimizing-loss/</guid>
      <description>Problem Consider a simple MLP that takes in combined 3 character embeddings as an input and we predicts a new character.
# A simple MLP n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 200 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) b1 = torch.</description>
    </item>
    
    
    <item>
      <title>TITLE</title>
      <link>https://cohlem.github.io/sub-notes/template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/template/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
