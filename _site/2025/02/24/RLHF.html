<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Rlhf - </title>
    <link rel="stylesheet" href="/assets/css/main.css" />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Zalando+Sans+Expanded:ital,wght@0,200..900;1,200..900&family=Zalando+Sans:ital,wght@0,200..900;1,200..900&display=swap"
      rel="stylesheet"
    />

    <!-- MathJax for LaTeX -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
          displayMath: [
            ["$$", "$$"],
            ["\\[", "\\]"],
          ],
          processEscapes: true,
          processEnvironments: true,
        },
        options: {
          skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"],
        },
      };
    </script>
    <script
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
      id="MathJax-script"
      async
    ></script>
  </head>
  <body>
    <nav>
      <a href="/">Home</a>
      <a href="/posts">Posts</a>
      <a href="/notes">Notes</a>
      <a href="/blogs">Blogs</a>
    </nav>

    <main><h1>Rlhf</h1>
<p>24 Feb 2025 - cohlem</p>

<p>Before starting, it’s advisable to first complete <a href="https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-">David Silver’s Course on RL</a> and read <a href="https://lilianweng.github.io/posts/2018-02-19-rl-overview/#deadly-triad-issue">Lilian’s notes on RL</a> which explains/provides notes on the David’s course in sequential manner.</p>

<p>In simple problems, we simply start with an arbitrary value function, and then go on updating that value function incrementally, using different algorithms such as Monte Carlo (which collects reward over the whole episoe), Temporal difference, aka TD(0) (which considers bootstrapping, i.e only considering the immediate reward and then approximating other remaining rewards with the help of value function $r + V(s)$ ) and other algorithms. But, over time maintaining a table is not feasible for real world problems, so move towards approximating these value functions with the help of a vector of features or more concretely with Neural Networks but approximating these state and action value functions are infeasible too which is explained in the next section.</p>

<h3 id="why-we-choose-policy-approximation-instead-of-stateaction-value-approximation">Why we choose policy approximation instead of state/action value approximation.</h3>

<p>In real work tasks, the action space is not discrete, i.e up, down, right, left instead it’s continuous for eg. A robot can do infinite tasks. Deep Q Network works by taking the maximum over the action, and minimizing the error over that action. BUT, approximating every action becomes expensive task.</p>

<p>\(Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma \max_{a \in \mathcal{A}} Q(S_{t+1}, a) - Q(S_t, A_t))\)
So we move on to approximating our policy, where the approximated policy will provide the probability over the considerable actions, allowing the agent to <strong>sample actions</strong> rather than search for the best one.</p>

<h2 id="policy-gradient-algorithms">Policy Gradient Algorithms</h2>

<p>Proof of policy gradient theorem.</p>

<p>First we collect trajectory $\tau$, which is a collection of states and actions. $\tau = s_0, a_0, s_1, a_1, \ldots, s_H, a_H, s_{H+1}$</p>

<p>The reward for this trajectory is simply the sum of all the rewards from each timesteps.</p>

\[R(\tau) = \sum_{t=0}^{H-1} R(s_t, a_t)\]

<p>Our overall objective is to construct a policy that gives us the highest reward for our trajectory. We construct a function $J(\theta)$ that is parametrized by this policy, that calculates the total expected reward and we want to maximize this expectation.</p>

\[J(\theta) = E[R(\tau); \theta] \tag{1}\]

\[\max_\theta J(\theta) = \max_\theta E\left[\sum_{t=0}^{H-1} R(s_t, a_t)\right]\]

<p>If we open this objective function it becomes.</p>

\[J(\theta) = \sum_\tau P(\tau) \cdot R(\tau)\]

<p>where $P(\tau)$ is the probability of trajectory $\tau$, and $R(\tau)$ is the total reward for that trajectory. and Lets take the derivative of this $J(\theta)$</p>

\[\nabla_\theta J(\theta) = \sum_\tau \frac{P(\tau)}{P(\tau)} \nabla_\theta P(\tau) \sum_{t=0}^{H-1} R(s_t, a_t)\]

\[\nabla_\theta J(\theta) = \sum_\tau P(\tau) \cdot \nabla_\theta \log P(\tau) \cdot \sum_{t=0}^H R(s_t, a_t) \quad \text{(using } \frac{\partial \log P(\tau)}{\partial \theta} = \frac{1}{P(\tau)} \partial_\theta P(\tau) \text{)}\]

<p>now representing it back into the expectation form we get,</p>

\[\nabla_\theta J(\theta)
= E\left[  \nabla_\theta \log P(\tau) \cdot \sum_{t=0}^{H-1} R(s_t, a_t) \right] \tag{2}\]

<p>Now what is $P(\tau)$, it’s the joint probability for that trajectory, which is given by the chain rule of probability,</p>

\[P(\tau) = P(s_0) \cdot P(a_0|s_0) \cdot P(s_1|a_0, s_0) \cdot P(a_1|s_1) \cdot P(s_2|a_1, s_1) \cdots\]

<p>In the equation above, we assume Markov’s property that next action only depends on the current state.</p>

\[P(a_t|s_t, s_{t-1}) = P(a_t|s_t)\]

\[P(s_{t+1}|a_t, s_t, a_{t-1}, s_{t-1}) = P(s_{t+1}|a_t, s_t)\]

<p>It boils down to this equation below.</p>

\[P(\tau) = P(s_0) \prod_{t=0}^H P(a_t|s_t) \cdot P(s_{t+1}|a_t, s_t)\]

<p>and then to this</p>

\[P(\tau) = P(s_0) \prod_{t=0}^H \pi(a_t|s_t) \cdot P(s_{t+1}|a_t, s_t)\]

<table>
  <tbody>
    <tr>
      <td>because action distribution $\pi(a_t\ s_t)$ is governed by the policy $\theta$ whereas the transition probability $P(s_{t+1}</td>
      <td>a_t, s_t)$ is governed by the environment. Let’s take the log of that $P(\tau)$</td>
    </tr>
  </tbody>
</table>

\[\log P(\tau) = \log(P(s_0)) + \sum_{t=0}^{H-1} \log \pi(a_t|s_t) + \sum_{t=0}^{H-1}\log P(s_{t+1}|a_t, s_t)\]

<p>After taking the derivative of that $\log P(\tau)$, we end up with this equation below, because all the other terms down depend on our policy $\theta$</p>

\[\nabla_\theta \log P(\tau) = \sum_{t=1}^H \nabla_\theta \log \pi(a_t|s_t)\]

<p>Let’s replace the equation above in our equation 2. we get.</p>

\[\nabla_\theta J(\theta) = \frac{1}{M} \sum_{m=1}^{M} \left( \sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m |s_t^m) \cdot \left( \sum_{t=0}^{H-1} R(s_k^m, a_k^m)  \right) \right) \tag{3}\]

<h4 id="reducing-variance">Reducing Variance</h4>

<p>BUT, there’s a problem withe equation (3), it is high variance, because in stochastic environment R can sometimes take high value, and some times low. Since R acts as a magnitude term in the equation (3) it would make the gradient estimation and update unstable because of the high variance, to stabilize it we subtract a baseline from the the return in equation (3).</p>

\[\nabla_\theta J(\theta) = \frac{1}{M} \sum_{m=1}^{M} \left( \sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m |s_t^m)  \cdot \left( \sum_{t=0}^{H-1} R(s_k^m, a_k^m)  - b\right) \right) \tag{3}\]

<p>subtracting b from the trajectory reward doesn’t increase it’s bias and there is a proof of that which I’m not going to explain in this section, it helps us by decreasing the variance. If we’re able to keep the bias stable and variance low, it would be a win win situation and this is what we do here.</p>

<p>What should the b value be?</p>

<p>It would be better if we have that baseline as an average return that we might expect when we are in state t, and that is simply our value function $V^\pi(s)$.</p>

\[\nabla_\theta J(\theta) = \frac{1}{M} \sum_{m=1}^{M} \left( \sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m |s_t^m)  \cdot \left( \sum_{t=0}^{H-1} R(s_k^m, a_k^m)  - V^\pi(s_k^m)\right) \right) \tag{4}\]

<p>Equation (4) tell us we are increasing the logprobs of actions that gives us the return that is better than the average.</p>

<p>There still one way we can reduce further variance from equation (4), we can break down equation (4) into the equation below.</p>

\[\nabla_\theta J(\theta) = \frac{1}{M} \sum_{m=1}^{M} \left( \sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m |s_t^m)  \cdot \left( \sum_{t=0}^{t-1}R(s_k^m, a_k^m) + \sum_{k=t}^{H-1} R(s_k^m, a_k^m)  - V^\pi(s_k^m)\right) \right) \tag{5}\]

\[\nabla_\theta J(\theta) = \frac{1}{M} \sum_{m=1}^{M} \left( \sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m |s_t^m)  \cdot \left(  \sum_{k=t}^{H-1} R(s_k^m, a_k^m)  - V^\pi(s_k^m)\right) \right) \tag{6}\]

<p>we can remove this term $\sum_{t=0}^{t-1}R(s_k^m, a_k^m)$, because why would the rewards at time step 4 affect the logprobs of action at time step 7. We only care about the FUTURE rewards not the past rewards, we are essentially deleting the past rewards from there. For concrete understanding, let’s say we have H = 3, M=1</p>

<p>t=0
$\nabla_\theta \log(a_0|s_0) = 0.5$
$R(s_0,a_0) = 10$</p>

<p>t=1
$\nabla_\theta \log(a_1|s_1) = 0.4$
$R(s_1,a_1) = 4$</p>

<p>t=2
$\nabla_\theta \log(a_2|s_2) = 0.2$
$R(s_2,a_2) = 7$</p>

<p>So,</p>

<table>
  <tbody>
    <tr>
      <td>$\sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m</td>
      <td>s_t^m) = (0.5 + 0.4 + 0.2)$</td>
    </tr>
  </tbody>
</table>

<p>$\sum_{t=0}^{H-1} R(s_k^m, a_k^m) = (10 + 4 + 7)$</p>

<p>Let’s add those values in the equation 4.</p>

<p>$(0.5 + 0.4 + 0.2) \cdot (10 + 4 + 7) = (0.5<em>(10 + 4 + 7) + 0.4</em> (10 + 4 + 7) + 0.2 * (10 + 4 + 7))$</p>

<p>as you can see we are including rewards at past timestep for future actions too (i.e 0.4 is the logprobs for timestep 1 and it is multiplied with (10 + 4 + 7), where 10 is the reward from the past timestep 0, we don’t want that) so, if we use the equation (6), we end up with something like this</p>

<p>$(0.5<em>(10 + 4 + 7) + 0.4</em> (4 + 7) + 0.2 * (7))$</p>

<p>We only consider the future rewards, not the past ones.</p>

<h4 id="how-to-approximate-value-function">How to approximate value function?</h4>

<p>We can approximate value function $V^\pi$ as taught in this <a href="https://www.youtube.com/watch?v=UoPei5o4fps&amp;list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&amp;index=6">lecture</a> we can simply sample from the trajectory and then minimize the squared error i.e</p>

<p>let’s sample $M$ trajectories, $\tau_1, \tau_2, … \tau_M$ and then minimize this squared error</p>

\[\frac{1}{M} \sum_{m=1}^{M} \sum_{t=0}^{H-1} \left( V^\pi(s_t^m) - \sum_{k=t}^{H-1} R(s_k^m, a_k^m) \right)^2\]

<p>There’s one more thing we can do, we can approximate this term $\sum_{k=t}^{H-1} R(s_k^m, a_k^m)$ with our Q value function which would further reduce the variance.</p>

<p>Since $Q^\pi= E[r_0 + \gamma r_1 + \gamma^2 r_2 …]$
adding gamma would reduce the variance further, and approximating this $Q^\pi$ with the help of value function would further reduce the variance.</p>

\[\nabla_\theta J(\theta) = \frac{1}{M} \sum_{m=1}^{M}  \sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m |s_t^m)  \cdot \left( Q^\pi (s_t^m, a_t^m)  - V^\pi(s_t^m)\right) ) \tag{7}\]

\[\nabla_\theta J(\theta) = \frac{1}{M} \sum_{m=1}^{M}  \sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m |s_t^m)  \cdot A^\pi(s_t, a_t) ) \tag{8}\]

<p>We can approximate this using bootstrapping, $A^\pi(s,a)$ in different ways such as.</p>

\[A^\pi(s_t,a_t)= r_0 + V^\pi (s_{t+1}) - V^\pi(s_t))\]

<p>Or more compact form called Generalized Advantaged Estimation (GAE)</p>

\[\delta_t = r_t + \gamma V(s_{t+1} - V(s_t))\]

\[A_t(s_t, a_t) = \sum_{l=0}^\infty \gamma \lambda \delta_{t + l}\]

<h3 id="general-rl-setting">General RL setting</h3>

\[J(\pi) =
  \mathbb{E}\_{\tau \sim \pi} \left[ \sum\_{t=0}^{\infty} \gamma^t r(s\_t,
  a\_t) \right],\qquad{(1)}\]

<p>In RL setting we aim to optimize the objective function $J(\pi)$ by updating the policy $\pi$
given a reward function $r(s,a)$ that takes in a state and the action performed at that state. The next action is determined by $\pi(a|s)$. We find the expected(average) reward over all the trajectories $\tau$. $\gamma$ is the discount factor from 0 to 1 that balances the desirability of of near vs future-rewards.</p>

<p>An example: If our RL setup is confined to finding a maze, a reward function could be $r(s,a)=-\text{distance to goal}$</p>

<h3 id="rlhf">RLHF</h3>

<p>RLHF reduces the equation 1 by removing this discounted factor</p>

\[J(\pi) =
  \mathbb{E}\_{\tau \sim \pi} \left[ \sum\_{t=0}^{\infty} r(s\_t,
  a\_t) \right],\qquad{(2)}\]

<p>We aim to maximize our objective function by optimizing the policy. The reward function is designed such that the actions must align with the human preferences.</p>

<p>The most common reward model predicts the probability that a piece of text was close to a “preferred” piece of text from the training comparisons.</p>

<h3 id="reward-models">Reward Models</h3>

<p>Given two prompts $y1$ and $y2$ we want to have a reward model that gives high score to $y1$ and low score to $y2$ meaning $y1$ is always preferred over $y2$. Their relative preference is given by the Bradly Terry model.</p>

\[P(i &gt; j) =
  \frac{p\_i}{p\_i + p\_j}\qquad{(3)}\]

<p>It gives the probability of i being preferred over j where $p_{i}$ and $p_{j}$ represent “strengths” for those prompts.</p>

<p>We want our model to maximize this probability because later $i$ would represent the text we want (aligned) and $j$ would represent the text we don’t want (not aligned). The training objective can be derived from the equation above. The “strengths” are exponential because we want them to be strictly positive.</p>

\[P(y\_1 &gt;
  y\_2) = \frac{\exp(r(y\_1))}{\exp(r(y\_1)) +
  \exp(r(y\_2))}\qquad{(4)}\]

<p>The loss function becomes</p>

\[\mathcal{L}(\theta) = - \log \left( \sigma
  \left( r\_{\theta}(x, y\_w) - r\_{\theta}(x, y\_l) \right)
  \right)\qquad{(6)}\]

<p>Our existing model can be configured to output just one value by adding a linear head to the model.</p>

<p>In case of language models, we want a model to rate our answer i.e (give a score based on how good or bad it is), it is the most important part because it will guide our training, it is providing supervision for our PPO algorithm.</p>

<h4 id="steps-for-training-a-reward-model">Steps for training a reward model</h4>

<p>Collect pairs of data, it could be either contrasting pairs or some pairs of prompt with priority. Eg. We want our llm to be trained (later using PPO) to be to generate positive response. In this case our priority prompt would be positive prompt and non-priority prompt would be negative prompt.</p>

<p>We take a language model and add a linear head to it. For instance, for each token a LM outputs 512 dimension vector we add a new head that takes in 512 dimension and outputs a one dimensional vector which gives us the reward.</p>

<p>The loss function for the reward model is constructed likewise, which is same the equation 6.</p>

\[L = -log(\sigma(r1 - r2))\]

<p>where $r1$ is reward for priority prompt and $r2$ is reward for non-priority prompt. We want to maximize this difference $r1 - r2$ and minimize the this function. $\sigma$ represents sigmoid function.</p>

<h3 id="notes">NOTES</h3>

<ul>
  <li>we calculate the reward for the ending token which represents a reward given to the whole token.</li>
  <li>reward models overfit fast, so we can consider smaller LM for reward model training.</li>
</ul>

<p>——-<strong>Skipping other reward models for now</strong>——-</p>

<h3 id="regularization">Regularization</h3>

<p>We want aligned reward models but would still “not go off the rails” meaning it stays within the limitation of our reference model. It allows the policy being trained to stay close to the reference policy.</p>

\[r = r\_\theta -
  \lambda r\_{\text{reg.}} \qquad{(1)}\]

\[r = r\_\theta - \lambda\_{\text{KL}} \mathcal{D}\_{\text{KL}} \left(
  \pi^{\text{RL}}(y \mid x) \, \| \, \pi^{\text{Ref.}}(y \mid x) \right)
  \qquad{(2)}\]

<p>KL divergence is calculated as</p>

\[D\_{\text{KL}}(P \,||\, Q) = \mathbb{E}\_{x \sim P} \left[ \log P(x) -
  \log Q(x) \right].
  \qquad{(3)}\]

<h3 id="rejection-sampling">Rejection Sampling</h3>

<p>This process is kind of a filtering process. We first sample outputs from our base language model that we’ll be training next. For example, we generate 5 example outputs for a sample prompt. We then provide a score to each of the generated examples using our reward function. We sort and only take the top-K elements per prompt and fine-tune our base model on these examples. So it’s kind of like a filtering and only keeping the highly rewarded examples.</p>

<h3 id="ppo">PPO</h3>

<h4 id="ppo-algorithm-analogy">PPO algorithm analogy</h4>

<p>Suppose we are taking an exam paper. Our objective is to maximize the change of getting good marks by updating our brain (weights and biases).</p>

<p>$s_t$ is a question that we are looking at and trying to solve. $a_t$ is the our answer for that question. $r_t$ is the immediate reward we get after writing $a_t$, $s_{t+1}$ is the next-question.</p>

<p>$R_t$ is the actual total exam score.</p>

<p>Suppose there is an imaginary machine that gives us the expected exam score that we can get just by looking at our question which is $V(s)$.</p>

<p>$A(s)$ is our advantage i.e how good we are compared to the predicted score.</p>

<p>$A(s) = R_t - V(s)$</p>

<p>this can be modified as as $\delta_t = R_t - V(s)$ and $A_t = \delta_t + \lambda\cdot\gamma\cdot A_{t+1}$. This is just a modified version of advantage to remove the bias and variance.</p>

<p>This can be a little confusing as we might have no idea what our actual total score($R_t$) will be while we are still writing some questions $s$ so approximate this $R_t$ with the help of current reward $r$ and future reward that we might get from next question i.e $V(s+1)$ i.e</p>

<p>this becomes</p>

<p>$\delta_t =  r_t + V(s+1) - V(s)$</p>

<p>so this will still give us our advantage at a point t.</p>

<p>i.e how good/bad we did at point t= immediate reward(marks) after writing answer to t + expected future reward from new question - expected future reward from previous question t.</p>

<p>This will give us our advantage.</p>

<p>PPO is done using two phases.</p>

<ol>
  <li>Rollout phase</li>
  <li>Weight update phase.</li>
</ol>

<h4 id="rollout-phase">Rollout phase</h4>

<p>We write a lot of exam paper in this phase in parallel.
for each exam paper and for each question in the exam paper we calculate $r_t$, $V(s_t)$ and $R_t$ $A_t$ and use it in our equation.</p>

\[L^{PPO}(\theta) = \mathbb{E}\_t \left[
\min \left( r\_t(\theta) \hat{A}\_t, \text{clip}\left(r\_t(\theta), 1 - \epsilon,1+\epsilon\right) \hat{A}\_t \right) - c\_1 \left( V\_\theta(s\_t) -V\_t^\text{target} \right)^2 + c\_2 \mathcal{H}\left[\pi\_\theta\right(s\_t)\right]\]

\[r\_t(\theta) = \frac{\pi\_\theta(a\_t | s\_t)}{\pi\_{\theta\_\text{old}}(a\_t|s\_t)}\]

<h4 id="weight-update-phase">Weight update phase</h4>

<p>We find this loss and try to maximize this clipped loss and entropy loss, but minimize the value function loss.</p>

<p>The PPO clipped surrogate objective is given as:</p>

\[L^{\text{CLIP}}(\theta) = \mathbb{E}\_t \Big[ \min \big( r\_t(\theta) \hat{A}\_t, \; \mathrm{clip}(r\_t(\theta), 1-\varepsilon, 1+\varepsilon) \hat{A}\_t \big) \Big]\]

<p>The gradient ascent update rule is:</p>

\[\theta \gets \theta + \alpha \nabla\_\theta L^{\text{CLIP}}(\theta)\]

<p>The most important part to understand here is</p>

<p>$r_t(\theta) \hat{A}_t$</p>

<p>so when this term gets clipped to either $1 + \varepsilon$ or $1-\varepsilon$ the gradient of this loss $\nabla_{\theta}r_t(\theta) \hat{A}_t$ is 0. So no update to the weights. But when this gradient is $\nabla_{\theta}r_t(\theta) \hat{A}_t$ the update depends on whether $\hat{A}_t$ is &gt;0 or &lt;0,</p>

<p>If $\hat{A}_t &gt; 0$, the gradient update will be in the direction that increases ${\pi_{\theta}}$.
If $\hat{A}_t &gt; 0$, the gradient update will be in the direction that decreases ${\pi_{\theta}}$.</p>

<h3 id="integrating-reward-model-to-ppo">Integrating reward model to PPO</h3>

<p>As you might have noticed, the PPO loss comes from the token level, meaning we need loprobs, value, reward, return and advantage for each token. Buuuuut, this reward function we just trained is trained to output only one reward for last token so how does that work?</p>

<p>Answer: the reward is propagated.</p>

<p>The advantage is calculated reverse recursively i.e advantage at token n is also passed to the n-1 token. This means that we are looking ahead and telling our token n-1 that we already ended up a good state because of the state we are in. Lets look at this from the lens of advantage formula.</p>

<p>$\delta_t = R_t - V(s)$ and $A_t = \delta_t + \lambda\cdot\gamma\cdot A_{t+1}$</p>

<p>Lets consider we are at 100th token which is the ending token of our sequence</p>

<p>\(R\_t = r\_t + V\_{t+1}(s) - V\_t(s)\)
reward model assigns $r_{100}$ 100 and lets ignore both the value function assuming they cancel out as we are already at the end.</p>

\[\begin{gather}
R\_t= 100\\\\A\_{100}=100, considering A\_{101}=0
\end{gather}\]

<p>At 100th token we are at an advantage, now lets calculate $A_{99}$</p>

\[A\_{99} = \delta\_{99} + \lambda\cdot\gamma\cdot A\_{100}\]

<p>as you can see the reward 100 is propagated to the 99th token, considering $\delta_{99}$ is positive here, it tells us that token 99 is still at an advantage because the we can already see the future here i.e 100th token which was at an advantage, so taking action 99 is still an advantage to us.</p>

<h3 id="reward-hacking">Reward Hacking</h3>

<p>our models can find a loophole to maximize their return by generating high reward tokens but no-so-good answer. Example: If we are trying to make our model positive, model may find a way to output tokens such as “thank you” and add it our answer, which will provide a high reward to it, but it is meaningless to us. So we don’t want our new (trained via PPO) model to deviate significantly from the model that we started from (SFT model), so we add KL divergence as a penalty to our each token’s reward.</p>

<p>As described earlier, reward is provided to only the ending token, and the reward for other token becomes this KL penalty.</p>

<p>i.e if we have token_1, token_2, and token_3</p>

<p>r(token_3) = some reward from the reward model
r(token_2) = KL penalty i.e (logprobs_for_token_2_from_model_being_trained - logprobs_for_token_2_from_SFT_model)* -KL_penalty_coefficient.
and so on…</p>

<p>Some key points</p>

<ul>
  <li>Human preferences that are used to train LLMs are multi-dimensional but the reward is a single score. LLMs being complex and “intelligent” will always find a way to exploit these rewards thus the reward hacking, so in large scale RLHF, reward models get saturated very fast, and we might need to train a new reward model.</li>
</ul>

<h3 id="grpo">GRPO</h3>

<p>The per token loss for GRPO is likewise..</p>

<p>Key difference between GRPO and PPO.</p>

<p>GRPO completely removes values function.
as value function is removed the advantage calculate is simplified.</p>

\[A\_i = \frac{r\_i - \text{mean}(\{r\_1, r\_2, \cdots, r\_G\})}{\text{std}(\{r\_1,r\_2, \cdots,r\_G\})}\]

<p>For each question/prompt different G samples are generated, and for each advantage for each question the reward is normalized to form the advantage.</p>

<p>Previously, in PPO we added KL penalty to the rewards themselves, but in GRPO we add it to the loss function directly.</p>

\[J(\theta) =
  \frac{1}{G}\sum\_{i=1}^G  \frac{1}{|a\_i|} \sum\_{t=1}^{|a\_i|} \left(
  \min\left(\frac{\pi\_\theta(a\_{i,t}|s\_{i,t})}{\pi\_{\theta\_{old}}(a\_{i,t}|s\_{i,t})}A\_{i,t},
  \text{clip} \left(
  \frac{\pi\_\theta(a\_{i,t}|s\_{i,t})}{\pi\_{\theta\_{old}}(a\_{i,t}|s\_{i,t})},
  1-\varepsilon, 1+\varepsilon \right) A\_{i,t} \right) - \beta
  D\_{KL}(\pi\_\theta(\cdot|s\_{i,t})||\pi\_{ref}(\cdot|s\_{i,t}))
  \right)\]

<h2 id="observations-from-dapo-paper">Observations from DAPO paper</h2>

<h3 id="decoupled-clipping-parameter">Decoupled Clipping parameter</h3>

<p>The same $\epsilon$ using in clipping doesn’t favor exploration token i.e tokens with less probability ex. 0.02. After some steps, the entropy of the model starts to dampen, i.e model is becoming less explorative, to restrict this entropy collapse, DAPO paper recommends using a high $\epsilon$ for (1 + $\epsilon$) and same $\epsilon=0.2$ for (1-$\epsilon$).</p>

<h3 id="increase-sample-output-variations">Increase sample output variations</h3>

<p>when the accuracy for all the samples in G have same accuracy 1. The Advantage becomes 0, and the gradient becomes 0 as well. Also, the number of samples with accuracy=1 increases as we train further which decreases the effective prompts in the batch which can lead to larger variance in gradients. The direction for the gradient update is not accurate since the effective prompts in batch is small because of which the gradient updates are noisy.</p>

<p>The proposed method is to oversample and filter out prompts with accuracy 0 or 1</p>

<h3 id="per-token-loss">Per token loss</h3>

<p>In GRPO, loss is first averaged over token and then averaged over all samples G. In doing so, there remains no difference between samples with very large/small sequence length i.e each sample will get averaged loss and tokens in long sequence will get less priority which could be a problem for use if that token is an important token in our long sequence.</p>

<p>DAPO solves this by first summing the loss for all tokens for all samples and then averaging it over all tokens.</p>

\[\mathcal{J}\_{\text{DAPO}}(\theta) = \mathbb{E}\_{(q,a) \sim \mathcal{D}, \{o\_i\}\_{i=1}^G \sim \pi\_{\theta\_{old}}(\cdot|q)} \left[
\frac{1}{\sum\_{i=1}^G |o\_i|} \sum\_{i=1}^G \sum\_{t=1}^{|o\_i|}
\min \left(
r\_{i,t}(\theta) \hat{A}\_{i,t}, \;
\text{clip}\left(
r\_{i,t}(\theta), 1 - \epsilon\_{\text{low}}, 1 + \epsilon\_{\text{high}}
\right) \hat{A}\_{i,t}
\right)
\right]\]

<h3 id="overlong-reward-shaping">Overlong Reward Shaping</h3>

<p>Instead of adding penalty for overly long outputs, just mask the truncated samples. This way the training is stable and enhances performances. Why do this ? adding penalty may confuse when the long reasoning is a valid/good reasoning trace.</p>

<h3 id="multi-turn-rl">Multi-turn RL</h3>

<p>llm and user interact over a period of time.</p>

<h4 id="problems">Problems</h4>

<p>(BENCHMARK)
no benchmark to test multi-turn interactions, and is not reasoning intensive(meaning they only focus on narrow domains, require max engineering overhead)</p>

<p>(RL ALGORITHMS)
PPO, DPO they often suffer from high variance when the horizon gets longer, resulting in poor performance.</p>

<h3 id="verl-algorithm">veRL algorithm</h3>

<p>I forget what these variables that we provide in .sh file while training, so I wrote this boilerplate to remember how these variables play out in the veRL implementation.</p>

<p>rollout code: https://github.com/volcengine/verl/blob/332c7d53c131869690578c0ab84f1e016ed4d817/verl/trainer/ppo/ray_trainer.py#L1081
update code: https://github.com/volcengine/verl/blob/332c7d53c131869690578c0ab84f1e016ed4d817/recipe/sppo/dp_actor.py#L60</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Hyperparameters:
# - train_batch_size = Total experiences per PPO iteration (rollout buffer size)
# - ppo_mini_batch_size = Batch size for each gradient step (subdivided from train_batch_size)
# - ppo_micro_batch_size_per_gpu = Per-GPU batch size (for multi-GPU training)
# - ppo_epochs = Number of passes over the rollout data
</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">total_epoch</span><span class="p">:</span>
	<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">complete_train_data</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">train_batch_size</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span> <span class="c1"># each batch size is provided by train_batch_size
</span>		<span class="nf">generate_rollout</span><span class="p">()</span> <span class="c1"># if GRPO use actor.rollout.n variable
</span>		<span class="nf">generate_old_logprobs</span><span class="p">()</span>
		<span class="nf">generate_ref_logprobs</span><span class="p">()</span>
		<span class="nf">calculate_advantages</span><span class="p">()</span>

		<span class="c1"># split batch into mini_batches.
</span>		<span class="n">minibatch_dataloader</span> <span class="o">=</span> <span class="n">batch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">ppo_mini_batch_size</span><span class="p">)</span> <span class="c1"># this is a dataloader with each minibatch of size ppo_mini_batch_size
</span>		<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">ppo_epoch</span><span class="p">:</span>
			<span class="k">for</span> <span class="n">minibatch</span> <span class="ow">in</span> <span class="n">minibatch_dataloader</span><span class="p">:</span>
				<span class="c1">#split minibatch into microbatches if needed to train on different GPUs
</span>				<span class="n">micro_batches</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">ppo_micro_batch_size_per_gpu</span><span class="p">)</span>
				<span class="n">gradient_accumulation</span> <span class="o">=</span> <span class="n">ppo_mini_batch_size</span> <span class="o">//</span> <span class="n">ppo_micro_batch_size_per_gpu</span>
				<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">micro_batches</span><span class="p">:</span>
					<span class="nf">generate_logprobs</span><span class="p">()</span>
					<span class="n">loss</span> <span class="o">=</span> <span class="nf">calculate_ppo_loss</span><span class="p">()</span> <span class="o">/</span> <span class="n">gradient_accumulation</span>
					<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
				<span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>


</code></pre></div></div>

<p>gradient_accumulation step is not used in a sense that we generally do while pretraining, it just maintains the count total number of micro batches that are processed in separate GPU, by dividing loss by gradient_accumulation we obtain loss as if the minibatch was processed directly without using any micro batch splits.</p>

<p>In this code total_training_steps the total number of rollouts that we do, and gradient update steps the number of gradient updates that we do. So keep in mind there two are two different things.</p>

<p>use dynamic_batch size to prevent OOM.
Here’s more complete version</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ---------------------------------
# 0.  CONSTANTS (from config)
# ---------------------------------
</span><span class="n">TOTAL_EPOCHS</span>               <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">trainer</span><span class="p">.</span><span class="n">total_epochs</span>
<span class="n">TRAIN_BATCH_SIZE</span>           <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">train_batch_size</span>          <span class="c1"># raw data loader batch
</span><span class="n">GEN_BATCH_SIZE</span>             <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">gen_batch_size</span><span class="sh">"</span><span class="p">,</span> <span class="n">TRAIN_BATCH_SIZE</span><span class="p">)</span>
<span class="n">ROLLOUT_N</span>                  <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">actor_rollout_ref</span><span class="p">.</span><span class="n">rollout</span><span class="p">.</span><span class="n">n</span>    <span class="c1"># GRPO: responses per prompt
</span><span class="n">PPO_MINI_BATCH_SIZE</span>        <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">actor_rollout_ref</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="n">ppo_mini_batch_size</span>
<span class="n">PPO_MICRO_BATCH_SIZE_PER_GPU</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">actor_rollout_ref</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="n">ppo_micro_batch_size_per_gpu</span>
<span class="n">PPO_EPOCHS</span>                 <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">actor_rollout_ref</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="n">ppo_epochs</span>
<span class="n">GRAD_ACCUM_STEPS</span>           <span class="o">=</span> <span class="n">PPO_MINI_BATCH_SIZE</span> <span class="o">//</span> <span class="n">PPO_MICRO_BATCH_SIZE_PER_GPU</span>  <span class="c1"># only when NOT dynamic-bsz
</span>
<span class="c1"># ---------------------------------
# 1.  OUTER LOOP
# ---------------------------------
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">TOTAL_EPOCHS</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">raw_dict</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span>               <span class="c1"># yields batches of size GEN_BATCH_SIZE
</span>        <span class="n">batch</span> <span class="o">=</span> <span class="n">DataProto</span><span class="p">.</span><span class="nf">from_single_dict</span><span class="p">(</span><span class="n">raw_dict</span><span class="p">)</span>

        <span class="c1"># ---------------------------------
</span>        <span class="c1"># 2.  ROLLOUT PHASE
</span>        <span class="c1"># ---------------------------------
</span>        <span class="n">gen_batch</span> <span class="o">=</span> <span class="n">batch</span><span class="p">.</span><span class="nf">pop_generation_keys</span><span class="p">()</span>     <span class="c1"># strip everything except what the policy needs
</span>        <span class="n">gen_batch</span> <span class="o">=</span> <span class="n">gen_batch</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">ROLLOUT_N</span><span class="p">)</span>     <span class="c1"># GRPO: duplicate prompts ROLLOUT_N times
</span>        <span class="n">rollout_output</span> <span class="o">=</span> <span class="n">actor_rollout_wg</span><span class="p">.</span><span class="nf">generate_sequences</span><span class="p">(</span><span class="n">gen_batch</span><span class="p">)</span>

        <span class="c1"># optional: max-baseline generation for REMAX
</span>        <span class="k">if</span> <span class="n">adv_estimator</span> <span class="o">==</span> <span class="n">REMAX</span><span class="p">:</span>
            <span class="n">baseline_output</span> <span class="o">=</span> <span class="n">actor_rollout_wg</span><span class="p">.</span><span class="nf">generate_sequences</span><span class="p">(</span><span class="n">gen_batch</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="c1"># ---------------------------------
</span>        <span class="c1"># 3.  BUILD FULL PPO BATCH
</span>        <span class="c1"># ---------------------------------
</span>        <span class="n">ppo_batch</span> <span class="o">=</span> <span class="n">batch</span><span class="p">.</span><span class="nf">union</span><span class="p">(</span><span class="n">rollout_output</span><span class="p">)</span>     <span class="c1"># concat prompts + responses
</span>        <span class="n">ppo_batch</span> <span class="o">=</span> <span class="n">ppo_batch</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">ROLLOUT_N</span><span class="p">)</span>     <span class="c1"># align prompt copies with responses
</span>
        <span class="c1"># 3a.  FILL IN EXTRA FIELDS
</span>        <span class="n">ppo_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">response_mask</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="nf">compute_response_mask</span><span class="p">(</span><span class="n">ppo_batch</span><span class="p">)</span>
        <span class="n">ppo_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">old_log_probs</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">actor_rollout_wg</span><span class="p">.</span><span class="nf">compute_log_prob</span><span class="p">(</span><span class="n">ppo_batch</span><span class="p">).</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">old_log_probs</span><span class="sh">"</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">use_reference_policy</span><span class="p">:</span>
            <span class="n">ppo_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">ref_log_prob</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">ref_policy_wg</span><span class="p">.</span><span class="nf">compute_log_prob</span><span class="p">(</span><span class="n">ppo_batch</span><span class="p">).</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">ref_log_prob</span><span class="sh">"</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">use_critic</span><span class="p">:</span>
            <span class="n">ppo_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">values</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">critic_wg</span><span class="p">.</span><span class="nf">compute_values</span><span class="p">(</span><span class="n">ppo_batch</span><span class="p">).</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">values</span><span class="sh">"</span><span class="p">]</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="nf">reward_fn</span><span class="p">(</span><span class="n">ppo_batch</span><span class="p">)</span>               <span class="c1"># rule or RM
</span>        <span class="n">ppo_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">token_level_scores</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">rewards</span>
        <span class="n">ppo_batch</span> <span class="o">=</span> <span class="nf">compute_advantage</span><span class="p">(</span><span class="n">ppo_batch</span><span class="p">)</span>     <span class="c1"># adds "advantages" &amp; "returns"
</span>
        <span class="c1"># ---------------------------------
</span>        <span class="c1"># 4.  MINI-BATCH LOOP
</span>        <span class="c1"># ---------------------------------
</span>        <span class="n">mini_batches</span> <span class="o">=</span> <span class="n">ppo_batch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">PPO_MINI_BATCH_SIZE</span><span class="p">)</span>   <span class="c1"># list of DataProto, each ≤ PPO_MINI_BATCH_SIZE
</span>        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">PPO_EPOCHS</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">mini_batch</span> <span class="ow">in</span> <span class="n">mini_batches</span><span class="p">:</span>
                <span class="c1"># 4a.  MICRO-BATCH LOOP  (gradient accumulation)
</span>                <span class="k">if</span> <span class="n">use_dynamic_bsz</span><span class="p">:</span>
                    <span class="n">micro_batches</span> <span class="o">=</span> <span class="nf">prepare_dynamic_batch</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">,</span>
                                                          <span class="n">max_token_len</span><span class="o">=</span><span class="n">PPO_MAX_TOKEN_LEN_PER_GPU</span> <span class="o">*</span> <span class="n">sp_size</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">micro_batches</span> <span class="o">=</span> <span class="n">mini_batch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">PPO_MICRO_BATCH_SIZE_PER_GPU</span><span class="p">)</span>

                <span class="n">actor_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">micro_batch</span> <span class="ow">in</span> <span class="n">micro_batches</span><span class="p">:</span>
                    <span class="n">log_probs</span><span class="p">,</span> <span class="n">entropy</span> <span class="o">=</span> <span class="n">actor_rollout_wg</span><span class="p">.</span><span class="nf">forward_micro_batch</span><span class="p">(</span><span class="n">micro_batch</span><span class="p">)</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="nf">compute_ppo_loss</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span>
                                            <span class="n">micro_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">old_log_probs</span><span class="sh">"</span><span class="p">],</span>
                                            <span class="n">micro_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">advantages</span><span class="sh">"</span><span class="p">],</span>
                                            <span class="n">micro_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">response_mask</span><span class="sh">"</span><span class="p">])</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">GRAD_ACCUM_STEPS</span>         <span class="c1"># scale for accumulation
</span>                    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

                <span class="n">actor_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>                    <span class="c1"># update happens once per mini-batch
</span></code></pre></div></div>


<!-- <article class="blog-post">
  <header>
    <h1>Rlhf</h1>
    <time datetime="2025-02-24T00:00:00+05:45">
      February 24, 2025
    </time>
  </header>

  <div class="post-content"><p>Before starting, it’s advisable to first complete <a href="https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-">David Silver’s Course on RL</a> and read <a href="https://lilianweng.github.io/posts/2018-02-19-rl-overview/#deadly-triad-issue">Lilian’s notes on RL</a> which explains/provides notes on the David’s course in sequential manner.</p>

<p>In simple problems, we simply start with an arbitrary value function, and then go on updating that value function incrementally, using different algorithms such as Monte Carlo (which collects reward over the whole episoe), Temporal difference, aka TD(0) (which considers bootstrapping, i.e only considering the immediate reward and then approximating other remaining rewards with the help of value function $r + V(s)$ ) and other algorithms. But, over time maintaining a table is not feasible for real world problems, so move towards approximating these value functions with the help of a vector of features or more concretely with Neural Networks but approximating these state and action value functions are infeasible too which is explained in the next section.</p>

<h3 id="why-we-choose-policy-approximation-instead-of-stateaction-value-approximation">Why we choose policy approximation instead of state/action value approximation.</h3>

<p>In real work tasks, the action space is not discrete, i.e up, down, right, left instead it’s continuous for eg. A robot can do infinite tasks. Deep Q Network works by taking the maximum over the action, and minimizing the error over that action. BUT, approximating every action becomes expensive task.</p>

<p>\(Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma \max_{a \in \mathcal{A}} Q(S_{t+1}, a) - Q(S_t, A_t))\)
So we move on to approximating our policy, where the approximated policy will provide the probability over the considerable actions, allowing the agent to <strong>sample actions</strong> rather than search for the best one.</p>

<h2 id="policy-gradient-algorithms">Policy Gradient Algorithms</h2>

<p>Proof of policy gradient theorem.</p>

<p>First we collect trajectory $\tau$, which is a collection of states and actions. $\tau = s_0, a_0, s_1, a_1, \ldots, s_H, a_H, s_{H+1}$</p>

<p>The reward for this trajectory is simply the sum of all the rewards from each timesteps.</p>

\[R(\tau) = \sum_{t=0}^{H-1} R(s_t, a_t)\]

<p>Our overall objective is to construct a policy that gives us the highest reward for our trajectory. We construct a function $J(\theta)$ that is parametrized by this policy, that calculates the total expected reward and we want to maximize this expectation.</p>

\[J(\theta) = E[R(\tau); \theta] \tag{1}\]

\[\max_\theta J(\theta) = \max_\theta E\left[\sum_{t=0}^{H-1} R(s_t, a_t)\right]\]

<p>If we open this objective function it becomes.</p>

\[J(\theta) = \sum_\tau P(\tau) \cdot R(\tau)\]

<p>where $P(\tau)$ is the probability of trajectory $\tau$, and $R(\tau)$ is the total reward for that trajectory. and Lets take the derivative of this $J(\theta)$</p>

\[\nabla_\theta J(\theta) = \sum_\tau \frac{P(\tau)}{P(\tau)} \nabla_\theta P(\tau) \sum_{t=0}^{H-1} R(s_t, a_t)\]

\[\nabla_\theta J(\theta) = \sum_\tau P(\tau) \cdot \nabla_\theta \log P(\tau) \cdot \sum_{t=0}^H R(s_t, a_t) \quad \text{(using } \frac{\partial \log P(\tau)}{\partial \theta} = \frac{1}{P(\tau)} \partial_\theta P(\tau) \text{)}\]

<p>now representing it back into the expectation form we get,</p>

\[\nabla_\theta J(\theta)
= E\left[  \nabla_\theta \log P(\tau) \cdot \sum_{t=0}^{H-1} R(s_t, a_t) \right] \tag{2}\]

<p>Now what is $P(\tau)$, it’s the joint probability for that trajectory, which is given by the chain rule of probability,</p>

\[P(\tau) = P(s_0) \cdot P(a_0|s_0) \cdot P(s_1|a_0, s_0) \cdot P(a_1|s_1) \cdot P(s_2|a_1, s_1) \cdots\]

<p>In the equation above, we assume Markov’s property that next action only depends on the current state.</p>

\[P(a_t|s_t, s_{t-1}) = P(a_t|s_t)\]

\[P(s_{t+1}|a_t, s_t, a_{t-1}, s_{t-1}) = P(s_{t+1}|a_t, s_t)\]

<p>It boils down to this equation below.</p>

\[P(\tau) = P(s_0) \prod_{t=0}^H P(a_t|s_t) \cdot P(s_{t+1}|a_t, s_t)\]

<p>and then to this</p>

\[P(\tau) = P(s_0) \prod_{t=0}^H \pi(a_t|s_t) \cdot P(s_{t+1}|a_t, s_t)\]

<table>
  <tbody>
    <tr>
      <td>because action distribution $\pi(a_t\ s_t)$ is governed by the policy $\theta$ whereas the transition probability $P(s_{t+1}</td>
      <td>a_t, s_t)$ is governed by the environment. Let’s take the log of that $P(\tau)$</td>
    </tr>
  </tbody>
</table>

\[\log P(\tau) = \log(P(s_0)) + \sum_{t=0}^{H-1} \log \pi(a_t|s_t) + \sum_{t=0}^{H-1}\log P(s_{t+1}|a_t, s_t)\]

<p>After taking the derivative of that $\log P(\tau)$, we end up with this equation below, because all the other terms down depend on our policy $\theta$</p>

\[\nabla_\theta \log P(\tau) = \sum_{t=1}^H \nabla_\theta \log \pi(a_t|s_t)\]

<p>Let’s replace the equation above in our equation 2. we get.</p>

\[\nabla_\theta J(\theta) = \frac{1}{M} \sum_{m=1}^{M} \left( \sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m |s_t^m) \cdot \left( \sum_{t=0}^{H-1} R(s_k^m, a_k^m)  \right) \right) \tag{3}\]

<h4 id="reducing-variance">Reducing Variance</h4>

<p>BUT, there’s a problem withe equation (3), it is high variance, because in stochastic environment R can sometimes take high value, and some times low. Since R acts as a magnitude term in the equation (3) it would make the gradient estimation and update unstable because of the high variance, to stabilize it we subtract a baseline from the the return in equation (3).</p>

\[\nabla_\theta J(\theta) = \frac{1}{M} \sum_{m=1}^{M} \left( \sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m |s_t^m)  \cdot \left( \sum_{t=0}^{H-1} R(s_k^m, a_k^m)  - b\right) \right) \tag{3}\]

<p>subtracting b from the trajectory reward doesn’t increase it’s bias and there is a proof of that which I’m not going to explain in this section, it helps us by decreasing the variance. If we’re able to keep the bias stable and variance low, it would be a win win situation and this is what we do here.</p>

<p>What should the b value be?</p>

<p>It would be better if we have that baseline as an average return that we might expect when we are in state t, and that is simply our value function $V^\pi(s)$.</p>

\[\nabla_\theta J(\theta) = \frac{1}{M} \sum_{m=1}^{M} \left( \sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m |s_t^m)  \cdot \left( \sum_{t=0}^{H-1} R(s_k^m, a_k^m)  - V^\pi(s_k^m)\right) \right) \tag{4}\]

<p>Equation (4) tell us we are increasing the logprobs of actions that gives us the return that is better than the average.</p>

<p>There still one way we can reduce further variance from equation (4), we can break down equation (4) into the equation below.</p>

\[\nabla_\theta J(\theta) = \frac{1}{M} \sum_{m=1}^{M} \left( \sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m |s_t^m)  \cdot \left( \sum_{t=0}^{t-1}R(s_k^m, a_k^m) + \sum_{k=t}^{H-1} R(s_k^m, a_k^m)  - V^\pi(s_k^m)\right) \right) \tag{5}\]

\[\nabla_\theta J(\theta) = \frac{1}{M} \sum_{m=1}^{M} \left( \sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m |s_t^m)  \cdot \left(  \sum_{k=t}^{H-1} R(s_k^m, a_k^m)  - V^\pi(s_k^m)\right) \right) \tag{6}\]

<p>we can remove this term $\sum_{t=0}^{t-1}R(s_k^m, a_k^m)$, because why would the rewards at time step 4 affect the logprobs of action at time step 7. We only care about the FUTURE rewards not the past rewards, we are essentially deleting the past rewards from there. For concrete understanding, let’s say we have H = 3, M=1</p>

<p>t=0
$\nabla_\theta \log(a_0|s_0) = 0.5$
$R(s_0,a_0) = 10$</p>

<p>t=1
$\nabla_\theta \log(a_1|s_1) = 0.4$
$R(s_1,a_1) = 4$</p>

<p>t=2
$\nabla_\theta \log(a_2|s_2) = 0.2$
$R(s_2,a_2) = 7$</p>

<p>So,</p>

<table>
  <tbody>
    <tr>
      <td>$\sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m</td>
      <td>s_t^m) = (0.5 + 0.4 + 0.2)$</td>
    </tr>
  </tbody>
</table>

<p>$\sum_{t=0}^{H-1} R(s_k^m, a_k^m) = (10 + 4 + 7)$</p>

<p>Let’s add those values in the equation 4.</p>

<p>$(0.5 + 0.4 + 0.2) \cdot (10 + 4 + 7) = (0.5<em>(10 + 4 + 7) + 0.4</em> (10 + 4 + 7) + 0.2 * (10 + 4 + 7))$</p>

<p>as you can see we are including rewards at past timestep for future actions too (i.e 0.4 is the logprobs for timestep 1 and it is multiplied with (10 + 4 + 7), where 10 is the reward from the past timestep 0, we don’t want that) so, if we use the equation (6), we end up with something like this</p>

<p>$(0.5<em>(10 + 4 + 7) + 0.4</em> (4 + 7) + 0.2 * (7))$</p>

<p>We only consider the future rewards, not the past ones.</p>

<h4 id="how-to-approximate-value-function">How to approximate value function?</h4>

<p>We can approximate value function $V^\pi$ as taught in this <a href="https://www.youtube.com/watch?v=UoPei5o4fps&amp;list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&amp;index=6">lecture</a> we can simply sample from the trajectory and then minimize the squared error i.e</p>

<p>let’s sample $M$ trajectories, $\tau_1, \tau_2, … \tau_M$ and then minimize this squared error</p>

\[\frac{1}{M} \sum_{m=1}^{M} \sum_{t=0}^{H-1} \left( V^\pi(s_t^m) - \sum_{k=t}^{H-1} R(s_k^m, a_k^m) \right)^2\]

<p>There’s one more thing we can do, we can approximate this term $\sum_{k=t}^{H-1} R(s_k^m, a_k^m)$ with our Q value function which would further reduce the variance.</p>

<p>Since $Q^\pi= E[r_0 + \gamma r_1 + \gamma^2 r_2 …]$
adding gamma would reduce the variance further, and approximating this $Q^\pi$ with the help of value function would further reduce the variance.</p>

\[\nabla_\theta J(\theta) = \frac{1}{M} \sum_{m=1}^{M}  \sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m |s_t^m)  \cdot \left( Q^\pi (s_t^m, a_t^m)  - V^\pi(s_t^m)\right) ) \tag{7}\]

\[\nabla_\theta J(\theta) = \frac{1}{M} \sum_{m=1}^{M}  \sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m |s_t^m)  \cdot A^\pi(s_t, a_t) ) \tag{8}\]

<p>We can approximate this using bootstrapping, $A^\pi(s,a)$ in different ways such as.</p>

\[A^\pi(s_t,a_t)= r_0 + V^\pi (s_{t+1}) - V^\pi(s_t))\]

<p>Or more compact form called Generalized Advantaged Estimation (GAE)</p>

\[\delta_t = r_t + \gamma V(s_{t+1} - V(s_t))\]

\[A_t(s_t, a_t) = \sum_{l=0}^\infty \gamma \lambda \delta_{t + l}\]

<h3 id="general-rl-setting">General RL setting</h3>

\[J(\pi) =
  \mathbb{E}\_{\tau \sim \pi} \left[ \sum\_{t=0}^{\infty} \gamma^t r(s\_t,
  a\_t) \right],\qquad{(1)}\]

<p>In RL setting we aim to optimize the objective function $J(\pi)$ by updating the policy $\pi$
given a reward function $r(s,a)$ that takes in a state and the action performed at that state. The next action is determined by $\pi(a|s)$. We find the expected(average) reward over all the trajectories $\tau$. $\gamma$ is the discount factor from 0 to 1 that balances the desirability of of near vs future-rewards.</p>

<p>An example: If our RL setup is confined to finding a maze, a reward function could be $r(s,a)=-\text{distance to goal}$</p>

<h3 id="rlhf">RLHF</h3>

<p>RLHF reduces the equation 1 by removing this discounted factor</p>

\[J(\pi) =
  \mathbb{E}\_{\tau \sim \pi} \left[ \sum\_{t=0}^{\infty} r(s\_t,
  a\_t) \right],\qquad{(2)}\]

<p>We aim to maximize our objective function by optimizing the policy. The reward function is designed such that the actions must align with the human preferences.</p>

<p>The most common reward model predicts the probability that a piece of text was close to a “preferred” piece of text from the training comparisons.</p>

<h3 id="reward-models">Reward Models</h3>

<p>Given two prompts $y1$ and $y2$ we want to have a reward model that gives high score to $y1$ and low score to $y2$ meaning $y1$ is always preferred over $y2$. Their relative preference is given by the Bradly Terry model.</p>

\[P(i &gt; j) =
  \frac{p\_i}{p\_i + p\_j}\qquad{(3)}\]

<p>It gives the probability of i being preferred over j where $p_{i}$ and $p_{j}$ represent “strengths” for those prompts.</p>

<p>We want our model to maximize this probability because later $i$ would represent the text we want (aligned) and $j$ would represent the text we don’t want (not aligned). The training objective can be derived from the equation above. The “strengths” are exponential because we want them to be strictly positive.</p>

\[P(y\_1 &gt;
  y\_2) = \frac{\exp(r(y\_1))}{\exp(r(y\_1)) +
  \exp(r(y\_2))}\qquad{(4)}\]

<p>The loss function becomes</p>

\[\mathcal{L}(\theta) = - \log \left( \sigma
  \left( r\_{\theta}(x, y\_w) - r\_{\theta}(x, y\_l) \right)
  \right)\qquad{(6)}\]

<p>Our existing model can be configured to output just one value by adding a linear head to the model.</p>

<p>In case of language models, we want a model to rate our answer i.e (give a score based on how good or bad it is), it is the most important part because it will guide our training, it is providing supervision for our PPO algorithm.</p>

<h4 id="steps-for-training-a-reward-model">Steps for training a reward model</h4>

<p>Collect pairs of data, it could be either contrasting pairs or some pairs of prompt with priority. Eg. We want our llm to be trained (later using PPO) to be to generate positive response. In this case our priority prompt would be positive prompt and non-priority prompt would be negative prompt.</p>

<p>We take a language model and add a linear head to it. For instance, for each token a LM outputs 512 dimension vector we add a new head that takes in 512 dimension and outputs a one dimensional vector which gives us the reward.</p>

<p>The loss function for the reward model is constructed likewise, which is same the equation 6.</p>

\[L = -log(\sigma(r1 - r2))\]

<p>where $r1$ is reward for priority prompt and $r2$ is reward for non-priority prompt. We want to maximize this difference $r1 - r2$ and minimize the this function. $\sigma$ represents sigmoid function.</p>

<h3 id="notes">NOTES</h3>

<ul>
  <li>we calculate the reward for the ending token which represents a reward given to the whole token.</li>
  <li>reward models overfit fast, so we can consider smaller LM for reward model training.</li>
</ul>

<p>——-<strong>Skipping other reward models for now</strong>——-</p>

<h3 id="regularization">Regularization</h3>

<p>We want aligned reward models but would still “not go off the rails” meaning it stays within the limitation of our reference model. It allows the policy being trained to stay close to the reference policy.</p>

\[r = r\_\theta -
  \lambda r\_{\text{reg.}} \qquad{(1)}\]

\[r = r\_\theta - \lambda\_{\text{KL}} \mathcal{D}\_{\text{KL}} \left(
  \pi^{\text{RL}}(y \mid x) \, \| \, \pi^{\text{Ref.}}(y \mid x) \right)
  \qquad{(2)}\]

<p>KL divergence is calculated as</p>

\[D\_{\text{KL}}(P \,||\, Q) = \mathbb{E}\_{x \sim P} \left[ \log P(x) -
  \log Q(x) \right].
  \qquad{(3)}\]

<h3 id="rejection-sampling">Rejection Sampling</h3>

<p>This process is kind of a filtering process. We first sample outputs from our base language model that we’ll be training next. For example, we generate 5 example outputs for a sample prompt. We then provide a score to each of the generated examples using our reward function. We sort and only take the top-K elements per prompt and fine-tune our base model on these examples. So it’s kind of like a filtering and only keeping the highly rewarded examples.</p>

<h3 id="ppo">PPO</h3>

<h4 id="ppo-algorithm-analogy">PPO algorithm analogy</h4>

<p>Suppose we are taking an exam paper. Our objective is to maximize the change of getting good marks by updating our brain (weights and biases).</p>

<p>$s_t$ is a question that we are looking at and trying to solve. $a_t$ is the our answer for that question. $r_t$ is the immediate reward we get after writing $a_t$, $s_{t+1}$ is the next-question.</p>

<p>$R_t$ is the actual total exam score.</p>

<p>Suppose there is an imaginary machine that gives us the expected exam score that we can get just by looking at our question which is $V(s)$.</p>

<p>$A(s)$ is our advantage i.e how good we are compared to the predicted score.</p>

<p>$A(s) = R_t - V(s)$</p>

<p>this can be modified as as $\delta_t = R_t - V(s)$ and $A_t = \delta_t + \lambda\cdot\gamma\cdot A_{t+1}$. This is just a modified version of advantage to remove the bias and variance.</p>

<p>This can be a little confusing as we might have no idea what our actual total score($R_t$) will be while we are still writing some questions $s$ so approximate this $R_t$ with the help of current reward $r$ and future reward that we might get from next question i.e $V(s+1)$ i.e</p>

<p>this becomes</p>

<p>$\delta_t =  r_t + V(s+1) - V(s)$</p>

<p>so this will still give us our advantage at a point t.</p>

<p>i.e how good/bad we did at point t= immediate reward(marks) after writing answer to t + expected future reward from new question - expected future reward from previous question t.</p>

<p>This will give us our advantage.</p>

<p>PPO is done using two phases.</p>

<ol>
  <li>Rollout phase</li>
  <li>Weight update phase.</li>
</ol>

<h4 id="rollout-phase">Rollout phase</h4>

<p>We write a lot of exam paper in this phase in parallel.
for each exam paper and for each question in the exam paper we calculate $r_t$, $V(s_t)$ and $R_t$ $A_t$ and use it in our equation.</p>

\[L^{PPO}(\theta) = \mathbb{E}\_t \left[
\min \left( r\_t(\theta) \hat{A}\_t, \text{clip}\left(r\_t(\theta), 1 - \epsilon,1+\epsilon\right) \hat{A}\_t \right) - c\_1 \left( V\_\theta(s\_t) -V\_t^\text{target} \right)^2 + c\_2 \mathcal{H}\left[\pi\_\theta\right(s\_t)\right]\]

\[r\_t(\theta) = \frac{\pi\_\theta(a\_t | s\_t)}{\pi\_{\theta\_\text{old}}(a\_t|s\_t)}\]

<h4 id="weight-update-phase">Weight update phase</h4>

<p>We find this loss and try to maximize this clipped loss and entropy loss, but minimize the value function loss.</p>

<p>The PPO clipped surrogate objective is given as:</p>

\[L^{\text{CLIP}}(\theta) = \mathbb{E}\_t \Big[ \min \big( r\_t(\theta) \hat{A}\_t, \; \mathrm{clip}(r\_t(\theta), 1-\varepsilon, 1+\varepsilon) \hat{A}\_t \big) \Big]\]

<p>The gradient ascent update rule is:</p>

\[\theta \gets \theta + \alpha \nabla\_\theta L^{\text{CLIP}}(\theta)\]

<p>The most important part to understand here is</p>

<p>$r_t(\theta) \hat{A}_t$</p>

<p>so when this term gets clipped to either $1 + \varepsilon$ or $1-\varepsilon$ the gradient of this loss $\nabla_{\theta}r_t(\theta) \hat{A}_t$ is 0. So no update to the weights. But when this gradient is $\nabla_{\theta}r_t(\theta) \hat{A}_t$ the update depends on whether $\hat{A}_t$ is &gt;0 or &lt;0,</p>

<p>If $\hat{A}_t &gt; 0$, the gradient update will be in the direction that increases ${\pi_{\theta}}$.
If $\hat{A}_t &gt; 0$, the gradient update will be in the direction that decreases ${\pi_{\theta}}$.</p>

<h3 id="integrating-reward-model-to-ppo">Integrating reward model to PPO</h3>

<p>As you might have noticed, the PPO loss comes from the token level, meaning we need loprobs, value, reward, return and advantage for each token. Buuuuut, this reward function we just trained is trained to output only one reward for last token so how does that work?</p>

<p>Answer: the reward is propagated.</p>

<p>The advantage is calculated reverse recursively i.e advantage at token n is also passed to the n-1 token. This means that we are looking ahead and telling our token n-1 that we already ended up a good state because of the state we are in. Lets look at this from the lens of advantage formula.</p>

<p>$\delta_t = R_t - V(s)$ and $A_t = \delta_t + \lambda\cdot\gamma\cdot A_{t+1}$</p>

<p>Lets consider we are at 100th token which is the ending token of our sequence</p>

<p>\(R\_t = r\_t + V\_{t+1}(s) - V\_t(s)\)
reward model assigns $r_{100}$ 100 and lets ignore both the value function assuming they cancel out as we are already at the end.</p>

\[\begin{gather}
R\_t= 100\\\\A\_{100}=100, considering A\_{101}=0
\end{gather}\]

<p>At 100th token we are at an advantage, now lets calculate $A_{99}$</p>

\[A\_{99} = \delta\_{99} + \lambda\cdot\gamma\cdot A\_{100}\]

<p>as you can see the reward 100 is propagated to the 99th token, considering $\delta_{99}$ is positive here, it tells us that token 99 is still at an advantage because the we can already see the future here i.e 100th token which was at an advantage, so taking action 99 is still an advantage to us.</p>

<h3 id="reward-hacking">Reward Hacking</h3>

<p>our models can find a loophole to maximize their return by generating high reward tokens but no-so-good answer. Example: If we are trying to make our model positive, model may find a way to output tokens such as “thank you” and add it our answer, which will provide a high reward to it, but it is meaningless to us. So we don’t want our new (trained via PPO) model to deviate significantly from the model that we started from (SFT model), so we add KL divergence as a penalty to our each token’s reward.</p>

<p>As described earlier, reward is provided to only the ending token, and the reward for other token becomes this KL penalty.</p>

<p>i.e if we have token_1, token_2, and token_3</p>

<p>r(token_3) = some reward from the reward model
r(token_2) = KL penalty i.e (logprobs_for_token_2_from_model_being_trained - logprobs_for_token_2_from_SFT_model)* -KL_penalty_coefficient.
and so on…</p>

<p>Some key points</p>

<ul>
  <li>Human preferences that are used to train LLMs are multi-dimensional but the reward is a single score. LLMs being complex and “intelligent” will always find a way to exploit these rewards thus the reward hacking, so in large scale RLHF, reward models get saturated very fast, and we might need to train a new reward model.</li>
</ul>

<h3 id="grpo">GRPO</h3>

<p>The per token loss for GRPO is likewise..</p>

<p>Key difference between GRPO and PPO.</p>

<p>GRPO completely removes values function.
as value function is removed the advantage calculate is simplified.</p>

\[A\_i = \frac{r\_i - \text{mean}(\{r\_1, r\_2, \cdots, r\_G\})}{\text{std}(\{r\_1,r\_2, \cdots,r\_G\})}\]

<p>For each question/prompt different G samples are generated, and for each advantage for each question the reward is normalized to form the advantage.</p>

<p>Previously, in PPO we added KL penalty to the rewards themselves, but in GRPO we add it to the loss function directly.</p>

\[J(\theta) =
  \frac{1}{G}\sum\_{i=1}^G  \frac{1}{|a\_i|} \sum\_{t=1}^{|a\_i|} \left(
  \min\left(\frac{\pi\_\theta(a\_{i,t}|s\_{i,t})}{\pi\_{\theta\_{old}}(a\_{i,t}|s\_{i,t})}A\_{i,t},
  \text{clip} \left(
  \frac{\pi\_\theta(a\_{i,t}|s\_{i,t})}{\pi\_{\theta\_{old}}(a\_{i,t}|s\_{i,t})},
  1-\varepsilon, 1+\varepsilon \right) A\_{i,t} \right) - \beta
  D\_{KL}(\pi\_\theta(\cdot|s\_{i,t})||\pi\_{ref}(\cdot|s\_{i,t}))
  \right)\]

<h2 id="observations-from-dapo-paper">Observations from DAPO paper</h2>

<h3 id="decoupled-clipping-parameter">Decoupled Clipping parameter</h3>

<p>The same $\epsilon$ using in clipping doesn’t favor exploration token i.e tokens with less probability ex. 0.02. After some steps, the entropy of the model starts to dampen, i.e model is becoming less explorative, to restrict this entropy collapse, DAPO paper recommends using a high $\epsilon$ for (1 + $\epsilon$) and same $\epsilon=0.2$ for (1-$\epsilon$).</p>

<h3 id="increase-sample-output-variations">Increase sample output variations</h3>

<p>when the accuracy for all the samples in G have same accuracy 1. The Advantage becomes 0, and the gradient becomes 0 as well. Also, the number of samples with accuracy=1 increases as we train further which decreases the effective prompts in the batch which can lead to larger variance in gradients. The direction for the gradient update is not accurate since the effective prompts in batch is small because of which the gradient updates are noisy.</p>

<p>The proposed method is to oversample and filter out prompts with accuracy 0 or 1</p>

<h3 id="per-token-loss">Per token loss</h3>

<p>In GRPO, loss is first averaged over token and then averaged over all samples G. In doing so, there remains no difference between samples with very large/small sequence length i.e each sample will get averaged loss and tokens in long sequence will get less priority which could be a problem for use if that token is an important token in our long sequence.</p>

<p>DAPO solves this by first summing the loss for all tokens for all samples and then averaging it over all tokens.</p>

\[\mathcal{J}\_{\text{DAPO}}(\theta) = \mathbb{E}\_{(q,a) \sim \mathcal{D}, \{o\_i\}\_{i=1}^G \sim \pi\_{\theta\_{old}}(\cdot|q)} \left[
\frac{1}{\sum\_{i=1}^G |o\_i|} \sum\_{i=1}^G \sum\_{t=1}^{|o\_i|}
\min \left(
r\_{i,t}(\theta) \hat{A}\_{i,t}, \;
\text{clip}\left(
r\_{i,t}(\theta), 1 - \epsilon\_{\text{low}}, 1 + \epsilon\_{\text{high}}
\right) \hat{A}\_{i,t}
\right)
\right]\]

<h3 id="overlong-reward-shaping">Overlong Reward Shaping</h3>

<p>Instead of adding penalty for overly long outputs, just mask the truncated samples. This way the training is stable and enhances performances. Why do this ? adding penalty may confuse when the long reasoning is a valid/good reasoning trace.</p>

<h3 id="multi-turn-rl">Multi-turn RL</h3>

<p>llm and user interact over a period of time.</p>

<h4 id="problems">Problems</h4>

<p>(BENCHMARK)
no benchmark to test multi-turn interactions, and is not reasoning intensive(meaning they only focus on narrow domains, require max engineering overhead)</p>

<p>(RL ALGORITHMS)
PPO, DPO they often suffer from high variance when the horizon gets longer, resulting in poor performance.</p>

<h3 id="verl-algorithm">veRL algorithm</h3>

<p>I forget what these variables that we provide in .sh file while training, so I wrote this boilerplate to remember how these variables play out in the veRL implementation.</p>

<p>rollout code: https://github.com/volcengine/verl/blob/332c7d53c131869690578c0ab84f1e016ed4d817/verl/trainer/ppo/ray_trainer.py#L1081
update code: https://github.com/volcengine/verl/blob/332c7d53c131869690578c0ab84f1e016ed4d817/recipe/sppo/dp_actor.py#L60</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Hyperparameters:
# - train_batch_size = Total experiences per PPO iteration (rollout buffer size)
# - ppo_mini_batch_size = Batch size for each gradient step (subdivided from train_batch_size)
# - ppo_micro_batch_size_per_gpu = Per-GPU batch size (for multi-GPU training)
# - ppo_epochs = Number of passes over the rollout data
</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">total_epoch</span><span class="p">:</span>
	<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">complete_train_data</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">train_batch_size</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span> <span class="c1"># each batch size is provided by train_batch_size
</span>		<span class="nf">generate_rollout</span><span class="p">()</span> <span class="c1"># if GRPO use actor.rollout.n variable
</span>		<span class="nf">generate_old_logprobs</span><span class="p">()</span>
		<span class="nf">generate_ref_logprobs</span><span class="p">()</span>
		<span class="nf">calculate_advantages</span><span class="p">()</span>

		<span class="c1"># split batch into mini_batches.
</span>		<span class="n">minibatch_dataloader</span> <span class="o">=</span> <span class="n">batch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">ppo_mini_batch_size</span><span class="p">)</span> <span class="c1"># this is a dataloader with each minibatch of size ppo_mini_batch_size
</span>		<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">ppo_epoch</span><span class="p">:</span>
			<span class="k">for</span> <span class="n">minibatch</span> <span class="ow">in</span> <span class="n">minibatch_dataloader</span><span class="p">:</span>
				<span class="c1">#split minibatch into microbatches if needed to train on different GPUs
</span>				<span class="n">micro_batches</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">ppo_micro_batch_size_per_gpu</span><span class="p">)</span>
				<span class="n">gradient_accumulation</span> <span class="o">=</span> <span class="n">ppo_mini_batch_size</span> <span class="o">//</span> <span class="n">ppo_micro_batch_size_per_gpu</span>
				<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">micro_batches</span><span class="p">:</span>
					<span class="nf">generate_logprobs</span><span class="p">()</span>
					<span class="n">loss</span> <span class="o">=</span> <span class="nf">calculate_ppo_loss</span><span class="p">()</span> <span class="o">/</span> <span class="n">gradient_accumulation</span>
					<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
				<span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>


</code></pre></div></div>

<p>gradient_accumulation step is not used in a sense that we generally do while pretraining, it just maintains the count total number of micro batches that are processed in separate GPU, by dividing loss by gradient_accumulation we obtain loss as if the minibatch was processed directly without using any micro batch splits.</p>

<p>In this code total_training_steps the total number of rollouts that we do, and gradient update steps the number of gradient updates that we do. So keep in mind there two are two different things.</p>

<p>use dynamic_batch size to prevent OOM.
Here’s more complete version</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ---------------------------------
# 0.  CONSTANTS (from config)
# ---------------------------------
</span><span class="n">TOTAL_EPOCHS</span>               <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">trainer</span><span class="p">.</span><span class="n">total_epochs</span>
<span class="n">TRAIN_BATCH_SIZE</span>           <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">train_batch_size</span>          <span class="c1"># raw data loader batch
</span><span class="n">GEN_BATCH_SIZE</span>             <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">gen_batch_size</span><span class="sh">"</span><span class="p">,</span> <span class="n">TRAIN_BATCH_SIZE</span><span class="p">)</span>
<span class="n">ROLLOUT_N</span>                  <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">actor_rollout_ref</span><span class="p">.</span><span class="n">rollout</span><span class="p">.</span><span class="n">n</span>    <span class="c1"># GRPO: responses per prompt
</span><span class="n">PPO_MINI_BATCH_SIZE</span>        <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">actor_rollout_ref</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="n">ppo_mini_batch_size</span>
<span class="n">PPO_MICRO_BATCH_SIZE_PER_GPU</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">actor_rollout_ref</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="n">ppo_micro_batch_size_per_gpu</span>
<span class="n">PPO_EPOCHS</span>                 <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">actor_rollout_ref</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="n">ppo_epochs</span>
<span class="n">GRAD_ACCUM_STEPS</span>           <span class="o">=</span> <span class="n">PPO_MINI_BATCH_SIZE</span> <span class="o">//</span> <span class="n">PPO_MICRO_BATCH_SIZE_PER_GPU</span>  <span class="c1"># only when NOT dynamic-bsz
</span>
<span class="c1"># ---------------------------------
# 1.  OUTER LOOP
# ---------------------------------
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">TOTAL_EPOCHS</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">raw_dict</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span>               <span class="c1"># yields batches of size GEN_BATCH_SIZE
</span>        <span class="n">batch</span> <span class="o">=</span> <span class="n">DataProto</span><span class="p">.</span><span class="nf">from_single_dict</span><span class="p">(</span><span class="n">raw_dict</span><span class="p">)</span>

        <span class="c1"># ---------------------------------
</span>        <span class="c1"># 2.  ROLLOUT PHASE
</span>        <span class="c1"># ---------------------------------
</span>        <span class="n">gen_batch</span> <span class="o">=</span> <span class="n">batch</span><span class="p">.</span><span class="nf">pop_generation_keys</span><span class="p">()</span>     <span class="c1"># strip everything except what the policy needs
</span>        <span class="n">gen_batch</span> <span class="o">=</span> <span class="n">gen_batch</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">ROLLOUT_N</span><span class="p">)</span>     <span class="c1"># GRPO: duplicate prompts ROLLOUT_N times
</span>        <span class="n">rollout_output</span> <span class="o">=</span> <span class="n">actor_rollout_wg</span><span class="p">.</span><span class="nf">generate_sequences</span><span class="p">(</span><span class="n">gen_batch</span><span class="p">)</span>

        <span class="c1"># optional: max-baseline generation for REMAX
</span>        <span class="k">if</span> <span class="n">adv_estimator</span> <span class="o">==</span> <span class="n">REMAX</span><span class="p">:</span>
            <span class="n">baseline_output</span> <span class="o">=</span> <span class="n">actor_rollout_wg</span><span class="p">.</span><span class="nf">generate_sequences</span><span class="p">(</span><span class="n">gen_batch</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="c1"># ---------------------------------
</span>        <span class="c1"># 3.  BUILD FULL PPO BATCH
</span>        <span class="c1"># ---------------------------------
</span>        <span class="n">ppo_batch</span> <span class="o">=</span> <span class="n">batch</span><span class="p">.</span><span class="nf">union</span><span class="p">(</span><span class="n">rollout_output</span><span class="p">)</span>     <span class="c1"># concat prompts + responses
</span>        <span class="n">ppo_batch</span> <span class="o">=</span> <span class="n">ppo_batch</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">ROLLOUT_N</span><span class="p">)</span>     <span class="c1"># align prompt copies with responses
</span>
        <span class="c1"># 3a.  FILL IN EXTRA FIELDS
</span>        <span class="n">ppo_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">response_mask</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="nf">compute_response_mask</span><span class="p">(</span><span class="n">ppo_batch</span><span class="p">)</span>
        <span class="n">ppo_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">old_log_probs</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">actor_rollout_wg</span><span class="p">.</span><span class="nf">compute_log_prob</span><span class="p">(</span><span class="n">ppo_batch</span><span class="p">).</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">old_log_probs</span><span class="sh">"</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">use_reference_policy</span><span class="p">:</span>
            <span class="n">ppo_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">ref_log_prob</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">ref_policy_wg</span><span class="p">.</span><span class="nf">compute_log_prob</span><span class="p">(</span><span class="n">ppo_batch</span><span class="p">).</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">ref_log_prob</span><span class="sh">"</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">use_critic</span><span class="p">:</span>
            <span class="n">ppo_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">values</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">critic_wg</span><span class="p">.</span><span class="nf">compute_values</span><span class="p">(</span><span class="n">ppo_batch</span><span class="p">).</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">values</span><span class="sh">"</span><span class="p">]</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="nf">reward_fn</span><span class="p">(</span><span class="n">ppo_batch</span><span class="p">)</span>               <span class="c1"># rule or RM
</span>        <span class="n">ppo_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">token_level_scores</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">rewards</span>
        <span class="n">ppo_batch</span> <span class="o">=</span> <span class="nf">compute_advantage</span><span class="p">(</span><span class="n">ppo_batch</span><span class="p">)</span>     <span class="c1"># adds "advantages" &amp; "returns"
</span>
        <span class="c1"># ---------------------------------
</span>        <span class="c1"># 4.  MINI-BATCH LOOP
</span>        <span class="c1"># ---------------------------------
</span>        <span class="n">mini_batches</span> <span class="o">=</span> <span class="n">ppo_batch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">PPO_MINI_BATCH_SIZE</span><span class="p">)</span>   <span class="c1"># list of DataProto, each ≤ PPO_MINI_BATCH_SIZE
</span>        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">PPO_EPOCHS</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">mini_batch</span> <span class="ow">in</span> <span class="n">mini_batches</span><span class="p">:</span>
                <span class="c1"># 4a.  MICRO-BATCH LOOP  (gradient accumulation)
</span>                <span class="k">if</span> <span class="n">use_dynamic_bsz</span><span class="p">:</span>
                    <span class="n">micro_batches</span> <span class="o">=</span> <span class="nf">prepare_dynamic_batch</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">,</span>
                                                          <span class="n">max_token_len</span><span class="o">=</span><span class="n">PPO_MAX_TOKEN_LEN_PER_GPU</span> <span class="o">*</span> <span class="n">sp_size</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">micro_batches</span> <span class="o">=</span> <span class="n">mini_batch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">PPO_MICRO_BATCH_SIZE_PER_GPU</span><span class="p">)</span>

                <span class="n">actor_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">micro_batch</span> <span class="ow">in</span> <span class="n">micro_batches</span><span class="p">:</span>
                    <span class="n">log_probs</span><span class="p">,</span> <span class="n">entropy</span> <span class="o">=</span> <span class="n">actor_rollout_wg</span><span class="p">.</span><span class="nf">forward_micro_batch</span><span class="p">(</span><span class="n">micro_batch</span><span class="p">)</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="nf">compute_ppo_loss</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span>
                                            <span class="n">micro_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">old_log_probs</span><span class="sh">"</span><span class="p">],</span>
                                            <span class="n">micro_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">advantages</span><span class="sh">"</span><span class="p">],</span>
                                            <span class="n">micro_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">response_mask</span><span class="sh">"</span><span class="p">])</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">GRAD_ACCUM_STEPS</span>         <span class="c1"># scale for accumulation
</span>                    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

                <span class="n">actor_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>                    <span class="c1"># update happens once per mini-batch
</span></code></pre></div></div>
</div>

  
  <div class="tags">
    
    <span class="tag">blog</span>
    
    <span class="tag">RL</span>
    
  </div>
  
</article> -->
</main>
  </body>
</html>
