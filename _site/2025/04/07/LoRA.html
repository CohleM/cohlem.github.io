<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Lora - </title>
    <link rel="stylesheet" href="/assets/css/main.css" />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Zalando+Sans+Expanded:ital,wght@0,200..900;1,200..900&family=Zalando+Sans:ital,wght@0,200..900;1,200..900&display=swap"
      rel="stylesheet"
    />

    <!-- MathJax for LaTeX -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
          displayMath: [
            ["$$", "$$"],
            ["\\[", "\\]"],
          ],
          processEscapes: true,
          processEnvironments: true,
        },
        options: {
          skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"],
        },
      };
    </script>
    <script
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
      id="MathJax-script"
      async
    ></script>
  </head>
  <body>
    <nav>
      <a href="/">Home</a>
      <a href="/posts">Posts</a>
      <a href="/notes">Notes</a>
      <a href="/blogs">Blogs</a>
    </nav>

    <main><h1>Lora</h1>
<p>07 Apr 2025 - cohlem</p>

<h3 id="lora">LoRA</h3>

<p>Main idea is to approximate the change in weights dW by the use of low-rank matrices</p>

<p>Eg: Usually the weight update is done by adding the change in weights dW to the original weight matrix W. dW is obtained through backpropagation, ex if W is 512 x 512 the parameter size of dW is 262,144.</p>

<p>In LoRA, we approximate that dW but by breaking down into two low rank matrices B @ A where B = matrix of size 512 x r and A = matrix of size r x 512,</p>

<p>previously if the forward pass was like this</p>

<p>out = X @ W</p>

<p>we change the forward pass:
out = X @ W + X @ B@A</p>

<p>we freeze all the other parameters (W in this case), and only find gradients for B,A and update only these weights.</p>

<p>First lets implement toy example that approximates sin function, we implement manual backpropagation so that It’ll be easier to understand what gets updated in our LoRA implementation.</p>

<h3 id="paper-findings-only-focus-on-highlighted-sentences">Paper findings (only focus on highlighted sentences)</h3>

<p><img src="/assets/images/2025-04-07-LoRA/lora1.png" alt="lora1" /></p>

<p><img src="/assets/images/2025-04-07-LoRA/lora2.png" alt="lora2" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># import torch
</span><span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Generate synthetic data (sine wave)
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">100</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Convert to PyTorch tensors
</span><span class="n">X_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Define a small MLP
</span><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="bp">True</span><span class="p">),</span>  <span class="c1"># 1 input → 10 hidden
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>    <span class="c1"># 10 hidden → 1 output
</span>        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Initialize model, loss, and optimizer
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">()</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>
<span class="c1"># optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
</span>
<span class="c1"># Training loop
</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

    <span class="c1">#Instead of this forward pass
#     outputs = model(X_tensor)
</span>
    <span class="c1"># Implement this forward to manually do the forward pass
</span>    <span class="n">o1</span> <span class="o">=</span> <span class="n">X_tensor</span> <span class="o">@</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span>
    <span class="n">o2</span> <span class="o">=</span> <span class="n">o1</span> <span class="o">+</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">bias</span>
    <span class="n">o3</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">o2</span><span class="p">)</span> <span class="c1"># relu layer
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="n">o3</span> <span class="o">@</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span>


    <span class="n">diff</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">-</span> <span class="n">y_tensor</span>
    <span class="n">squared_diff</span> <span class="o">=</span> <span class="n">diff</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">o_seven</span> <span class="o">=</span> <span class="n">squared_diff</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">o_seven</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">)</span>

    <span class="c1">#clear all the gradients
</span>    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="n">loss</span><span class="p">,</span> <span class="n">o_seven</span><span class="p">,</span> <span class="n">squared_diff</span><span class="p">,</span> <span class="n">diff</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">o3</span><span class="p">,</span> <span class="n">o2</span><span class="p">,</span> <span class="n">o1</span><span class="p">,</span> <span class="n">X_tensor</span><span class="p">]:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">p</span><span class="p">.</span><span class="nf">retain_grad</span><span class="p">()</span>


    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>


    <span class="c1">### Manual backpropagation
</span>    <span class="n">dL</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">do_seven</span> <span class="o">=</span> <span class="n">dL</span><span class="o">*</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">))</span>
    <span class="n">dsquared_diff</span> <span class="o">=</span> <span class="n">do_seven</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">)</span>
    <span class="n">ddiff</span> <span class="o">=</span> <span class="n">dsquared_diff</span> <span class="o">*</span> <span class="mi">2</span><span class="o">*</span><span class="n">diff</span>
    <span class="n">doutputs</span> <span class="o">=</span> <span class="n">ddiff</span><span class="o">*</span><span class="mi">1</span>

    <span class="n">do3</span> <span class="o">=</span> <span class="n">doutputs</span><span class="o">@</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">weight</span>
    <span class="n">dl2w</span> <span class="o">=</span> <span class="n">o3</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">doutputs</span>
    <span class="n">mo2</span> <span class="o">=</span> <span class="n">o2</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="n">do2</span> <span class="o">=</span> <span class="n">do3</span> <span class="o">*</span> <span class="n">mo2</span>
    <span class="n">do1</span> <span class="o">=</span> <span class="n">do2</span>
    <span class="n">dl0bias</span> <span class="o">=</span> <span class="n">do2</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">dl0w</span> <span class="o">=</span> <span class="n">X_tensor</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">do1</span>


<span class="c1"># #     cmp('dL', dL, loss)
#     cmp('do_seven', do_seven, o_seven)
#     cmp('dsquared_diff', dsquared_diff, squared_diff)
#     cmp('ddiff', ddiff, diff)
#     cmp('doutputs', doutputs, outputs)
#     cmp('do3', do3, o3)
#     cmp('dl2w', dl2w.T, model.layers[2].weight)
#     cmp('do2', do2, o2)
#     cmp('dl0bias', dl0bias, model.layers[0].bias)
#     cmp('do1', do1, o1)
#     cmp('dl0w', dl0w.T, model.layers[0].weight )
</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span>
        <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="n">dl0w</span><span class="p">.</span><span class="n">T</span>
        <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="n">dl0bias</span>
        <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="n">dl2w</span><span class="p">.</span><span class="n">T</span>


<span class="c1">#     optimizer.step()
</span>    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Plot results
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">X_tensor</span><span class="p">).</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">True</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Predicted</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Now, we implement LoRA.
Here we,</p>

<ul>
  <li>construct parameters A, B when B@A the resulting matrix size matches layer 0’s weight matrix</li>
  <li>modify forward pass by adding <code class="language-plaintext highlighter-rouge">lora_w</code></li>
  <li>continue other forward passes as they were previously,</li>
  <li>We don’t find the gradients for weights and don’t update those matrices, which is essentially freezing</li>
  <li>through intermediate gradients find the gradients for B and A and only update those weights.</li>
  <li>that’s it!!!!</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

<span class="c1"># Training loop
</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">100000</span>

<span class="c1"># lets only train only the lora parameters for layer0's weight
</span><span class="n">d</span><span class="p">,</span><span class="n">k</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span>
<span class="n">r</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">d</span><span class="p">,</span><span class="n">r</span><span class="p">)))</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="n">r</span><span class="p">,</span><span class="n">k</span><span class="p">)))</span>
<span class="n">scale</span> <span class="o">=</span> <span class="mi">2</span>


<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>

    <span class="c1"># Implement this forward to manually do the forward pass
</span>    <span class="n">o1</span> <span class="o">=</span> <span class="n">X_tensor</span> <span class="o">@</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span>
    <span class="c1">## add lora part for the layer0's model weight here
</span>
    <span class="n">lora_w</span> <span class="o">=</span> <span class="n">scale</span><span class="o">*</span><span class="n">B</span><span class="nd">@A</span>
    <span class="n">lora_o1</span> <span class="o">=</span> <span class="n">X_tensor</span> <span class="o">@</span> <span class="n">lora_w</span> <span class="c1"># size of B@A should match model.layers[0].weight.T i.e (1,20)
</span>    <span class="n">h</span> <span class="o">=</span> <span class="n">o1</span> <span class="o">+</span> <span class="n">lora_o1</span>

    <span class="n">o2</span> <span class="o">=</span> <span class="n">h</span> <span class="o">+</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">bias</span>
    <span class="n">o3</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">o2</span><span class="p">)</span> <span class="c1"># relu layer
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="n">o3</span> <span class="o">@</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span>


    <span class="n">diff</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">-</span> <span class="n">y_tensor</span>
    <span class="n">squared_diff</span> <span class="o">=</span> <span class="n">diff</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">o_seven</span> <span class="o">=</span> <span class="n">squared_diff</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">o_seven</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">)</span>

    <span class="c1">#Freeze all the model parameters
</span>    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">A</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">B</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="n">loss</span><span class="p">,</span> <span class="n">o_seven</span><span class="p">,</span> <span class="n">squared_diff</span><span class="p">,</span> <span class="n">diff</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">o3</span><span class="p">,</span> <span class="n">o2</span><span class="p">,</span> <span class="n">o1</span><span class="p">,</span> <span class="n">X_tensor</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">lora_o1</span><span class="p">,</span> <span class="n">lora_w</span><span class="p">]:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">p</span><span class="p">.</span><span class="nf">retain_grad</span><span class="p">()</span>


    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>


    <span class="c1">### Manual backpropagation
</span>    <span class="n">dL</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">do_seven</span> <span class="o">=</span> <span class="n">dL</span><span class="o">*</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">))</span>
    <span class="n">dsquared_diff</span> <span class="o">=</span> <span class="n">do_seven</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">)</span>
    <span class="n">ddiff</span> <span class="o">=</span> <span class="n">dsquared_diff</span> <span class="o">*</span> <span class="mi">2</span><span class="o">*</span><span class="n">diff</span>
    <span class="n">doutputs</span> <span class="o">=</span> <span class="n">ddiff</span><span class="o">*</span><span class="mi">1</span>

    <span class="n">do3</span> <span class="o">=</span> <span class="n">doutputs</span><span class="o">@</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">weight</span>
    <span class="c1"># We freeze this weight
#     dl2w = o3.T @ doutputs
</span>
    <span class="n">mo2</span> <span class="o">=</span> <span class="n">o2</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="n">do2</span> <span class="o">=</span> <span class="n">do3</span> <span class="o">*</span> <span class="n">mo2</span>

    <span class="n">dh</span> <span class="o">=</span> <span class="n">do2</span>
    <span class="n">do1</span> <span class="o">=</span> <span class="n">dh</span>
    <span class="n">dlora_o1</span> <span class="o">=</span> <span class="n">dh</span>

    <span class="n">dlora_w</span> <span class="o">=</span> <span class="n">X_tensor</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dlora_o1</span>
    <span class="n">dB</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">dlora_w</span><span class="nd">@A.T</span>
    <span class="n">dA</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">B</span><span class="p">.</span><span class="n">T</span><span class="nd">@dlora_w</span>

    <span class="c1"># And we freeze these weights too
#     dl0bias = do2.sum(0)
#     dl0w = X_tensor.T @ do1
</span>

<span class="c1">#     cmp('dL', dL, loss)
#     cmp('do_seven', do_seven, o_seven)
#     cmp('dsquared_diff', dsquared_diff, squared_diff)
#     cmp('ddiff', ddiff, diff)
#     cmp('doutputs', doutputs, outputs)
#     cmp('do3', do3, o3)
# #     cmp('dl2w', dl2w.T, model.layers[2].weight)
#     cmp('do2', do2, o2)
#     cmp('dh', dh, h)
#     cmp('do1', do1, o1)
#     cmp('dlora_o1', dlora_o1, lora_o1)
#     cmp('dlora_w', dlora_w, lora_w)
#     cmp('dB', dB, B)
#     cmp('dA', dA, A)
</span>



<span class="c1">#     cmp('dl0w', dl0w.T, model.layers[0].weight )
</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span>
        <span class="n">A</span><span class="p">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">dA</span>
        <span class="n">B</span><span class="p">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">dB</span>


<span class="c1">#     optimizer.step()
</span>    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Plot results
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">X_tensor</span><span class="p">).</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">True</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Predicted</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>


<!-- <article class="blog-post">
  <header>
    <h1>Lora</h1>
    <time datetime="2025-04-07T00:00:00+05:45">
      April 07, 2025
    </time>
  </header>

  <div class="post-content"><h3 id="lora">LoRA</h3>

<p>Main idea is to approximate the change in weights dW by the use of low-rank matrices</p>

<p>Eg: Usually the weight update is done by adding the change in weights dW to the original weight matrix W. dW is obtained through backpropagation, ex if W is 512 x 512 the parameter size of dW is 262,144.</p>

<p>In LoRA, we approximate that dW but by breaking down into two low rank matrices B @ A where B = matrix of size 512 x r and A = matrix of size r x 512,</p>

<p>previously if the forward pass was like this</p>

<p>out = X @ W</p>

<p>we change the forward pass:
out = X @ W + X @ B@A</p>

<p>we freeze all the other parameters (W in this case), and only find gradients for B,A and update only these weights.</p>

<p>First lets implement toy example that approximates sin function, we implement manual backpropagation so that It’ll be easier to understand what gets updated in our LoRA implementation.</p>

<h3 id="paper-findings-only-focus-on-highlighted-sentences">Paper findings (only focus on highlighted sentences)</h3>

<p><img src="/assets/images/2025-04-07-LoRA/lora1.png" alt="lora1" /></p>

<p><img src="/assets/images/2025-04-07-LoRA/lora2.png" alt="lora2" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># import torch
</span><span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Generate synthetic data (sine wave)
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">100</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Convert to PyTorch tensors
</span><span class="n">X_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Define a small MLP
</span><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="bp">True</span><span class="p">),</span>  <span class="c1"># 1 input → 10 hidden
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>    <span class="c1"># 10 hidden → 1 output
</span>        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Initialize model, loss, and optimizer
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">()</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>
<span class="c1"># optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
</span>
<span class="c1"># Training loop
</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

    <span class="c1">#Instead of this forward pass
#     outputs = model(X_tensor)
</span>
    <span class="c1"># Implement this forward to manually do the forward pass
</span>    <span class="n">o1</span> <span class="o">=</span> <span class="n">X_tensor</span> <span class="o">@</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span>
    <span class="n">o2</span> <span class="o">=</span> <span class="n">o1</span> <span class="o">+</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">bias</span>
    <span class="n">o3</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">o2</span><span class="p">)</span> <span class="c1"># relu layer
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="n">o3</span> <span class="o">@</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span>


    <span class="n">diff</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">-</span> <span class="n">y_tensor</span>
    <span class="n">squared_diff</span> <span class="o">=</span> <span class="n">diff</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">o_seven</span> <span class="o">=</span> <span class="n">squared_diff</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">o_seven</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">)</span>

    <span class="c1">#clear all the gradients
</span>    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="n">loss</span><span class="p">,</span> <span class="n">o_seven</span><span class="p">,</span> <span class="n">squared_diff</span><span class="p">,</span> <span class="n">diff</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">o3</span><span class="p">,</span> <span class="n">o2</span><span class="p">,</span> <span class="n">o1</span><span class="p">,</span> <span class="n">X_tensor</span><span class="p">]:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">p</span><span class="p">.</span><span class="nf">retain_grad</span><span class="p">()</span>


    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>


    <span class="c1">### Manual backpropagation
</span>    <span class="n">dL</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">do_seven</span> <span class="o">=</span> <span class="n">dL</span><span class="o">*</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">))</span>
    <span class="n">dsquared_diff</span> <span class="o">=</span> <span class="n">do_seven</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">)</span>
    <span class="n">ddiff</span> <span class="o">=</span> <span class="n">dsquared_diff</span> <span class="o">*</span> <span class="mi">2</span><span class="o">*</span><span class="n">diff</span>
    <span class="n">doutputs</span> <span class="o">=</span> <span class="n">ddiff</span><span class="o">*</span><span class="mi">1</span>

    <span class="n">do3</span> <span class="o">=</span> <span class="n">doutputs</span><span class="o">@</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">weight</span>
    <span class="n">dl2w</span> <span class="o">=</span> <span class="n">o3</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">doutputs</span>
    <span class="n">mo2</span> <span class="o">=</span> <span class="n">o2</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="n">do2</span> <span class="o">=</span> <span class="n">do3</span> <span class="o">*</span> <span class="n">mo2</span>
    <span class="n">do1</span> <span class="o">=</span> <span class="n">do2</span>
    <span class="n">dl0bias</span> <span class="o">=</span> <span class="n">do2</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">dl0w</span> <span class="o">=</span> <span class="n">X_tensor</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">do1</span>


<span class="c1"># #     cmp('dL', dL, loss)
#     cmp('do_seven', do_seven, o_seven)
#     cmp('dsquared_diff', dsquared_diff, squared_diff)
#     cmp('ddiff', ddiff, diff)
#     cmp('doutputs', doutputs, outputs)
#     cmp('do3', do3, o3)
#     cmp('dl2w', dl2w.T, model.layers[2].weight)
#     cmp('do2', do2, o2)
#     cmp('dl0bias', dl0bias, model.layers[0].bias)
#     cmp('do1', do1, o1)
#     cmp('dl0w', dl0w.T, model.layers[0].weight )
</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span>
        <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="n">dl0w</span><span class="p">.</span><span class="n">T</span>
        <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="n">dl0bias</span>
        <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="n">dl2w</span><span class="p">.</span><span class="n">T</span>


<span class="c1">#     optimizer.step()
</span>    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Plot results
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">X_tensor</span><span class="p">).</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">True</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Predicted</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Now, we implement LoRA.
Here we,</p>

<ul>
  <li>construct parameters A, B when B@A the resulting matrix size matches layer 0’s weight matrix</li>
  <li>modify forward pass by adding <code class="language-plaintext highlighter-rouge">lora_w</code></li>
  <li>continue other forward passes as they were previously,</li>
  <li>We don’t find the gradients for weights and don’t update those matrices, which is essentially freezing</li>
  <li>through intermediate gradients find the gradients for B and A and only update those weights.</li>
  <li>that’s it!!!!</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

<span class="c1"># Training loop
</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">100000</span>

<span class="c1"># lets only train only the lora parameters for layer0's weight
</span><span class="n">d</span><span class="p">,</span><span class="n">k</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span>
<span class="n">r</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">d</span><span class="p">,</span><span class="n">r</span><span class="p">)))</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="n">r</span><span class="p">,</span><span class="n">k</span><span class="p">)))</span>
<span class="n">scale</span> <span class="o">=</span> <span class="mi">2</span>


<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>

    <span class="c1"># Implement this forward to manually do the forward pass
</span>    <span class="n">o1</span> <span class="o">=</span> <span class="n">X_tensor</span> <span class="o">@</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span>
    <span class="c1">## add lora part for the layer0's model weight here
</span>
    <span class="n">lora_w</span> <span class="o">=</span> <span class="n">scale</span><span class="o">*</span><span class="n">B</span><span class="nd">@A</span>
    <span class="n">lora_o1</span> <span class="o">=</span> <span class="n">X_tensor</span> <span class="o">@</span> <span class="n">lora_w</span> <span class="c1"># size of B@A should match model.layers[0].weight.T i.e (1,20)
</span>    <span class="n">h</span> <span class="o">=</span> <span class="n">o1</span> <span class="o">+</span> <span class="n">lora_o1</span>

    <span class="n">o2</span> <span class="o">=</span> <span class="n">h</span> <span class="o">+</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">bias</span>
    <span class="n">o3</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">o2</span><span class="p">)</span> <span class="c1"># relu layer
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="n">o3</span> <span class="o">@</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span>


    <span class="n">diff</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">-</span> <span class="n">y_tensor</span>
    <span class="n">squared_diff</span> <span class="o">=</span> <span class="n">diff</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">o_seven</span> <span class="o">=</span> <span class="n">squared_diff</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">o_seven</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">)</span>

    <span class="c1">#Freeze all the model parameters
</span>    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">A</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">B</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="n">loss</span><span class="p">,</span> <span class="n">o_seven</span><span class="p">,</span> <span class="n">squared_diff</span><span class="p">,</span> <span class="n">diff</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">o3</span><span class="p">,</span> <span class="n">o2</span><span class="p">,</span> <span class="n">o1</span><span class="p">,</span> <span class="n">X_tensor</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">lora_o1</span><span class="p">,</span> <span class="n">lora_w</span><span class="p">]:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">p</span><span class="p">.</span><span class="nf">retain_grad</span><span class="p">()</span>


    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>


    <span class="c1">### Manual backpropagation
</span>    <span class="n">dL</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">do_seven</span> <span class="o">=</span> <span class="n">dL</span><span class="o">*</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">))</span>
    <span class="n">dsquared_diff</span> <span class="o">=</span> <span class="n">do_seven</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">)</span>
    <span class="n">ddiff</span> <span class="o">=</span> <span class="n">dsquared_diff</span> <span class="o">*</span> <span class="mi">2</span><span class="o">*</span><span class="n">diff</span>
    <span class="n">doutputs</span> <span class="o">=</span> <span class="n">ddiff</span><span class="o">*</span><span class="mi">1</span>

    <span class="n">do3</span> <span class="o">=</span> <span class="n">doutputs</span><span class="o">@</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">weight</span>
    <span class="c1"># We freeze this weight
#     dl2w = o3.T @ doutputs
</span>
    <span class="n">mo2</span> <span class="o">=</span> <span class="n">o2</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="n">do2</span> <span class="o">=</span> <span class="n">do3</span> <span class="o">*</span> <span class="n">mo2</span>

    <span class="n">dh</span> <span class="o">=</span> <span class="n">do2</span>
    <span class="n">do1</span> <span class="o">=</span> <span class="n">dh</span>
    <span class="n">dlora_o1</span> <span class="o">=</span> <span class="n">dh</span>

    <span class="n">dlora_w</span> <span class="o">=</span> <span class="n">X_tensor</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dlora_o1</span>
    <span class="n">dB</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">dlora_w</span><span class="nd">@A.T</span>
    <span class="n">dA</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">B</span><span class="p">.</span><span class="n">T</span><span class="nd">@dlora_w</span>

    <span class="c1"># And we freeze these weights too
#     dl0bias = do2.sum(0)
#     dl0w = X_tensor.T @ do1
</span>

<span class="c1">#     cmp('dL', dL, loss)
#     cmp('do_seven', do_seven, o_seven)
#     cmp('dsquared_diff', dsquared_diff, squared_diff)
#     cmp('ddiff', ddiff, diff)
#     cmp('doutputs', doutputs, outputs)
#     cmp('do3', do3, o3)
# #     cmp('dl2w', dl2w.T, model.layers[2].weight)
#     cmp('do2', do2, o2)
#     cmp('dh', dh, h)
#     cmp('do1', do1, o1)
#     cmp('dlora_o1', dlora_o1, lora_o1)
#     cmp('dlora_w', dlora_w, lora_w)
#     cmp('dB', dB, B)
#     cmp('dA', dA, A)
</span>



<span class="c1">#     cmp('dl0w', dl0w.T, model.layers[0].weight )
</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span>
        <span class="n">A</span><span class="p">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">dA</span>
        <span class="n">B</span><span class="p">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">dB</span>


<span class="c1">#     optimizer.step()
</span>    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Plot results
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">X_tensor</span><span class="p">).</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">True</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Predicted</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>
</div>

  
  <div class="tags">
    
    <span class="tag">architecture</span>
    
  </div>
  
</article> -->
</main>
  </body>
</html>
