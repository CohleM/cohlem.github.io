<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Pytorch - </title>
    <link rel="stylesheet" href="/assets/css/main.css" />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Zalando+Sans+Expanded:ital,wght@0,200..900;1,200..900&family=Zalando+Sans:ital,wght@0,200..900;1,200..900&display=swap"
      rel="stylesheet"
    />

    <!-- MathJax for LaTeX -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
          displayMath: [
            ["$$", "$$"],
            ["\\[", "\\]"],
          ],
          processEscapes: true,
          processEnvironments: true,
        },
        options: {
          skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"],
        },
      };
    </script>
    <script
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
      id="MathJax-script"
      async
    ></script>
  </head>
  <body>
    <nav>
      <a href="/">Home</a>
      <a href="/posts">Posts</a>
      <a href="/notes">Notes</a>
      <a href="/blogs">Blogs</a>
    </nav>

    <main><h1>Pytorch</h1>
<p>29 Jan 2025 - cohlem</p>

<h4 id="torchstacktensors-dim"><code class="language-plaintext highlighter-rouge">torch.stack(tensors, dim)</code></h4>

<p>stacks the tensors across dim</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#usage
# data has to be tensor
</span><span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">some_number</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>
</code></pre></div></div>

<h4 id="torchfrom_numpynumpy_array"><code class="language-plaintext highlighter-rouge">torch.from_numpy(numpy_array)</code></h4>

<p>shares the memory with the numpy_array but is tensor type</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="c1"># creates copy
</span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="c1"># shares memory
</span>
<span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">11</span>
<span class="n">c</span>

<span class="c1"># outputs: tensor([11,  2,  3])
</span></code></pre></div></div>

<h4 id="torchflatteninput-startend-1"><code class="language-plaintext highlighter-rouge">torch.flatten(input, start,end=-1)</code></h4>

<p>flattens the input from dim start to end (-1 by default)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span>
                  <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]])</span>
<span class="n">torch</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (2,2,2) --&gt; (2,2*2)
</span></code></pre></div></div>

<p>```tensor([[1, 2, 3, 4],
        [5, 6, 7, 8]])</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
#### torch.stack and torch.cat((tensors), dim)

torch.stack stacks tensors along new dim, whereas
torch.cat concatenates along that specific dim.

example:

```python
a = torch.randn(2,5,8,32)
b = torch.randn(2,1,8,32)

torch.cat((a,b), dim=1).shape

#outputs : torch.Size([2, 6, 8, 32])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">32</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">32</span><span class="p">)</span>

<span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">((</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">shape</span>

<span class="c1">#outputs: torch.Size([3, 2, 5, 8, 32])
</span></code></pre></div></div>

<p>For the past 2 years I’ve been involved in training and experimenting machine learning systems, mostly using third party packages such as sklearn, huggingface and so on. Sometimes the experiments become too specific and the abstraction provided by these packages become a bottleneck for the performance optimization. My research goal is to understand these bottlenecks in deep and write my own optimized code for hardware-specific optimization which enables resource efficient training or inference.</p>

<h3 id="while-training-make-to-take-care-of-these-things">While training, make to take care of these things</h3>

<ul>
  <li>are models using the same precision ? verify explicitly</li>
  <li>models will ouput different logits, when using parallelism vs when not using it even with the same prevision. Maybe there’s a difference because of the partial result when applying rowwiseParallel where the results are summed.
Example: without parallelism: <code class="language-plaintext highlighter-rouge">[ 3.7969, 7.7500, 3.3125, -1.0938, 6.9688]</code> with parallelism: <code class="language-plaintext highlighter-rouge">[ 3.9062, 7.8750, 3.2656, -0.8867, 6.6562]</code></li>
  <li>actions must be shifted by [1:] whereas states with [:-1] why?
because we pass the states to our model, it will output logits, logits are the (next token prediction), so shift the action by [1:], so for a state[i] it’s actual next token will be state[i+1], thus we construct action from the states but by shifting it by 1. Since, there’s actual next token available for the ending token -1 in the states, we shift the states by -1, because we don’t have any tokens available for that logits to find the logprobs.</li>
</ul>

<p>If you forget doing the step above, it might haunt you for hours/days, if you’re fine-tuning it should be faily easy to debug, because when you print the logprobs for the tokens, you should see something like below (its logprobs, not probability so don’t confuse, 0.00 represents the token is more probable), most tokens are probable because it’s already trained. If you’re pretraining from the beginning, it may be hard to debug.</p>

<p><code class="language-plaintext highlighter-rouge">tensor([-11.7500, -10.8125,  -8.6250, -11.2500, -11.3125, -12.5000,  -7.3125,
        -14.2500, -13.6875, -11.8125, -15.5000, -11.2500, -17.5000, -15.5000,
        -12.7500, -14.5000,  -8.5625, -12.8750, -17.7500, -20.0000, -13.3750,
        -18.5000, -22.2500, -12.3750, -13.8125, -13.3750, -13.3125, -15.6250,
        -11.5625, -14.8125, -10.1875, -14.1250, -18.6250, -17.0000, -13.6250,
        -21.7500, -14.2500,  -9.7500,  -3.7500, -10.1250,  -6.6875, -12.7500,
        -10.9375, -13.4375, -10.9375, -12.9375, -14.7500, -13.5000,  -0.5000,
        -12.5625, -10.3750,  -8.9375, -14.8750,  -6.5000, -15.3125, -13.5000,
         -8.9375, -15.0625, -31.0000, -39.2500, -21.1250, -18.5000,   0.0000,
         -1.1250,  -0.2500,   0.0000,  -0.2500,   0.0000,  -0.1250,   0.0000,
         -0.1250,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  -0.2500,
          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  -1.5000,
         -1.1250,   0.0000,  -0.6250,   0.0000,  -1.0000,  -0.3750,   0.0000,
          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
         -1.0000,  -0.8750,   0.0000,   0.0000,  -0.1250,  -0.5000,   0.0000,
          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  -0.1250,  -0.1250,
          0.0000,   0.0000,   0.0000,   0.0000,  -0.1250,   0.0000,   0.0000,
          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
         -0.1250,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
          0.0000,   0.0000,   0.0000,  -0.1250,   0.0000,  -0.1250,   0.0000,
          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
          0.0000,  -1.8750,   0.0000,  -0.1250,   0.0000,  -0.1250,  -0.3750,
          0.0000,  -1.2500,  -1.7500,  -0.8750,   0.0000,  -0.1250,   0.0000,
          0.0000,   0.0000,  -0.7500,   0.0000,   0.0000,  -0.1250,   0.0000,
          0.0000], device='cuda:0', dtype=torch.bfloat16)
</code></p>


<!-- <article class="blog-post">
  <header>
    <h1>Pytorch</h1>
    <time datetime="2025-01-29T00:00:00+05:45">
      January 29, 2025
    </time>
  </header>

  <div class="post-content"><h4 id="torchstacktensors-dim"><code class="language-plaintext highlighter-rouge">torch.stack(tensors, dim)</code></h4>

<p>stacks the tensors across dim</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#usage
# data has to be tensor
</span><span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">some_number</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>
</code></pre></div></div>

<h4 id="torchfrom_numpynumpy_array"><code class="language-plaintext highlighter-rouge">torch.from_numpy(numpy_array)</code></h4>

<p>shares the memory with the numpy_array but is tensor type</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="c1"># creates copy
</span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="c1"># shares memory
</span>
<span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">11</span>
<span class="n">c</span>

<span class="c1"># outputs: tensor([11,  2,  3])
</span></code></pre></div></div>

<h4 id="torchflatteninput-startend-1"><code class="language-plaintext highlighter-rouge">torch.flatten(input, start,end=-1)</code></h4>

<p>flattens the input from dim start to end (-1 by default)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span>
                  <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]])</span>
<span class="n">torch</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (2,2,2) --&gt; (2,2*2)
</span></code></pre></div></div>

<p>```tensor([[1, 2, 3, 4],
        [5, 6, 7, 8]])</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
#### torch.stack and torch.cat((tensors), dim)

torch.stack stacks tensors along new dim, whereas
torch.cat concatenates along that specific dim.

example:

```python
a = torch.randn(2,5,8,32)
b = torch.randn(2,1,8,32)

torch.cat((a,b), dim=1).shape

#outputs : torch.Size([2, 6, 8, 32])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">32</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">32</span><span class="p">)</span>

<span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">((</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">shape</span>

<span class="c1">#outputs: torch.Size([3, 2, 5, 8, 32])
</span></code></pre></div></div>

<p>For the past 2 years I’ve been involved in training and experimenting machine learning systems, mostly using third party packages such as sklearn, huggingface and so on. Sometimes the experiments become too specific and the abstraction provided by these packages become a bottleneck for the performance optimization. My research goal is to understand these bottlenecks in deep and write my own optimized code for hardware-specific optimization which enables resource efficient training or inference.</p>

<h3 id="while-training-make-to-take-care-of-these-things">While training, make to take care of these things</h3>

<ul>
  <li>are models using the same precision ? verify explicitly</li>
  <li>models will ouput different logits, when using parallelism vs when not using it even with the same prevision. Maybe there’s a difference because of the partial result when applying rowwiseParallel where the results are summed.
Example: without parallelism: <code class="language-plaintext highlighter-rouge">[ 3.7969, 7.7500, 3.3125, -1.0938, 6.9688]</code> with parallelism: <code class="language-plaintext highlighter-rouge">[ 3.9062, 7.8750, 3.2656, -0.8867, 6.6562]</code></li>
  <li>actions must be shifted by [1:] whereas states with [:-1] why?
because we pass the states to our model, it will output logits, logits are the (next token prediction), so shift the action by [1:], so for a state[i] it’s actual next token will be state[i+1], thus we construct action from the states but by shifting it by 1. Since, there’s actual next token available for the ending token -1 in the states, we shift the states by -1, because we don’t have any tokens available for that logits to find the logprobs.</li>
</ul>

<p>If you forget doing the step above, it might haunt you for hours/days, if you’re fine-tuning it should be faily easy to debug, because when you print the logprobs for the tokens, you should see something like below (its logprobs, not probability so don’t confuse, 0.00 represents the token is more probable), most tokens are probable because it’s already trained. If you’re pretraining from the beginning, it may be hard to debug.</p>

<p><code class="language-plaintext highlighter-rouge">tensor([-11.7500, -10.8125,  -8.6250, -11.2500, -11.3125, -12.5000,  -7.3125,
        -14.2500, -13.6875, -11.8125, -15.5000, -11.2500, -17.5000, -15.5000,
        -12.7500, -14.5000,  -8.5625, -12.8750, -17.7500, -20.0000, -13.3750,
        -18.5000, -22.2500, -12.3750, -13.8125, -13.3750, -13.3125, -15.6250,
        -11.5625, -14.8125, -10.1875, -14.1250, -18.6250, -17.0000, -13.6250,
        -21.7500, -14.2500,  -9.7500,  -3.7500, -10.1250,  -6.6875, -12.7500,
        -10.9375, -13.4375, -10.9375, -12.9375, -14.7500, -13.5000,  -0.5000,
        -12.5625, -10.3750,  -8.9375, -14.8750,  -6.5000, -15.3125, -13.5000,
         -8.9375, -15.0625, -31.0000, -39.2500, -21.1250, -18.5000,   0.0000,
         -1.1250,  -0.2500,   0.0000,  -0.2500,   0.0000,  -0.1250,   0.0000,
         -0.1250,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  -0.2500,
          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  -1.5000,
         -1.1250,   0.0000,  -0.6250,   0.0000,  -1.0000,  -0.3750,   0.0000,
          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
         -1.0000,  -0.8750,   0.0000,   0.0000,  -0.1250,  -0.5000,   0.0000,
          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  -0.1250,  -0.1250,
          0.0000,   0.0000,   0.0000,   0.0000,  -0.1250,   0.0000,   0.0000,
          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
         -0.1250,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
          0.0000,   0.0000,   0.0000,  -0.1250,   0.0000,  -0.1250,   0.0000,
          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
          0.0000,  -1.8750,   0.0000,  -0.1250,   0.0000,  -0.1250,  -0.3750,
          0.0000,  -1.2500,  -1.7500,  -0.8750,   0.0000,  -0.1250,   0.0000,
          0.0000,   0.0000,  -0.7500,   0.0000,   0.0000,  -0.1250,   0.0000,
          0.0000], device='cuda:0', dtype=torch.bfloat16)
</code></p>
</div>

  
  <div class="tags">
    
  </div>
  
</article> -->
</main>

    <footer>
      <p>&copy; 2025 </p>
    </footer>
  </body>
</html>
