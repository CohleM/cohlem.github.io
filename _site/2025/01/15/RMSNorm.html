<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Rmsnorm - </title>
    <link rel="stylesheet" href="/assets/css/main.css" />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Zalando+Sans+Expanded:ital,wght@0,200..900;1,200..900&family=Zalando+Sans:ital,wght@0,200..900;1,200..900&display=swap"
      rel="stylesheet"
    />

    <!-- MathJax for LaTeX -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
          displayMath: [
            ["$$", "$$"],
            ["\\[", "\\]"],
          ],
          processEscapes: true,
          processEnvironments: true,
        },
        options: {
          skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"],
        },
      };
    </script>
    <script
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
      id="MathJax-script"
      async
    ></script>
  </head>
  <body>
    <nav>
      <a href="/">Home</a>
      <a href="/posts">Posts</a>
      <a href="/notes">Notes</a>
      <a href="/blogs">Blogs</a>
    </nav>

    <main><h1>Rmsnorm</h1>
<p>15 Jan 2025 - cohlem</p>

<h3 id="recap-of-layernorm">Recap of LayerNorm</h3>

<p>let’s first recap by understanding why LayerNorm was used:</p>

<ul>
  <li>We needed to balance the distribution of inputs (internal covariance shift) i.e we want inputs to be roughly gaussian (mean 0, std 1), it not maintained it would result in zeroing out the gradients.</li>
  <li>output of some blocks (transformer block) may produce large values or very small values that would result in either exploding or vanishing gradient problem, in order to have stable training, we needed to have stable range for those outputs.</li>
</ul>

<p>In this <a href="https://arxiv.org/pdf/1910.07467">paper</a>, the authors raise concern about LayerNorm.</p>

<ul>
  <li>it’s computationally expensive
and they claim that</li>
  <li>re-centering (calculating the x - mean) has little impact for stabilization.</li>
</ul>

<p><img src="rms1.png" alt="rms1" /></p>

<p>they</p>

<ul>
  <li>completely get rid of mean statistic</li>
  <li>now there is less overhead because we don’t have to calculate one extra statistic.</li>
</ul>

<p>they also provide that RMSNorm is invariant (does not change) to inputs or weights matrices.</p>

<p><img src="rms2.png" alt="rms2" /></p>

<p>which indicates that the change in scale of input of weights doesn’t affect the RMSNorm.</p>


<!-- <article class="blog-post">
  <header>
    <h1>Rmsnorm</h1>
    <time datetime="2025-01-15T00:00:00+05:45">
      January 15, 2025
    </time>
  </header>

  <div class="post-content"><h3 id="recap-of-layernorm">Recap of LayerNorm</h3>

<p>let’s first recap by understanding why LayerNorm was used:</p>

<ul>
  <li>We needed to balance the distribution of inputs (internal covariance shift) i.e we want inputs to be roughly gaussian (mean 0, std 1), it not maintained it would result in zeroing out the gradients.</li>
  <li>output of some blocks (transformer block) may produce large values or very small values that would result in either exploding or vanishing gradient problem, in order to have stable training, we needed to have stable range for those outputs.</li>
</ul>

<p>In this <a href="https://arxiv.org/pdf/1910.07467">paper</a>, the authors raise concern about LayerNorm.</p>

<ul>
  <li>it’s computationally expensive
and they claim that</li>
  <li>re-centering (calculating the x - mean) has little impact for stabilization.</li>
</ul>

<p><img src="rms1.png" alt="rms1" /></p>

<p>they</p>

<ul>
  <li>completely get rid of mean statistic</li>
  <li>now there is less overhead because we don’t have to calculate one extra statistic.</li>
</ul>

<p>they also provide that RMSNorm is invariant (does not change) to inputs or weights matrices.</p>

<p><img src="rms2.png" alt="rms2" /></p>

<p>which indicates that the change in scale of input of weights doesn’t affect the RMSNorm.</p>
</div>

  
  <div class="tags">
    
    <span class="tag">optimization</span>
    
  </div>
  
</article> -->
</main>
  </body>
</html>
