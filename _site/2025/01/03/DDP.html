<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Ddp - </title>
    <link rel="stylesheet" href="/assets/css/main.css" />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Zalando+Sans+Expanded:ital,wght@0,200..900;1,200..900&family=Zalando+Sans:ital,wght@0,200..900;1,200..900&display=swap"
      rel="stylesheet"
    />

    <!-- MathJax for LaTeX -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
          displayMath: [
            ["$$", "$$"],
            ["\\[", "\\]"],
          ],
          processEscapes: true,
          processEnvironments: true,
        },
        options: {
          skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"],
        },
      };
    </script>
    <script
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
      id="MathJax-script"
      async
    ></script>
  </head>
  <body>
    <nav>
      <a href="/">Home</a>
      <a href="/posts">Posts</a>
      <a href="/notes">Notes</a>
      <a href="/blogs">Blogs</a>
    </nav>

    <main><h1>Ddp</h1>
<p>03 Jan 2025 - cohlem</p>

<p>When we have enough resources we would want to train our neural networks in parallel, the way to do this is to train our NN with different data (different batches of data) in each GPU in parallel. For instance, if we have 8X A100 we run 8 different batches of data on each A100 GPU.</p>

<p>The way to do this in pytorch is to use DDP (take a look into their docs)</p>

<p>The important thing to be careful about is that when we train our NN in different GPUs, each GPU calculates gradient and that gradient is averaged among all the all the gradients calculated from each GPUs and then deposited on each of the gpu and then we do the descent step.</p>

<p>Why do we do this ?</p>

<p>So that our weights become consistent and mathematically equivalent to when we train on 8x the batch size but on same GPU.</p>

<p>At first I was confused how would this be equivalent to the gradients when we trained it on single GPU but with 8x the batch size.</p>

<p>Hereâ€™s a simple mathematical formula.</p>

<p>FIGURE1
<img src="sub-notes/DDP/fig1.png" alt="DDP" /></p>

<p>But lets look at our manual backprogpagation which will help us understand better.</p>

<h3 id="forward-pass">Forward pass</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_embd</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># the dimensionality of the character embedding vectors
</span><span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">64</span> <span class="c1"># the number of neurons in the hidden layer of the MLP
</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Generator</span><span class="p">().</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">2147483647</span><span class="p">)</span> <span class="c1"># for reproducibility
</span><span class="n">C</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">),</span>            <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
<span class="c1"># Layer 1
</span><span class="n">W1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="n">n_embd</span> <span class="o">*</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">5</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span><span class="o">/</span><span class="p">((</span><span class="n">n_embd</span> <span class="o">*</span> <span class="n">block_size</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span>                        <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span> <span class="c1"># using b1 just for fun, it's useless because of BN
# Layer 2
</span><span class="n">W2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">),</span>          <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span>                      <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
<span class="c1"># BatchNorm parameters
</span><span class="n">bngain</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span><span class="o">*</span><span class="mf">0.1</span> <span class="o">+</span> <span class="mf">1.0</span>
<span class="n">bnbias</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span><span class="o">*</span><span class="mf">0.1</span>

<span class="c1"># Note: I am initializating many of these parameters in non-standard ways
# because sometimes initializating with e.g. all zeros could mask an incorrect
# implementation of the backward pass.
</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">C</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">bngain</span><span class="p">,</span> <span class="n">bnbias</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">))</span> <span class="c1"># number of parameters in total
</span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
  <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>



<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="c1"># a shorter variable also, for convenience
# construct a minibatch
</span><span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xtr</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
<span class="n">Xb</span><span class="p">,</span> <span class="n">Yb</span> <span class="o">=</span> <span class="n">Xtr</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="n">Ytr</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="c1"># batch X,Y
</span>

<span class="c1"># forward pass, "chunkated" into smaller steps that are possible to backward one at a time
</span>
<span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">Xb</span><span class="p">]</span> <span class="c1"># embed the characters into vectors
</span><span class="n">embcat</span> <span class="o">=</span> <span class="n">emb</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">emb</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># concatenate the vectors
# Linear layer 1
</span><span class="n">hprebn</span> <span class="o">=</span> <span class="n">embcat</span> <span class="o">@</span> <span class="n">W1</span> <span class="o">+</span> <span class="n">b1</span> <span class="c1"># hidden layer pre-activation
# BatchNorm layer
</span><span class="n">bnmeani</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">n</span><span class="o">*</span><span class="n">hprebn</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">bndiff</span> <span class="o">=</span> <span class="n">hprebn</span> <span class="o">-</span> <span class="n">bnmeani</span>
<span class="n">bndiff2</span> <span class="o">=</span> <span class="n">bndiff</span><span class="o">**</span><span class="mi">2</span>
<span class="n">bnvar</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">bndiff2</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># note: Bessel's correction (dividing by n-1, not n)
</span><span class="n">bnvar_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">bnvar</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span><span class="o">**-</span><span class="mf">0.5</span>
<span class="n">bnraw</span> <span class="o">=</span> <span class="n">bndiff</span> <span class="o">*</span> <span class="n">bnvar_inv</span>
<span class="n">hpreact</span> <span class="o">=</span> <span class="n">bngain</span> <span class="o">*</span> <span class="n">bnraw</span> <span class="o">+</span> <span class="n">bnbias</span>
<span class="c1"># Non-linearity
</span><span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">hpreact</span><span class="p">)</span> <span class="c1"># hidden layer
# Linear layer 2
</span><span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">W2</span> <span class="o">+</span> <span class="n">b2</span> <span class="c1"># output layer
# cross entropy loss (same as F.cross_entropy(logits, Yb))
</span><span class="n">logit_maxes</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">values</span>
<span class="n">norm_logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">-</span> <span class="n">logit_maxes</span> <span class="c1"># subtract max for numerical stability
</span><span class="n">counts</span> <span class="o">=</span> <span class="n">norm_logits</span><span class="p">.</span><span class="nf">exp</span><span class="p">()</span>
<span class="n">counts_sum</span> <span class="o">=</span> <span class="n">counts</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">counts_sum_inv</span> <span class="o">=</span> <span class="n">counts_sum</span><span class="o">**-</span><span class="mi">1</span> <span class="c1"># if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...
</span><span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">*</span> <span class="n">counts_sum_inv</span>
<span class="n">logprobs</span> <span class="o">=</span> <span class="n">probs</span><span class="p">.</span><span class="nf">log</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">logprobs</span><span class="p">[</span><span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">Yb</span><span class="p">].</span><span class="nf">mean</span><span class="p">()</span>

<span class="c1"># PyTorch backward pass
</span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
  <span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">None</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="p">[</span><span class="n">logprobs</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">counts</span><span class="p">,</span> <span class="n">counts_sum</span><span class="p">,</span> <span class="n">counts_sum_inv</span><span class="p">,</span> <span class="c1"># afaik there is no cleaner way
</span>          <span class="n">norm_logits</span><span class="p">,</span> <span class="n">logit_maxes</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">hpreact</span><span class="p">,</span> <span class="n">bnraw</span><span class="p">,</span>
         <span class="n">bnvar_inv</span><span class="p">,</span> <span class="n">bnvar</span><span class="p">,</span> <span class="n">bndiff2</span><span class="p">,</span> <span class="n">bndiff</span><span class="p">,</span> <span class="n">hprebn</span><span class="p">,</span> <span class="n">bnmeani</span><span class="p">,</span>
         <span class="n">embcat</span><span class="p">,</span> <span class="n">emb</span><span class="p">]:</span>
  <span class="n">t</span><span class="p">.</span><span class="nf">retain_grad</span><span class="p">()</span>
<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="n">loss</span>
</code></pre></div></div>

<h3 id="backprop">Backprop</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Exercise 1: backprop through the whole thing manually,
# backpropagating through exactly all of the variables
# as they are defined in the forward pass above, one by one
</span>
<span class="c1"># -----------------
# YOUR CODE HERE :)
</span><span class="n">dlogprobs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">logprobs</span><span class="p">)</span>
<span class="n">dlogprobs</span><span class="p">[</span><span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">Yb</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">logprobs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># 1 &lt;=== look here
</span>
<span class="n">dprobs</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">probs</span><span class="p">)</span><span class="o">*</span><span class="n">dlogprobs</span> <span class="c1"># 2
</span><span class="n">dcounts_sum_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">dprobs</span><span class="o">*</span><span class="n">counts</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">dcounts</span> <span class="o">=</span> <span class="n">dprobs</span> <span class="o">*</span> <span class="n">counts_sum_inv</span>
<span class="n">dcounts_sum</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span><span class="o">*</span><span class="p">((</span><span class="n">counts_sum</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">))</span><span class="o">*</span><span class="n">dcounts_sum_inv</span>
<span class="n">dcounts</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">counts_sum</span><span class="p">)</span><span class="o">*</span><span class="n">dcounts_sum</span>
<span class="n">dnorm_logits</span> <span class="o">=</span> <span class="n">norm_logits</span><span class="p">.</span><span class="nf">exp</span><span class="p">()</span><span class="o">*</span><span class="n">dcounts</span>
<span class="n">dlogit_maxes</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="o">*</span><span class="n">dnorm_logits</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">dlogits</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">*</span><span class="n">dnorm_logits</span><span class="p">)</span>

<span class="n">dlogits</span> <span class="o">+=</span> <span class="n">F</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">logits</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">indices</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">logits</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">*</span><span class="n">dlogit_maxes</span>
<span class="n">db2</span> <span class="o">=</span> <span class="p">(</span><span class="n">dlogits</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">logits</span><span class="p">)).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dh</span> <span class="o">=</span> <span class="n">dlogits</span> <span class="o">@</span> <span class="n">W2</span><span class="p">.</span><span class="n">T</span>
<span class="n">dW2</span> <span class="o">=</span> <span class="n">h</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dlogits</span>
<span class="n">dhpreact</span> <span class="o">=</span> <span class="n">dh</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">h</span><span class="o">**</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="n">dbnbias</span> <span class="o">=</span> <span class="p">(</span><span class="n">dhpreact</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">bnraw</span><span class="p">)).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">dbngain</span> <span class="o">=</span> <span class="p">(</span><span class="n">dhpreact</span><span class="o">*</span><span class="n">bnraw</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">bnraw</span><span class="p">)).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">dbnraw</span> <span class="o">=</span> <span class="n">dhpreact</span><span class="o">*</span><span class="n">bngain</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">bnraw</span><span class="p">)</span>
<span class="n">dbnvar_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">dbnraw</span><span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">bndiff</span><span class="p">)</span> <span class="o">*</span> <span class="n">bndiff</span><span class="p">)).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">dbndiff</span> <span class="o">=</span> <span class="p">(</span><span class="n">dbnraw</span><span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">bndiff</span><span class="p">)</span> <span class="o">*</span> <span class="n">bnvar_inv</span><span class="p">))</span>
<span class="n">dbnvar</span> <span class="o">=</span> <span class="n">dbnvar_inv</span><span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span><span class="o">*</span><span class="p">(((</span><span class="n">bnvar</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">))</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">))</span>
<span class="n">dbndiff2</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">)</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">bndiff2</span><span class="p">)</span> <span class="o">*</span> <span class="n">dbnvar</span>
<span class="n">dbndiff</span> <span class="o">+=</span> <span class="n">dbndiff2</span><span class="o">*</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">bndiff</span><span class="p">)</span>
<span class="n">dhprebn</span> <span class="o">=</span> <span class="n">dbndiff</span><span class="o">*</span><span class="mf">1.0</span>
<span class="n">dbnmeani</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">hprebn</span><span class="p">)</span><span class="o">*-</span><span class="mf">1.0</span><span class="o">*</span><span class="n">dbndiff</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">dhprebn</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">hprebn</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="n">dbnmeani</span>
<span class="n">db1</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">dhprebn</span><span class="p">)</span><span class="o">*</span><span class="n">dhprebn</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dembcat</span> <span class="o">=</span> <span class="n">dhprebn</span> <span class="o">@</span> <span class="n">W1</span><span class="p">.</span><span class="n">T</span>
<span class="n">dW1</span> <span class="o">=</span> <span class="n">embcat</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dhprebn</span>
<span class="n">demb</span> <span class="o">=</span> <span class="n">dembcat</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">emb</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">emb</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">emb</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">dC</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">Xb</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">Xb</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">dC</span><span class="p">[</span><span class="n">Xb</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">demb</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span>
<span class="c1">#         print(demb[i,j].shape)
# -----------------
</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">logprobs</span><span class="sh">'</span><span class="p">,</span> <span class="n">dlogprobs</span><span class="p">,</span> <span class="n">logprobs</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">probs</span><span class="sh">'</span><span class="p">,</span> <span class="n">dprobs</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">counts_sum_inv</span><span class="sh">'</span><span class="p">,</span> <span class="n">dcounts_sum_inv</span><span class="p">,</span> <span class="n">counts_sum_inv</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">counts_sum</span><span class="sh">'</span><span class="p">,</span> <span class="n">dcounts_sum</span><span class="p">,</span> <span class="n">counts_sum</span><span class="p">)</span>

<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">counts</span><span class="sh">'</span><span class="p">,</span> <span class="n">dcounts</span><span class="p">,</span> <span class="n">counts</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">norm_logits</span><span class="sh">'</span><span class="p">,</span> <span class="n">dnorm_logits</span><span class="p">,</span> <span class="n">norm_logits</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">logit_maxes</span><span class="sh">'</span><span class="p">,</span> <span class="n">dlogit_maxes</span><span class="p">,</span> <span class="n">logit_maxes</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">logits</span><span class="sh">'</span><span class="p">,</span> <span class="n">dlogits</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">h</span><span class="sh">'</span><span class="p">,</span> <span class="n">dh</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">W2</span><span class="sh">'</span><span class="p">,</span> <span class="n">dW2</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">b2</span><span class="sh">'</span><span class="p">,</span> <span class="n">db2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">hpreact</span><span class="sh">'</span><span class="p">,</span> <span class="n">dhpreact</span><span class="p">,</span> <span class="n">hpreact</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">bngain</span><span class="sh">'</span><span class="p">,</span> <span class="n">dbngain</span><span class="p">,</span> <span class="n">bngain</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">bnbias</span><span class="sh">'</span><span class="p">,</span> <span class="n">dbnbias</span><span class="p">,</span> <span class="n">bnbias</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">bnraw</span><span class="sh">'</span><span class="p">,</span> <span class="n">dbnraw</span><span class="p">,</span> <span class="n">bnraw</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">bnvar_inv</span><span class="sh">'</span><span class="p">,</span> <span class="n">dbnvar_inv</span><span class="p">,</span> <span class="n">bnvar_inv</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">bnvar</span><span class="sh">'</span><span class="p">,</span> <span class="n">dbnvar</span><span class="p">,</span> <span class="n">bnvar</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">bndiff2</span><span class="sh">'</span><span class="p">,</span> <span class="n">dbndiff2</span><span class="p">,</span> <span class="n">bndiff2</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">bndiff</span><span class="sh">'</span><span class="p">,</span> <span class="n">dbndiff</span><span class="p">,</span> <span class="n">bndiff</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">bnmeani</span><span class="sh">'</span><span class="p">,</span> <span class="n">dbnmeani</span><span class="p">,</span> <span class="n">bnmeani</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">hprebn</span><span class="sh">'</span><span class="p">,</span> <span class="n">dhprebn</span><span class="p">,</span> <span class="n">hprebn</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">embcat</span><span class="sh">'</span><span class="p">,</span> <span class="n">dembcat</span><span class="p">,</span> <span class="n">embcat</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">W1</span><span class="sh">'</span><span class="p">,</span> <span class="n">dW1</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">b1</span><span class="sh">'</span><span class="p">,</span> <span class="n">db1</span><span class="p">,</span> <span class="n">b1</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">emb</span><span class="sh">'</span><span class="p">,</span> <span class="n">demb</span><span class="p">,</span> <span class="n">emb</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">,</span> <span class="n">dC</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
</code></pre></div></div>

<p>lets look at our last line of code in forward pass</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">logprobs</span><span class="p">[</span><span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">Yb</span><span class="p">].</span><span class="nf">mean</span><span class="p">()</span>
</code></pre></div></div>

<p>what we do is simply the cross entropy i.e normalize using softmax function and pluck out the log probability from the output tokenâ€™s index.</p>

<p>and calculate itâ€™s derivative here in second line of code in the backward pass like this.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dlogprobs</span><span class="p">[</span><span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">Yb</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">logprobs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># 1 &lt;=== look here
</span></code></pre></div></div>

<p>what weâ€™re doing is simply calculating the average over all the batches and that average is deposited in each element in [range(n), Yb] .</p>

<p>Why do we do the average?
dL/dlogprobs = 1 (which comes from dL/dL) x d(dlogprobs[range(n), Yb])/dlogprobs</p>

<p>since the elements in this range [range(n), Yb]) is averaged, each element will get (1/total_number) x itself</p>

<p>so the local derivative of (1/total_number) x iteself w.r.t itself is 1/total_number which will be now deposited into the elements of dlogprobs[range(n), Yb]</p>

<p>letâ€™s stop right here and this matches our first equation in FIGURE1, and if we do this operation for 8 times more batches on single GPU we get the second equation in FIGURE1. If we look at this code, we can see that we are only increasing the n value here.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dlogprobs</span><span class="p">[</span><span class="nf">range</span><span class="p">(</span><span class="mi">8</span><span class="o">*</span><span class="n">n</span><span class="p">),</span> <span class="n">Yb</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">logprobs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># 1 &lt;=== look here
</span></code></pre></div></div>

<p>So from this itâ€™s safe to say that training on multiple GPUs and averaging their gradients is same as training on single GPU with 8 times more batches.</p>


<!-- <article class="blog-post">
  <header>
    <h1>Ddp</h1>
    <time datetime="2025-01-03T00:00:00+05:45">
      January 03, 2025
    </time>
  </header>

  <div class="post-content"><p>When we have enough resources we would want to train our neural networks in parallel, the way to do this is to train our NN with different data (different batches of data) in each GPU in parallel. For instance, if we have 8X A100 we run 8 different batches of data on each A100 GPU.</p>

<p>The way to do this in pytorch is to use DDP (take a look into their docs)</p>

<p>The important thing to be careful about is that when we train our NN in different GPUs, each GPU calculates gradient and that gradient is averaged among all the all the gradients calculated from each GPUs and then deposited on each of the gpu and then we do the descent step.</p>

<p>Why do we do this ?</p>

<p>So that our weights become consistent and mathematically equivalent to when we train on 8x the batch size but on same GPU.</p>

<p>At first I was confused how would this be equivalent to the gradients when we trained it on single GPU but with 8x the batch size.</p>

<p>Hereâ€™s a simple mathematical formula.</p>

<p>FIGURE1
<img src="sub-notes/DDP/fig1.png" alt="DDP" /></p>

<p>But lets look at our manual backprogpagation which will help us understand better.</p>

<h3 id="forward-pass">Forward pass</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_embd</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># the dimensionality of the character embedding vectors
</span><span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">64</span> <span class="c1"># the number of neurons in the hidden layer of the MLP
</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Generator</span><span class="p">().</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">2147483647</span><span class="p">)</span> <span class="c1"># for reproducibility
</span><span class="n">C</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">),</span>            <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
<span class="c1"># Layer 1
</span><span class="n">W1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="n">n_embd</span> <span class="o">*</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">5</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span><span class="o">/</span><span class="p">((</span><span class="n">n_embd</span> <span class="o">*</span> <span class="n">block_size</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span>                        <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span> <span class="c1"># using b1 just for fun, it's useless because of BN
# Layer 2
</span><span class="n">W2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">),</span>          <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span>                      <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
<span class="c1"># BatchNorm parameters
</span><span class="n">bngain</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span><span class="o">*</span><span class="mf">0.1</span> <span class="o">+</span> <span class="mf">1.0</span>
<span class="n">bnbias</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span><span class="o">*</span><span class="mf">0.1</span>

<span class="c1"># Note: I am initializating many of these parameters in non-standard ways
# because sometimes initializating with e.g. all zeros could mask an incorrect
# implementation of the backward pass.
</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">C</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">bngain</span><span class="p">,</span> <span class="n">bnbias</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">))</span> <span class="c1"># number of parameters in total
</span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
  <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>



<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="c1"># a shorter variable also, for convenience
# construct a minibatch
</span><span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xtr</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
<span class="n">Xb</span><span class="p">,</span> <span class="n">Yb</span> <span class="o">=</span> <span class="n">Xtr</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="n">Ytr</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="c1"># batch X,Y
</span>

<span class="c1"># forward pass, "chunkated" into smaller steps that are possible to backward one at a time
</span>
<span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">Xb</span><span class="p">]</span> <span class="c1"># embed the characters into vectors
</span><span class="n">embcat</span> <span class="o">=</span> <span class="n">emb</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">emb</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># concatenate the vectors
# Linear layer 1
</span><span class="n">hprebn</span> <span class="o">=</span> <span class="n">embcat</span> <span class="o">@</span> <span class="n">W1</span> <span class="o">+</span> <span class="n">b1</span> <span class="c1"># hidden layer pre-activation
# BatchNorm layer
</span><span class="n">bnmeani</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">n</span><span class="o">*</span><span class="n">hprebn</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">bndiff</span> <span class="o">=</span> <span class="n">hprebn</span> <span class="o">-</span> <span class="n">bnmeani</span>
<span class="n">bndiff2</span> <span class="o">=</span> <span class="n">bndiff</span><span class="o">**</span><span class="mi">2</span>
<span class="n">bnvar</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">bndiff2</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># note: Bessel's correction (dividing by n-1, not n)
</span><span class="n">bnvar_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">bnvar</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span><span class="o">**-</span><span class="mf">0.5</span>
<span class="n">bnraw</span> <span class="o">=</span> <span class="n">bndiff</span> <span class="o">*</span> <span class="n">bnvar_inv</span>
<span class="n">hpreact</span> <span class="o">=</span> <span class="n">bngain</span> <span class="o">*</span> <span class="n">bnraw</span> <span class="o">+</span> <span class="n">bnbias</span>
<span class="c1"># Non-linearity
</span><span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">hpreact</span><span class="p">)</span> <span class="c1"># hidden layer
# Linear layer 2
</span><span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">W2</span> <span class="o">+</span> <span class="n">b2</span> <span class="c1"># output layer
# cross entropy loss (same as F.cross_entropy(logits, Yb))
</span><span class="n">logit_maxes</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">values</span>
<span class="n">norm_logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">-</span> <span class="n">logit_maxes</span> <span class="c1"># subtract max for numerical stability
</span><span class="n">counts</span> <span class="o">=</span> <span class="n">norm_logits</span><span class="p">.</span><span class="nf">exp</span><span class="p">()</span>
<span class="n">counts_sum</span> <span class="o">=</span> <span class="n">counts</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">counts_sum_inv</span> <span class="o">=</span> <span class="n">counts_sum</span><span class="o">**-</span><span class="mi">1</span> <span class="c1"># if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...
</span><span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">*</span> <span class="n">counts_sum_inv</span>
<span class="n">logprobs</span> <span class="o">=</span> <span class="n">probs</span><span class="p">.</span><span class="nf">log</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">logprobs</span><span class="p">[</span><span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">Yb</span><span class="p">].</span><span class="nf">mean</span><span class="p">()</span>

<span class="c1"># PyTorch backward pass
</span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
  <span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">None</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="p">[</span><span class="n">logprobs</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">counts</span><span class="p">,</span> <span class="n">counts_sum</span><span class="p">,</span> <span class="n">counts_sum_inv</span><span class="p">,</span> <span class="c1"># afaik there is no cleaner way
</span>          <span class="n">norm_logits</span><span class="p">,</span> <span class="n">logit_maxes</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">hpreact</span><span class="p">,</span> <span class="n">bnraw</span><span class="p">,</span>
         <span class="n">bnvar_inv</span><span class="p">,</span> <span class="n">bnvar</span><span class="p">,</span> <span class="n">bndiff2</span><span class="p">,</span> <span class="n">bndiff</span><span class="p">,</span> <span class="n">hprebn</span><span class="p">,</span> <span class="n">bnmeani</span><span class="p">,</span>
         <span class="n">embcat</span><span class="p">,</span> <span class="n">emb</span><span class="p">]:</span>
  <span class="n">t</span><span class="p">.</span><span class="nf">retain_grad</span><span class="p">()</span>
<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="n">loss</span>
</code></pre></div></div>

<h3 id="backprop">Backprop</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Exercise 1: backprop through the whole thing manually,
# backpropagating through exactly all of the variables
# as they are defined in the forward pass above, one by one
</span>
<span class="c1"># -----------------
# YOUR CODE HERE :)
</span><span class="n">dlogprobs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">logprobs</span><span class="p">)</span>
<span class="n">dlogprobs</span><span class="p">[</span><span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">Yb</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">logprobs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># 1 &lt;=== look here
</span>
<span class="n">dprobs</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">probs</span><span class="p">)</span><span class="o">*</span><span class="n">dlogprobs</span> <span class="c1"># 2
</span><span class="n">dcounts_sum_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">dprobs</span><span class="o">*</span><span class="n">counts</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">dcounts</span> <span class="o">=</span> <span class="n">dprobs</span> <span class="o">*</span> <span class="n">counts_sum_inv</span>
<span class="n">dcounts_sum</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span><span class="o">*</span><span class="p">((</span><span class="n">counts_sum</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">))</span><span class="o">*</span><span class="n">dcounts_sum_inv</span>
<span class="n">dcounts</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">counts_sum</span><span class="p">)</span><span class="o">*</span><span class="n">dcounts_sum</span>
<span class="n">dnorm_logits</span> <span class="o">=</span> <span class="n">norm_logits</span><span class="p">.</span><span class="nf">exp</span><span class="p">()</span><span class="o">*</span><span class="n">dcounts</span>
<span class="n">dlogit_maxes</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="o">*</span><span class="n">dnorm_logits</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">dlogits</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">*</span><span class="n">dnorm_logits</span><span class="p">)</span>

<span class="n">dlogits</span> <span class="o">+=</span> <span class="n">F</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">logits</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">indices</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">logits</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">*</span><span class="n">dlogit_maxes</span>
<span class="n">db2</span> <span class="o">=</span> <span class="p">(</span><span class="n">dlogits</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">logits</span><span class="p">)).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dh</span> <span class="o">=</span> <span class="n">dlogits</span> <span class="o">@</span> <span class="n">W2</span><span class="p">.</span><span class="n">T</span>
<span class="n">dW2</span> <span class="o">=</span> <span class="n">h</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dlogits</span>
<span class="n">dhpreact</span> <span class="o">=</span> <span class="n">dh</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">h</span><span class="o">**</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="n">dbnbias</span> <span class="o">=</span> <span class="p">(</span><span class="n">dhpreact</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">bnraw</span><span class="p">)).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">dbngain</span> <span class="o">=</span> <span class="p">(</span><span class="n">dhpreact</span><span class="o">*</span><span class="n">bnraw</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">bnraw</span><span class="p">)).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">dbnraw</span> <span class="o">=</span> <span class="n">dhpreact</span><span class="o">*</span><span class="n">bngain</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">bnraw</span><span class="p">)</span>
<span class="n">dbnvar_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">dbnraw</span><span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">bndiff</span><span class="p">)</span> <span class="o">*</span> <span class="n">bndiff</span><span class="p">)).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">dbndiff</span> <span class="o">=</span> <span class="p">(</span><span class="n">dbnraw</span><span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">bndiff</span><span class="p">)</span> <span class="o">*</span> <span class="n">bnvar_inv</span><span class="p">))</span>
<span class="n">dbnvar</span> <span class="o">=</span> <span class="n">dbnvar_inv</span><span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span><span class="o">*</span><span class="p">(((</span><span class="n">bnvar</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">))</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">))</span>
<span class="n">dbndiff2</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">)</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">bndiff2</span><span class="p">)</span> <span class="o">*</span> <span class="n">dbnvar</span>
<span class="n">dbndiff</span> <span class="o">+=</span> <span class="n">dbndiff2</span><span class="o">*</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">bndiff</span><span class="p">)</span>
<span class="n">dhprebn</span> <span class="o">=</span> <span class="n">dbndiff</span><span class="o">*</span><span class="mf">1.0</span>
<span class="n">dbnmeani</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">hprebn</span><span class="p">)</span><span class="o">*-</span><span class="mf">1.0</span><span class="o">*</span><span class="n">dbndiff</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">dhprebn</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">hprebn</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="n">dbnmeani</span>
<span class="n">db1</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">dhprebn</span><span class="p">)</span><span class="o">*</span><span class="n">dhprebn</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dembcat</span> <span class="o">=</span> <span class="n">dhprebn</span> <span class="o">@</span> <span class="n">W1</span><span class="p">.</span><span class="n">T</span>
<span class="n">dW1</span> <span class="o">=</span> <span class="n">embcat</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dhprebn</span>
<span class="n">demb</span> <span class="o">=</span> <span class="n">dembcat</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">emb</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">emb</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">emb</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">dC</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">Xb</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">Xb</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">dC</span><span class="p">[</span><span class="n">Xb</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">demb</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span>
<span class="c1">#         print(demb[i,j].shape)
# -----------------
</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">logprobs</span><span class="sh">'</span><span class="p">,</span> <span class="n">dlogprobs</span><span class="p">,</span> <span class="n">logprobs</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">probs</span><span class="sh">'</span><span class="p">,</span> <span class="n">dprobs</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">counts_sum_inv</span><span class="sh">'</span><span class="p">,</span> <span class="n">dcounts_sum_inv</span><span class="p">,</span> <span class="n">counts_sum_inv</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">counts_sum</span><span class="sh">'</span><span class="p">,</span> <span class="n">dcounts_sum</span><span class="p">,</span> <span class="n">counts_sum</span><span class="p">)</span>

<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">counts</span><span class="sh">'</span><span class="p">,</span> <span class="n">dcounts</span><span class="p">,</span> <span class="n">counts</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">norm_logits</span><span class="sh">'</span><span class="p">,</span> <span class="n">dnorm_logits</span><span class="p">,</span> <span class="n">norm_logits</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">logit_maxes</span><span class="sh">'</span><span class="p">,</span> <span class="n">dlogit_maxes</span><span class="p">,</span> <span class="n">logit_maxes</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">logits</span><span class="sh">'</span><span class="p">,</span> <span class="n">dlogits</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">h</span><span class="sh">'</span><span class="p">,</span> <span class="n">dh</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">W2</span><span class="sh">'</span><span class="p">,</span> <span class="n">dW2</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">b2</span><span class="sh">'</span><span class="p">,</span> <span class="n">db2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">hpreact</span><span class="sh">'</span><span class="p">,</span> <span class="n">dhpreact</span><span class="p">,</span> <span class="n">hpreact</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">bngain</span><span class="sh">'</span><span class="p">,</span> <span class="n">dbngain</span><span class="p">,</span> <span class="n">bngain</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">bnbias</span><span class="sh">'</span><span class="p">,</span> <span class="n">dbnbias</span><span class="p">,</span> <span class="n">bnbias</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">bnraw</span><span class="sh">'</span><span class="p">,</span> <span class="n">dbnraw</span><span class="p">,</span> <span class="n">bnraw</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">bnvar_inv</span><span class="sh">'</span><span class="p">,</span> <span class="n">dbnvar_inv</span><span class="p">,</span> <span class="n">bnvar_inv</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">bnvar</span><span class="sh">'</span><span class="p">,</span> <span class="n">dbnvar</span><span class="p">,</span> <span class="n">bnvar</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">bndiff2</span><span class="sh">'</span><span class="p">,</span> <span class="n">dbndiff2</span><span class="p">,</span> <span class="n">bndiff2</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">bndiff</span><span class="sh">'</span><span class="p">,</span> <span class="n">dbndiff</span><span class="p">,</span> <span class="n">bndiff</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">bnmeani</span><span class="sh">'</span><span class="p">,</span> <span class="n">dbnmeani</span><span class="p">,</span> <span class="n">bnmeani</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">hprebn</span><span class="sh">'</span><span class="p">,</span> <span class="n">dhprebn</span><span class="p">,</span> <span class="n">hprebn</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">embcat</span><span class="sh">'</span><span class="p">,</span> <span class="n">dembcat</span><span class="p">,</span> <span class="n">embcat</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">W1</span><span class="sh">'</span><span class="p">,</span> <span class="n">dW1</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">b1</span><span class="sh">'</span><span class="p">,</span> <span class="n">db1</span><span class="p">,</span> <span class="n">b1</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">emb</span><span class="sh">'</span><span class="p">,</span> <span class="n">demb</span><span class="p">,</span> <span class="n">emb</span><span class="p">)</span>
<span class="nf">cmp</span><span class="p">(</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">,</span> <span class="n">dC</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
</code></pre></div></div>

<p>lets look at our last line of code in forward pass</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">logprobs</span><span class="p">[</span><span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">Yb</span><span class="p">].</span><span class="nf">mean</span><span class="p">()</span>
</code></pre></div></div>

<p>what we do is simply the cross entropy i.e normalize using softmax function and pluck out the log probability from the output tokenâ€™s index.</p>

<p>and calculate itâ€™s derivative here in second line of code in the backward pass like this.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dlogprobs</span><span class="p">[</span><span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">Yb</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">logprobs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># 1 &lt;=== look here
</span></code></pre></div></div>

<p>what weâ€™re doing is simply calculating the average over all the batches and that average is deposited in each element in [range(n), Yb] .</p>

<p>Why do we do the average?
dL/dlogprobs = 1 (which comes from dL/dL) x d(dlogprobs[range(n), Yb])/dlogprobs</p>

<p>since the elements in this range [range(n), Yb]) is averaged, each element will get (1/total_number) x itself</p>

<p>so the local derivative of (1/total_number) x iteself w.r.t itself is 1/total_number which will be now deposited into the elements of dlogprobs[range(n), Yb]</p>

<p>letâ€™s stop right here and this matches our first equation in FIGURE1, and if we do this operation for 8 times more batches on single GPU we get the second equation in FIGURE1. If we look at this code, we can see that we are only increasing the n value here.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dlogprobs</span><span class="p">[</span><span class="nf">range</span><span class="p">(</span><span class="mi">8</span><span class="o">*</span><span class="n">n</span><span class="p">),</span> <span class="n">Yb</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">logprobs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># 1 &lt;=== look here
</span></code></pre></div></div>

<p>So from this itâ€™s safe to say that training on multiple GPUs and averaging their gradients is same as training on single GPU with 8 times more batches.</p>
</div>

  
  <div class="tags">
    
  </div>
  
</article> -->
</main>

    <footer>
      <p>&copy; 2025 </p>
    </footer>
  </body>
</html>
