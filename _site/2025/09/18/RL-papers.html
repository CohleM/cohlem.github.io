<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Rl Papers - </title>
    <link rel="stylesheet" href="/assets/css/main.css" />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Zalando+Sans+Expanded:ital,wght@0,200..900;1,200..900&family=Zalando+Sans:ital,wght@0,200..900;1,200..900&display=swap"
      rel="stylesheet"
    />

    <!-- MathJax for LaTeX -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
          displayMath: [
            ["$$", "$$"],
            ["\\[", "\\]"],
          ],
          processEscapes: true,
          processEnvironments: true,
        },
        options: {
          skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"],
        },
      };
    </script>
    <script
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
      id="MathJax-script"
      async
    ></script>
  </head>
  <body>
    <nav>
      <a href="/">Home</a>
      <a href="/posts">Posts</a>
      <a href="/notes">Notes</a>
      <a href="/blogs">Blogs</a>
    </nav>

    <main><h1>Rl Papers</h1>
<p>18 Sep 2025 - cohlem</p>

<h3 id="skywork-open-reasoner-1-technical-report">Skywork Open Reasoner 1 Technical Report</h3>

<h4 id="multi-stage-training">Multi-stage training</h4>

<p>First, set context_length = T, train, wait for convergence, at some point afterwards set context_length=2T</p>

<p>There seems to be no problem with truncated responses, cause actor learns to produce accurate but shorter responses.
<img src="skywork-tr.png" alt="skywork-tr" /></p>

<h4 id="high-temperature-sampling">High temperature Sampling</h4>

<p>temperatue 1.0 provides more room for exploration, whereas lower temperature might cause early entropy collapse.
<img src="temp-sampling.png" alt="temp-sampling.png" /></p>

<h4 id="adaptive-entropy-control">Adaptive Entropy Control</h4>

<p>turn off entropy coefficient when entropy is already greater than target entropy else, increase it by del.
<img src="entropy_control.png" alt="entropy_control" /></p>

<h3 id="acereason-nemotron-11-advancing-math-and-code-reasoning-through-sft-and-rl-synergy">AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy</h3>

<h4 id="which-one-results-in-better-performance--large-no-of-unique-prompts-or-more-responses-per-prompt">Which one results in better performance ? large no. of unique prompts or more responses per prompt.</h4>

<p>Generally, large no. of unique prompts is better, but at some point obtaining those become difficult, afterwards, we can include more responses per prompt in our dataset. More epochs helps as well.</p>

<p><img src="ace-nemo.png" alt="ace-nemo" /></p>

<h4 id="is-stage-1-necessary">Is stage-1 necessary?</h4>

<p><img src="overall-trn.png" alt="overall-trn" />
stage-1 training seems to be necessary cause, it allows the model to compress it’s reasoning at early stage, not doing so results in accuracy that plateaus see fig below.</p>

<p><img src="stage-1.png" alt="stage-1" /></p>

<h4 id="exploration-vs-exploitation-ie-temperature-controlled-entropy">Exploration vs exploitation (i.e temperature controlled entropy)</h4>

<p>extremely high temp or low temp cause reward to dampen, thus noisy training.</p>

<p><img src="temp-exp.png" alt="temp-exp" /></p>

<h4 id="with-overlong-filtering-or-without-overlong-filtering">with overlong filtering or without overlong filtering</h4>

<p>with overlong filtering = don’t use the trajectory for reward calculation
without overlong filtering = use the trajectory and provide negative reward for truncated responses</p>

<p><img src="overlong.png" alt="overlong" /></p>

<p>overlong filtering helps during first stages (i.e when context lengths are small) otherwise, when context length is sufficient, it doesn’t help.</p>

<h4 id="performance-with-increase-in-k-value-passk">Performance with increase in K value (pass@K)</h4>

<p>The performance gain decreases for some benchmarks(AIME25) as we increase K. We can find answer within the SFT model if the K is sufficiently large</p>

<p><img src="passk.png" alt="passk" /></p>

<h3 id="something">Something</h3>

<h4 id="which-ones-better-group-normalization-grpo-batch-normalization-use-same-equation-as-grpo-but-taken-mean-and-std-between-whole-rollout-batch">which one’s better? group normalization (GRPO), batch normalization (use same equation as GRPO but taken mean and std between whole rollout batch)</h4>

<ul>
  <li>group normalization seems to be better.</li>
</ul>

<h4 id="do-we-need-std-normalization">Do we need std normalization?</h4>

<p>when problems are too easy for actor, the rewards are almost all correct, the std will be low, division by low std results in high advanrages causing higher gradients (sometimes exploding gradients) but when problems are medium-hard there’s not much of a difference between normalizing by std or just simply subtracting the mean from the reward. (i.e no use of std)</p>

<p><img src="std-use.png" alt="std-use" /></p>

<h4 id="reward-shaping-works-best-when-use-divide-by-global-std">Reward shaping works best when use divide by global std</h4>

<p><img src="global-std.png" alt="global-std" /></p>

<h4 id="liteppo">LitePPO</h4>

<p>simply using token-level loss i.e sum all tokens loss then divide by total tokens in the batch and using division by global std improves the overall performance.</p>

<h1 id="for-sop">FOR SOP</h1>

<h3 id="writing-zero">Writing zero</h3>

<p>train pairwise GenRM,</p>

<ol>
  <li>Gather pairwise data</li>
  <li>pass claude code with those pairwise data, generate critiques and final answer.</li>
  <li>remove the trajectories which has contradictory answer or doesn’t match the ground truth (we already know the real preferences)</li>
  <li>do cold-start SFT with those trajectories.</li>
  <li>do GRPO training with Reward = 1 if Sc &gt; Sr</li>
</ol>

<p>afterwards,
do BRPO (modified GRPO) i.e a model will try to generate best answers.
exp do rollout of 8 exp, select one randomly as a reference data, pass current answer and the reference answer to GenRM, GenRM provides scores S1,S2. If R = 1 if S1 &gt; S2 else -1. thats it.</p>

<h3 id="next-story-generation">Next story generation</h3>

<p>simply train the reasoning model not the generation model. i.e learn to generate intermediate p_hat, do this using GRPO, reward given by this formula ![[Screenshot 2025-10-13 at 11.48.44 AM.png]]</p>

<p>I is given by this formula
![[Screenshot 2025-10-13 at 11.49.14 AM.png]]</p>

<p>reminder, we need the ground truth answer for training, and PPL simply mean that we find the avg of negative log likelihood of all the tokens and exponentiate that avg and put - sign in front.</p>

<p><a href="https://arxiv.org/pdf/2503.19618">Beyond Verifiable Rewards: Scaling Reinforcement Learning for Language Models to Unverifiable Data</a></p>

<p>https://chatgpt.com/share/68ecbaf2-35e4-800c-8dcf-db3b183741ed</p>

<p>Given a prompt x, the intuitive role of chain-of-thought is such that it makes the marginal likelihood of the ground truth answer a∗ higher. As such, we can interpret chain-of- thought as a latent variable and formulate the optimization of chain-of-thought as latent variable modeling
![[Screenshot 2025-10-13 at 2.23.40 PM.png]]</p>

<p>the first term promotes CoT chains that produce best answer, whereas the second term promotes answer given the CoT chain, third term is simply the KL term.</p>

<p>So it’s kind of like providing the implicit rewards using RL + SFT</p>

<p>in</p>


<!-- <article class="blog-post">
  <header>
    <h1>Rl Papers</h1>
    <time datetime="2025-09-18T00:00:00+05:45">
      September 18, 2025
    </time>
  </header>

  <div class="post-content"><h3 id="skywork-open-reasoner-1-technical-report">Skywork Open Reasoner 1 Technical Report</h3>

<h4 id="multi-stage-training">Multi-stage training</h4>

<p>First, set context_length = T, train, wait for convergence, at some point afterwards set context_length=2T</p>

<p>There seems to be no problem with truncated responses, cause actor learns to produce accurate but shorter responses.
<img src="skywork-tr.png" alt="skywork-tr" /></p>

<h4 id="high-temperature-sampling">High temperature Sampling</h4>

<p>temperatue 1.0 provides more room for exploration, whereas lower temperature might cause early entropy collapse.
<img src="temp-sampling.png" alt="temp-sampling.png" /></p>

<h4 id="adaptive-entropy-control">Adaptive Entropy Control</h4>

<p>turn off entropy coefficient when entropy is already greater than target entropy else, increase it by del.
<img src="entropy_control.png" alt="entropy_control" /></p>

<h3 id="acereason-nemotron-11-advancing-math-and-code-reasoning-through-sft-and-rl-synergy">AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy</h3>

<h4 id="which-one-results-in-better-performance--large-no-of-unique-prompts-or-more-responses-per-prompt">Which one results in better performance ? large no. of unique prompts or more responses per prompt.</h4>

<p>Generally, large no. of unique prompts is better, but at some point obtaining those become difficult, afterwards, we can include more responses per prompt in our dataset. More epochs helps as well.</p>

<p><img src="ace-nemo.png" alt="ace-nemo" /></p>

<h4 id="is-stage-1-necessary">Is stage-1 necessary?</h4>

<p><img src="overall-trn.png" alt="overall-trn" />
stage-1 training seems to be necessary cause, it allows the model to compress it’s reasoning at early stage, not doing so results in accuracy that plateaus see fig below.</p>

<p><img src="stage-1.png" alt="stage-1" /></p>

<h4 id="exploration-vs-exploitation-ie-temperature-controlled-entropy">Exploration vs exploitation (i.e temperature controlled entropy)</h4>

<p>extremely high temp or low temp cause reward to dampen, thus noisy training.</p>

<p><img src="temp-exp.png" alt="temp-exp" /></p>

<h4 id="with-overlong-filtering-or-without-overlong-filtering">with overlong filtering or without overlong filtering</h4>

<p>with overlong filtering = don’t use the trajectory for reward calculation
without overlong filtering = use the trajectory and provide negative reward for truncated responses</p>

<p><img src="overlong.png" alt="overlong" /></p>

<p>overlong filtering helps during first stages (i.e when context lengths are small) otherwise, when context length is sufficient, it doesn’t help.</p>

<h4 id="performance-with-increase-in-k-value-passk">Performance with increase in K value (pass@K)</h4>

<p>The performance gain decreases for some benchmarks(AIME25) as we increase K. We can find answer within the SFT model if the K is sufficiently large</p>

<p><img src="passk.png" alt="passk" /></p>

<h3 id="something">Something</h3>

<h4 id="which-ones-better-group-normalization-grpo-batch-normalization-use-same-equation-as-grpo-but-taken-mean-and-std-between-whole-rollout-batch">which one’s better? group normalization (GRPO), batch normalization (use same equation as GRPO but taken mean and std between whole rollout batch)</h4>

<ul>
  <li>group normalization seems to be better.</li>
</ul>

<h4 id="do-we-need-std-normalization">Do we need std normalization?</h4>

<p>when problems are too easy for actor, the rewards are almost all correct, the std will be low, division by low std results in high advanrages causing higher gradients (sometimes exploding gradients) but when problems are medium-hard there’s not much of a difference between normalizing by std or just simply subtracting the mean from the reward. (i.e no use of std)</p>

<p><img src="std-use.png" alt="std-use" /></p>

<h4 id="reward-shaping-works-best-when-use-divide-by-global-std">Reward shaping works best when use divide by global std</h4>

<p><img src="global-std.png" alt="global-std" /></p>

<h4 id="liteppo">LitePPO</h4>

<p>simply using token-level loss i.e sum all tokens loss then divide by total tokens in the batch and using division by global std improves the overall performance.</p>

<h1 id="for-sop">FOR SOP</h1>

<h3 id="writing-zero">Writing zero</h3>

<p>train pairwise GenRM,</p>

<ol>
  <li>Gather pairwise data</li>
  <li>pass claude code with those pairwise data, generate critiques and final answer.</li>
  <li>remove the trajectories which has contradictory answer or doesn’t match the ground truth (we already know the real preferences)</li>
  <li>do cold-start SFT with those trajectories.</li>
  <li>do GRPO training with Reward = 1 if Sc &gt; Sr</li>
</ol>

<p>afterwards,
do BRPO (modified GRPO) i.e a model will try to generate best answers.
exp do rollout of 8 exp, select one randomly as a reference data, pass current answer and the reference answer to GenRM, GenRM provides scores S1,S2. If R = 1 if S1 &gt; S2 else -1. thats it.</p>

<h3 id="next-story-generation">Next story generation</h3>

<p>simply train the reasoning model not the generation model. i.e learn to generate intermediate p_hat, do this using GRPO, reward given by this formula ![[Screenshot 2025-10-13 at 11.48.44 AM.png]]</p>

<p>I is given by this formula
![[Screenshot 2025-10-13 at 11.49.14 AM.png]]</p>

<p>reminder, we need the ground truth answer for training, and PPL simply mean that we find the avg of negative log likelihood of all the tokens and exponentiate that avg and put - sign in front.</p>

<p><a href="https://arxiv.org/pdf/2503.19618">Beyond Verifiable Rewards: Scaling Reinforcement Learning for Language Models to Unverifiable Data</a></p>

<p>https://chatgpt.com/share/68ecbaf2-35e4-800c-8dcf-db3b183741ed</p>

<p>Given a prompt x, the intuitive role of chain-of-thought is such that it makes the marginal likelihood of the ground truth answer a∗ higher. As such, we can interpret chain-of- thought as a latent variable and formulate the optimization of chain-of-thought as latent variable modeling
![[Screenshot 2025-10-13 at 2.23.40 PM.png]]</p>

<p>the first term promotes CoT chains that produce best answer, whereas the second term promotes answer given the CoT chain, third term is simply the KL term.</p>

<p>So it’s kind of like providing the implicit rewards using RL + SFT</p>

<p>in</p>
</div>

  
  <div class="tags">
    
  </div>
  
</article> -->
</main>
  </body>
</html>
