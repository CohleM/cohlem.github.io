<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Distributed Rl Training Step - </title>
    <link rel="stylesheet" href="/assets/css/main.css" />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Zalando+Sans+Expanded:ital,wght@0,200..900;1,200..900&family=Zalando+Sans:ital,wght@0,200..900;1,200..900&display=swap"
      rel="stylesheet"
    />

    <!-- MathJax for LaTeX -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
          displayMath: [
            ["$$", "$$"],
            ["\\[", "\\]"],
          ],
          processEscapes: true,
          processEnvironments: true,
        },
        options: {
          skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"],
        },
      };
    </script>
    <script
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
      id="MathJax-script"
      async
    ></script>
  </head>
  <body>
    <nav>
      <a href="/">Home</a>
      <a href="/posts">Posts</a>
      <a href="/notes">Notes</a>
      <a href="/blogs">Blogs</a>
    </nav>

    <main><h1>Distributed Rl Training Step</h1>
<p>01 Sep 2025 - cohlem</p>

<p>I was learning how we can do distributed RL training, saw karpathy posting <a href="https://x.com/karpathy/status/1952076108565991588">this</a> and thought why not make a complete blog about what I learned so here it is.</p>

<p>The end goal of this blog is to explain clearly how to do distributed RL training, right now it contains explanations about fundamentals of distributed training, such as data parallelism, model parallelism, and tensor parallelism. Consider this as a part 1, where in the next blog I’ll be explaining how we apply the techniques learned in the blog.</p>

<p>Doing RL is simply not easy from the resource/computation standpoint because there isn’t just one model/optimizer states and gradients that we need to care of but the same model can be an actor, critic, reference policy and so on and we need to store them in our GPU. Doing RL even with 0.5B model with decent sequence length takes a LOT of GPU memory, which led me to understand how GPUs can work in a distributed manner (the part which I had always been ignoring).</p>

<h2 id="manual-distributed-data-parallel">Manual Distributed Data Parallel</h2>

<p>The simplest form of distributed training is distributed data parallel where <strong>each GPU</strong> has a copy of a model, but each GPU has different batches of data. Lets say we have batch_size = 64 and 2 GPUs, and a model. The model’s parameters will be replicated across both GPU. GPU1 will get (1, batch_size/2) and GPU2 will get (batch_size/2 , batch_size) data, each GPU will process on its own, find it’s own loss and find its own gradients and then sum those gradients over both the GPUs and average it (if needed), the averaged gradients will be replicated across both the GPUs and each will perform optimizer.step() on its own and end up with same set of parameters.</p>

<p>Let’s do manual distributed data parallel to see how it works under the hood with the help of a toy model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="c1">## ----------------- SETUP ----------------------------
</span><span class="n">rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">RANK</span><span class="sh">"</span><span class="p">])</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">WORLD_SIZE</span><span class="sh">"</span><span class="p">])</span> <span class="c1"># total number of GPUs
</span><span class="n">local_rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">LOCAL_RANK</span><span class="sh">"</span><span class="p">])</span> <span class="c1"># unique id for this GPU
</span><span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="n">backend</span> <span class="o">=</span> <span class="sh">'</span><span class="s">nccl</span><span class="sh">'</span> <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">else</span> <span class="sh">'</span><span class="s">gloo</span><span class="sh">'</span>
<span class="n">dist</span><span class="p">.</span><span class="nf">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>  <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>
<span class="c1">#torch.cuda.set_device(local_rank) # turn this on if you have GPU
## ----------------- SETUP ----------------------------
</span>
<span class="n">inp</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">9.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">9.</span><span class="p">,</span><span class="mf">17.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)]</span>


<span class="k">class</span> <span class="nc">ToyModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">net1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">5.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="n">self</span><span class="p">.</span><span class="n">net1</span><span class="p">.</span><span class="n">T</span>

<span class="n">local_data</span> <span class="o">=</span> <span class="n">inp</span><span class="p">[</span><span class="n">local_rank</span><span class="p">]</span> <span class="c1"># this way different gpus will get different data
</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">ToyModel</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">predicted</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">local_data</span><span class="p">)</span>
    <span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">predicted</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">/=</span> <span class="n">predicted</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s"> RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span> <span class="si">}</span><span class="se">\n</span><span class="s"> parameters</span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">net1</span><span class="p">.</span><span class="n">grad</span><span class="si">}</span><span class="s"> </span><span class="sh">'</span><span class="p">)</span>
    <span class="n">dist</span><span class="p">.</span><span class="nf">all_reduce</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">net1</span><span class="p">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="p">.</span><span class="n">ReduceOp</span><span class="p">.</span><span class="n">SUM</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">net1</span><span class="p">.</span><span class="n">grad</span> <span class="o">/=</span> <span class="n">world_size</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s"> AFTER ALL REDUCE </span><span class="se">\n</span><span class="s"> RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span> <span class="si">}</span><span class="se">\n</span><span class="s"> parameters</span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">net1</span><span class="p">.</span><span class="n">grad</span><span class="si">}</span><span class="s"> </span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># do the optimizer.step
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
<span class="n">dist</span><span class="p">.</span><span class="nf">destroy_process_group</span><span class="p">()</span>
</code></pre></div></div>

<p>run the code with <code class="language-plaintext highlighter-rouge">torchrun --node_rank=0 --nproc_per_node=2 --nnodes=1 --standalone test2.py</code>
<code class="language-plaintext highlighter-rouge">torchrun</code> will start different processes equals to the number specified in –nproc_per_node i.e 2 and this line <code class="language-plaintext highlighter-rouge">dist.init_process_group(backend=backend,  world_size=world_size)</code> will create a process group where ranks in the groups communicate with each other by using collectives (explained below). Each process will also get one rank, we can access it’s rank using <code class="language-plaintext highlighter-rouge">local_rank = int(os.environ["LOCAL_RANK"])</code> and can assign gpu to each process using <code class="language-plaintext highlighter-rouge">torch.cuda.set_device(local_rank)</code> for now I’ll be commenting our because, we can carry out these simple codes in CPU.</p>

<p>Now, we define our model structure, it’s a simply matrix multiplication (similar to nn.Linear with bias=False) and assign different data to different ranks using <code class="language-plaintext highlighter-rouge">local_data = inp[local_rank]</code>, we process different data on different process calculate loss, do backward and do
` dist.all_reduce(model.net1.grad, op=dist.ReduceOp.SUM)` it will first collect all the model.net1.grad from all the GPUs and sum it (provided by dist.ReduceOp.SUM) and transfer the summed result to each GPUs (thus the name reduce). The operation all_reduce should be intuitive by now.</p>

<p>This all_reduce is a collective operation performed by the <code class="language-plaintext highlighter-rouge">backend</code> i.e either ‘nccl’ or ‘gloo’.
‘nccl’ is mostly used for GPUs whereas ‘gloo’ can run in CPUs as well. Under the hood these backend implement the logic on how these collectives operations need to be implemented. all_reduce is one of many collective operations provided by the backend. Please late a look at the picture below to understand how these work.
<img src="/assets/images/2025-09-01-Distributed-RL-training-step/coll1.png" alt="coll1.png" />
<img src="/assets/images/2025-09-01-Distributed-RL-training-step/coll2.png" alt="coll2" />
Image source: <a href="https://docs.pytorch.org/tutorials/intermediate/dist_tuto.html">Writing Distributed Applications with PyTorch</a></p>

<p>You can take a look at this <a href="https://docs.pytorch.org/tutorials/intermediate/dist_tuto.html">source</a> if you want to understand how these collectives work under the hood.</p>

<h3 id="pytorchs-distributeddataparallel">Pytorch’s DistributedDataParallel</h3>

<p>The same function can be carried out by wrapping our model within pytorch’s DistributedDataParallel and we don’t have to call all_reduce manually, pytorch automatically takes care of that when we call loss.backward().</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>
<span class="c1">## ----------------- SETUP ----------------------------
</span><span class="n">rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">RANK</span><span class="sh">"</span><span class="p">])</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">WORLD_SIZE</span><span class="sh">"</span><span class="p">])</span>
<span class="n">local_rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">LOCAL_RANK</span><span class="sh">"</span><span class="p">])</span>
<span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="n">backend</span> <span class="o">=</span> <span class="sh">'</span><span class="s">nccl</span><span class="sh">'</span> <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">else</span> <span class="sh">'</span><span class="s">gloo</span><span class="sh">'</span>
<span class="n">dist</span><span class="p">.</span><span class="nf">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>  <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>
<span class="c1">## ----------------- SETUP ----------------------------
</span>
<span class="n">inp</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">9.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">9.</span><span class="p">,</span><span class="mf">17.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)]</span>


<span class="k">class</span> <span class="nc">ToyModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">net1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">5.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="n">self</span><span class="p">.</span><span class="n">net1</span><span class="p">.</span><span class="n">T</span>

<span class="n">local_data</span> <span class="o">=</span> <span class="n">inp</span><span class="p">[</span><span class="n">local_rank</span><span class="p">]</span> <span class="c1"># will get data based on its rank
</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">ToyModel</span><span class="p">()</span>
<span class="n">ddp_model</span> <span class="o">=</span> <span class="nc">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">ddp_model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">predicted</span> <span class="o">=</span> <span class="nf">ddp_model</span><span class="p">(</span><span class="n">local_data</span><span class="p">)</span>
    <span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="n">local_data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">predicted</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">/=</span> <span class="n">predicted</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span>

    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="c1"># dist.all_reduce(model.net1.grad, op=dist.ReduceOp.SUM)
</span>    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s"> AFTER ALL REDUCE </span><span class="se">\n</span><span class="s"> RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span> <span class="si">}</span><span class="se">\n</span><span class="s"> parameters</span><span class="se">\n</span><span class="s"> </span><span class="si">{</span> <span class="n">ddp_model</span><span class="p">.</span><span class="n">module</span><span class="p">.</span><span class="n">net1</span><span class="p">.</span><span class="n">grad</span><span class="si">}</span><span class="s"> </span><span class="sh">'</span><span class="p">)</span>
    <span class="c1"># do the optimizer.step
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
<span class="n">dist</span><span class="p">.</span><span class="nf">destroy_process_group</span><span class="p">()</span>
</code></pre></div></div>

<p>We need to take care of saving and loading the models which can be referred to <a href="https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html">this link</a></p>

<h3 id="fully-sharded-data-parallel-fsdp">Fully Sharded Data Parallel (FSDP)</h3>

<p>Let’s first remind ourselves about the drawbacks of DDP. DDP has a copy of whole model weights, gradients, and optimizer in each GPU. When models get bigger it won’t completely fit in One GPU’s memory. We don’t want to only fit model’s weight on a GPU, we need to fit it’s gradients, and the biiiiiig optimizer which contains twice the parameters as model’s weight.</p>

<p>How about distributing parameters on different GPUs and then gathering the required parameters when doing the specific operation. This is exactly what Fully Sharded Data Parallel does.</p>

<p>Let’s consider we have a layer that has a flat weight matrix and 3 GPUs, we divide 1/3 of the weights into each GPU as shown in the picture below and pass different batch to each GPU so that it can perform computation simultaneously.</p>

<p><img src="/assets/images/2025-09-01-Distributed-RL-training-step/fsdp1.png" alt="fsdp1" />
Image source: <a href="https://www.youtube.com/watch?v=6pVn6khIgiI">The SECRET Behind ChatGPT’s Training That Nobody Talks About | FSDP Explained</a></p>

<p>Data reaches the GPU but GPU doesn’t have the full weight to complete the computation, so each GPU calls all_gather collective to collect weights from all the GPUs. After all gather all 3 GPUs will have the full weight <code class="language-plaintext highlighter-rouge">[0..1024]</code> and it can do the computation as shown in the figure below.
<img src="/assets/images/2025-09-01-Distributed-RL-training-step/fsdp2.png" alt="fsdp2" />
After computing the activations it instantly frees up the GPU memory. Next, is the case of backward pass.</p>

<p>Similar to image above, it first all gathers the weights and compute the gradients, and reduce scatter so that each GPU only has gradients for their respective shard and then when we do optimizer.step() it will only update it’s respective shard of optimizer. See how we shard weights, gradients, and optimizer across GPUs but only gather weights, this helps a lot. BUT a GPU should have memory to at least store model’s full weights.
If you want to visually see how FSDP work, this <a href="https://www.youtube.com/watch?v=6pVn6khIgiI">video</a> might be very helpful.</p>

<p>We’ll see how FSDP shards weight in detail with code below along with Tensor Parallelism explained in the next section.</p>

<h3 id="tensor-parallelism">Tensor Parallelism</h3>

<p>Now let’s shard the computation. Previously, we sharded weights but in order to do computation (matrix multiplication), we all gathered all the parameters in each GPU then do the computation. Lets say we have very big parameter in our <code class="language-plaintext highlighter-rouge">net1</code>in the above example, so we shard that parameter into available GPUs, but when we do the .forward() call all weights need to be gathered inorder to do computation, so there’s a requirement that <code class="language-plaintext highlighter-rouge">net1</code>’s weight <strong>MUST</strong> fit in each GPU, what if it’s too big that it can’t fit in a single GPU. There’s a solution: Tensor Parallelism</p>

<p>As the name suggest we shard the matrix either row-wise or column-wise. Before understanding it would be easier to understand Tensor Parallelism if we remember some properties of matrix multiplication.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A = [  1   2   3   4
       5   6   7   8
       9  10  11  12
      13  14  15  16 ]

X = [ 1  0  0  0
      0  1  0  0 ]

# Shard A column wise, suppose we have 2 GPUS each half is stored in each GPU

A1 = [  1   2
        5   6
        9  10
       13  14 ]


A2 = [  3   4
        7   8
       11  12
       15  16 ]

X @ A1 = [ 1   2
           5   6 ]
X @ A2 = [ 3   4
           7   8 ]

# we can simply concatenate the two halves to get the full output

X @ A = [ 1   2    3   4
	      5   6   7   8 ]


</code></pre></div></div>

<p>Similarly, when we shard A row-wise,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
X @ A = [X @ A1 + X @ A2] # NOTE: "A" should be sharded such that its halves can be multiplied with X.
</code></pre></div></div>

<p>Let’s understand this with the help of this famous as well as the crucial implementation.
This is a sample code for FFN used in almost all the models these days. Since we apply gating mechanism here instead of using two weights we use three weights.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># forward in the FeedForward layer
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">w2</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">silu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">w1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="nf">w3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>

<p>Assume we have 2 GPUs. We have three weights.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>w1's shape = w3's shape =  (768, 1024)
w2's shape = (1024, 768)
</code></pre></div></div>

<p>We first shard w1 and w3’s weight column-wise and w2 row wise</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>GPU1 will get w1's (768, 512). # call it gpu1_w1
GPU1 will get w3's (768, 512). # call it gpu1_w3 and so on.
GPU1 will get w2's (512, 768)

------------------------------
GPU2 will get w1's (768, 512).
GPU2 will get w3's (768, 512).
GPU2 will get w2's (512, 768)

</code></pre></div></div>

<p>we have input x of shape (3,768) that is replicated across both the GPUs.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In GPU1, we can perform these operations below simultaneously,

a = x @ gpu1_w1
b = x @ gpu1_w3

since there was element-wise multplication between a and b ( from F.silu(self.w1(x)) * self.w3(x))

GPU1 can simply do c = a * b

then do

d1 = c @ gpu1_w2

--------------------------------------------------
While GPU1 was doing all these operations above, GPU2 was doing the same operation (at the same time) for it's own weights

a = x @ gpu2_w1
b = x @ gpu2_w3

since there was element-wise multplication between a and b ( from F.silu(self.w1(x)) * self.w3(x))

GPU1 can simply do c = a * b

then do

d2 = c @ gpu2_w2

</code></pre></div></div>

<p>Afterwards, we can simply do all_reduce that will add the d1 + d2 and provide GPU1 and GPU2 with the summed result.</p>

<p>Similarly, we can do row-wise and column-wise sharding for attention as well which can be seen in the illustration below.</p>

<p><img src="/assets/images/2025-09-01-Distributed-RL-training-step/attn1-n.png" alt="attn1-n" />
<img src="/assets/images/2025-09-01-Distributed-RL-training-step/attn2-n.png" alt="att2-n" /></p>

<p>Both should be easy to understand, if not please take a look at the <a href="https://insujang.github.io/2024-01-11/tensor-parallelism-and-sequence-parallelism-detailed-analysis/">source</a></p>

<p>Enough of theory, how do we do that in using pytorch?
——–&gt; <strong>DTensor</strong> &lt;——–</p>

<p>What is DTensor?</p>

<blockquote>
  <p>PyTorch DTensor offers simple and flexible tensor sharding primitives that transparently handles distributed logic,</p>
</blockquote>

<p>Basically, it aids sharding/replicating tensors in pytorch.
This is what we use under the hood in pytorch’s Tensor Parallelism and FSDP2.</p>

<p>Couple of things we need to understand properly are: device_mesh and placements.</p>

<p>For now lets consider we have 2 GPUs, with DeviceMesh of simple 1D mesh [0,1]. It simply denotes how GPUs are structured in a grid (more on 2D, 3D mesh later) and both 0 and 1 GPU will be used for Tensor Parallelism.</p>

<h4 id="placements">Placements</h4>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Shard</code>: Tensor sharded on the tensor dimension <code class="language-plaintext highlighter-rouge">dim</code> on the devices of the <code class="language-plaintext highlighter-rouge">DeviceMesh</code> dimension</li>
  <li><code class="language-plaintext highlighter-rouge">Replicate</code>: Tensor replicated on the devices of the <code class="language-plaintext highlighter-rouge">DeviceMesh</code> dimension</li>
  <li><code class="language-plaintext highlighter-rouge">Partial</code>: Tensor is pending reduction on the devices of the <code class="language-plaintext highlighter-rouge">DeviceMesh</code> dimension</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.distributed.device_mesh</span> <span class="kn">import</span> <span class="n">init_device_mesh</span>
<span class="c1"># from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy
</span><span class="kn">from</span> <span class="n">torch.distributed._composable.fsdp</span> <span class="kn">import</span> <span class="n">fully_shard</span>
<span class="kn">from</span> <span class="n">torch.distributed.tensor.parallel</span> <span class="kn">import</span> <span class="n">parallelize_module</span><span class="p">,</span> <span class="n">ColwiseParallel</span><span class="p">,</span> <span class="n">RowwiseParallel</span><span class="p">,</span> <span class="n">SequenceParallel</span><span class="p">,</span> <span class="n">PrepareModuleInput</span>
<span class="kn">from</span> <span class="n">torch.distributed._tensor</span> <span class="kn">import</span> <span class="n">Shard</span><span class="p">,</span> <span class="n">Replicate</span><span class="p">,</span> <span class="n">distribute_tensor</span><span class="p">,</span> <span class="n">DTensor</span>


<span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="n">backend</span> <span class="o">=</span> <span class="sh">'</span><span class="s">nccl</span><span class="sh">'</span> <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">else</span> <span class="sh">'</span><span class="s">gloo</span><span class="sh">'</span>

<span class="n">rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">RANK</span><span class="sh">"</span><span class="p">])</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">WORLD_SIZE</span><span class="sh">"</span><span class="p">])</span>
<span class="n">local_rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">LOCAL_RANK</span><span class="sh">"</span><span class="p">])</span>


<span class="n">dist</span><span class="p">.</span><span class="nf">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

<span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>


<span class="n">mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">17</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>

<span class="n">x_local</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Replicate</span><span class="p">()])</span>

<span class="n">y_local</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Replicate</span><span class="p">()])</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> X local </span><span class="si">{</span><span class="n">x_local</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> Y local </span><span class="si">{</span><span class="n">y_local</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> MM_local </span><span class="si">{</span><span class="n">x_local</span> <span class="o">@</span> <span class="n">y_local</span><span class="p">.</span><span class="n">T</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div>

<p>x and y are of type torch.Tensor and with distrbute_tensor they will be sharded/replicated based on the placements type over device_mesh. As you can see, we’ve replicated x and y so both of these will have the same output in both GPUs</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
RANK 1
 X local DTensor(local_tensor=tensor([[1, 2, 3, 4],
        [5, 6, 7, 8]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))
 Y local DTensor(local_tensor=tensor([[ 9, 10, 11, 12],
        [13, 14, 15, 16]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))
 MM_local DTensor(local_tensor=tensor([[110, 150],
        [278, 382]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))

RANK 0
 X local DTensor(local_tensor=tensor([[1, 2, 3, 4],
        [5, 6, 7, 8]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))
 Y local DTensor(local_tensor=tensor([[ 9, 10, 11, 12],
        [13, 14, 15, 16]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))
 MM_local DTensor(local_tensor=tensor([[110, 150],
        [278, 382]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))
</code></pre></div></div>

<p>Take this code now.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">17</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>

<span class="n">x_local</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Replicate</span><span class="p">()])</span>

<span class="n">y_local</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">0</span><span class="p">)])</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> Y local </span><span class="si">{</span><span class="n">y_local</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> MM_local </span><span class="si">{</span><span class="n">x_local</span> <span class="o">@</span> <span class="n">y_local</span><span class="p">.</span><span class="n">T</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div>

<p>we replicate x but we shard y so, y will be halved and saved with type Shard(0) in two GPUs with different value and x will be same in both GPUs. we multiply both and the output will be of type Shard(1) (why? because of transpose, the shard type is changed) remember this, this might be helpful later on. Here’s the output</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RANK 1 Y local DTensor(local_tensor=tensor([[13, 14, 15, 16]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=0),))
 MM_local DTensor(local_tensor=tensor([[150],
        [382]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=1),))

RANK 0 Y local DTensor(local_tensor=tensor([[ 9, 10, 11, 12]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=0),))
 MM_local DTensor(local_tensor=tensor([[110],
        [278]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=1),))
</code></pre></div></div>

<p>Let’s do some example similar to this code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">w2</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">silu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">w1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="nf">w3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">x</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Replicate</span><span class="p">()])</span>

<span class="n">w1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">w3</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">17.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">w1_local</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Shard</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)])</span>
<span class="n">w3_local</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">w3</span><span class="p">,</span> <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Shard</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)])</span>
<span class="n">w2_local</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">w2</span><span class="p">,</span> <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">0</span><span class="p">)])</span>


<span class="c1"># print(f'RANK {dist.get_rank()} \n X local {x_local} \n Y local {y_local} \n MM_local {x_local @ y_local.T}')
</span><span class="n">c</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">w1_local</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">w3_local</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">c</span> <span class="o">@</span> <span class="n">w2_local</span>

<span class="c1"># collect d from both GPUs
</span><span class="n">d_collect</span> <span class="o">=</span> <span class="n">d</span><span class="p">.</span><span class="nf">redistribute</span><span class="p">(</span><span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Replicate</span><span class="p">()])</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> w1.shape </span><span class="si">{</span><span class="n">w1_local</span><span class="p">.</span><span class="nf">to_local</span><span class="p">().</span><span class="n">shape</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s">  w3.shape </span><span class="si">{</span><span class="n">w3_local</span><span class="p">.</span><span class="nf">to_local</span><span class="p">().</span><span class="n">shape</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> w2.shape </span><span class="si">{</span><span class="n">w2_local</span><span class="p">.</span><span class="nf">to_local</span><span class="p">().</span><span class="n">shape</span><span class="si">}</span><span class="s"> c values </span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s">, </span><span class="se">\n</span><span class="s"> d values </span><span class="se">\n</span><span class="si">{</span><span class="n">d</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> d collect </span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">d_collect</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RANK 1 w1.shape torch.Size([2, 2])
  w3.shape torch.Size([2, 2])
 w2.shape torch.Size([2, 2]) c values
 DTensor(local_tensor=tensor([[260., 336.],
        [260., 336.],
        [260., 336.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=1),)),
 d values
DTensor(local_tensor=tensor([[596., 596.],
        [596., 596.],
        [596., 596.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Partial(sum),))
 d collect
 DTensor(local_tensor=tensor([[788., 596.],
        [788., 596.],
        [788., 596.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))
        ----------------------------------------------------------
RANK 0 w1.shape torch.Size([2, 2])
  w3.shape torch.Size([2, 2])
 w2.shape torch.Size([2, 2]) c values
 DTensor(local_tensor=tensor([[132., 192.],
        [132., 192.],
        [132., 192.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=1),)),
 d values
DTensor(local_tensor=tensor([[192.,   0.],
        [192.,   0.],
        [192.,   0.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Partial(sum),))
 d collect
 DTensor(local_tensor=tensor([[788., 596.],
        [788., 596.],
        [788., 596.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))
</code></pre></div></div>

<p>as you can see d has two different values with placement type Partial and when we call redistribute on d with Replicate() type the result is all gathered (summed) and will be same on both GPU.</p>

<p>This was all manual Tensor Parallelism. We don’t go on writing these for all the matrix multiplications so we use some different method which implements the same logic under the hood.</p>

<h4 id="colwiseparallel">ColwiseParallel()</h4>

<blockquote>
  <p>Partition a compatible nn.Module in a column-wise fashion.</p>
</blockquote>

<p>NOTE: ColwiseParallel() only operates on nn.Linear() and nn.Embedding() module.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">ToyModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ToyModel</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">w1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">w3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">w2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">self</span><span class="p">.</span><span class="n">w1</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span> <span class="c1"># cause weight will have shape opposite of whats specified in nn.Linear()
</span>            <span class="n">self</span><span class="p">.</span><span class="n">w3</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">17.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">w2</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">w2</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">w1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="nf">w3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">ToyModel</span><span class="p">()</span>

<span class="n">tp_plan</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">w1</span><span class="sh">"</span> <span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">w2</span><span class="sh">"</span> <span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">w3</span><span class="sh">"</span> <span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">()</span>
<span class="p">}</span>

<span class="n">model</span> <span class="o">=</span> <span class="nf">parallelize_module</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">tp_plan</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Replicate</span><span class="p">()])</span> <span class="c1"># this is simply a tensor because ColwiseParallel will automatically convert it's input from torch.Tensor to torch.DTensor
</span><span class="n">out</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> w1.weight </span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">w1</span><span class="p">.</span><span class="n">weight</span><span class="si">}</span><span class="s">, </span><span class="se">\n</span><span class="s"> MM values </span><span class="se">\n</span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="nf">w1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">model</span><span class="p">.</span><span class="nf">w3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s"> output values </span><span class="se">\n</span><span class="si">{</span><span class="n">out</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RANK 1 w1.weight
 DTensor(local_tensor=tensor([[5., 6.],
        [7., 8.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)),
 MM values
tensor([[297., 465.],
        [297., 465.],
        [297., 465.]], grad_fn=&lt;MulBackward0&gt;) output values
AsyncCollectiveTensor(tensor([[ 0., 57.],
        [ 0., 57.],
        [ 0., 57.]]))

RANK 0 w1.weight
 DTensor(local_tensor=tensor([[1., 2.],
        [3., 4.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)),
 MM values
tensor([[ 57., 161.],
        [ 57., 161.],
        [ 57., 161.]], grad_fn=&lt;MulBackward0&gt;) output values
AsyncCollectiveTensor(tensor([[ 0., 57.],
        [ 0., 57.],
        [ 0., 57.]])
</code></pre></div></div>

<p>This code tries to exactly do the task that we did previously, the logic is the same but the output is different because of how the arranged weights is transposed and sharded.</p>

<p>Let’s me explain how ColwiseParallel works internally.</p>

<ul>
  <li>It takes input x, if it’s a tensor it will make it a DTensor and if not in the desired layout it will redistribute it to be the desired layout.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="n">self</span><span class="p">.</span><span class="n">input_layouts</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_layouts</span> <span class="ow">or</span> <span class="nc">Replicate</span><span class="p">(),)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_layouts</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_layouts</span> <span class="ow">or</span> <span class="nc">Shard</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">desired_input_layouts</span> <span class="o">=</span> <span class="p">(</span><span class="nc">Replicate</span><span class="p">(),)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">_prepare_input_fn</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="p">,</span> <span class="n">desired_input_layouts</span><span class="p">,</span> <span class="n">mod</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">device_mesh</span>
    <span class="p">):</span>
        <span class="c1"># TODO: figure out dynamo support for instance method and switch this to instance method
</span>
        <span class="c1"># annotate module input placements/sharding with input_layouts
</span>        <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">DTensor</span><span class="p">):</span>
            <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">DTensor</span><span class="p">.</span><span class="nf">from_local</span><span class="p">(</span>
                <span class="n">input_tensor</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">,</span> <span class="n">input_layouts</span><span class="p">,</span> <span class="n">run_check</span><span class="o">=</span><span class="bp">False</span>
            <span class="p">)</span>

        <span class="c1"># transform the input layouts to the desired layouts of ColwiseParallel
</span>        <span class="k">if</span> <span class="n">input_layouts</span> <span class="o">!=</span> <span class="n">desired_input_layouts</span><span class="p">:</span>
            <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="p">.</span><span class="nf">redistribute</span><span class="p">(</span>
                <span class="n">placements</span><span class="o">=</span><span class="n">desired_input_layouts</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="bp">True</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">input_tenso</span>
</code></pre></div></div>

<ul>
  <li>ColwiseParallel will shard the module’s weights on 0’th dimension not 1, why??</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    <span class="k">def</span> <span class="nf">_partition_linear_fn</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">):</span>
        <span class="c1"># colwise shard weight/bias to Shard(0), weight be Shard(0)
</span>        <span class="c1"># means Colwise as Linear is input * weight^T + bias, where
</span>        <span class="c1"># weight would become Shard(1)
</span>        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">module</span><span class="p">.</span><span class="nf">named_parameters</span><span class="p">():</span>
            <span class="n">dist_param</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span>
                <span class="nf">distribute_tensor</span><span class="p">(</span>
                    <span class="n">param</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">,</span> <span class="p">[</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">0</span><span class="p">)],</span> <span class="n">src_data_rank</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">src_data_rank</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">module</span><span class="p">.</span><span class="nf">register_parameter</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">dist_param</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>because under the hood in nn.Linear, the operation is x @ w.T, the weights are transposed, so</li>
  <li>weights are sharded on dim=0, but output will be Shard(-1) because we multiply with the transpose weight (see example above where we implemented MM with transposed weights), and by default use_local_output is True which will convert the DTensor back to torch.Tensor</li>
</ul>

<p>This is a part of code from pytorch’s ColwiseParallel() <a href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/tensor/parallel/style.py#L89">here</a> shows the default input and output layouts for ColwiseParallel().</p>

<h4 id="rowwiseparallel">RowwiseParallel()</h4>

<p>it works exactly the opposite of ColwiseParallel(), it shards weights on dim=1, i.e Shard(dim=1)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="n">self</span><span class="p">.</span><span class="n">input_layouts</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_layouts</span> <span class="ow">or</span> <span class="nc">Shard</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_layouts</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_layouts</span> <span class="ow">or</span> <span class="nc">Replicate</span><span class="p">(),</span>
</code></pre></div></div>

<p>and it’s desired layout is Shard(-1).</p>

<p>That’s it, we need to take care of three things.</p>

<ol>
  <li>what input it expects (i.e Shard() or Replicate())</li>
  <li>what it outputs (i.e Shar() or Replicate())</li>
  <li>How it shards it’s module weights.</li>
</ol>

<h4 id="sequenceparallel">SequenceParallel()</h4>

<p>It generally operates on the sequence dimension. Say the activations are of shape B,T,C = batch_size, sequence_length, embedding dimension.</p>

<p>It will shard the sequence dimension T. There is no sharding of weight in this SequenceParallel(). It is applied to the LayerNorm()/RMSNorm() it will split on T and calculate mean/std based on the given T, there’s no need for communication because mean/std are calculated on C’th dimension and don’t depend on T.</p>

<p>However, the <strong>output</strong> of LayerNorm/RMSNorm is typically <strong>all-gathered</strong> across the <strong>T</strong> shards before it is consumed by the next layer (e.g., an attention or MLP block) (more on this below)
self.sequence_sharding = (Shard(sequence_dim)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">_prepare_input_fn</span><span class="p">(</span><span class="n">sequence_sharding</span><span class="p">,</span> <span class="n">mod</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">):</span>
        <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">DTensor</span><span class="p">):</span>
            <span class="c1"># if the passed in input DTensor is not sharded on the sequence dim, we need to redistribute it
</span>            <span class="k">if</span> <span class="n">input_tensor</span><span class="p">.</span><span class="n">placements</span> <span class="o">!=</span> <span class="n">sequence_sharding</span><span class="p">:</span>
                <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="p">.</span><span class="n">redistribute</span><span class="p">(</span>
                    <span class="n">placements</span><span class="o">=</span><span class="n">sequence_sharding</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="bp">True</span>
		<span class="k">return</span> <span class="n">input_tens</span>
</code></pre></div></div>

<h4 id="tensor-parallelism-on-transformers-model">Tensor Parallelism on Transformers Model</h4>

<p>Let’s apply what we learned above in a transformers model. I won’t be providing the output for this but will only explain what it’s doing. The code below is taken from <a href="https://docs.pytorch.org/tutorials/intermediate/TP_tutorial.html">here</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.distributed.device_mesh</span> <span class="kn">import</span> <span class="n">init_device_mesh</span>
<span class="c1"># from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy
</span><span class="kn">from</span> <span class="n">torch.distributed._composable.fsdp</span> <span class="kn">import</span> <span class="n">fully_shard</span>
<span class="kn">from</span> <span class="n">torch.distributed.tensor.parallel</span> <span class="kn">import</span> <span class="n">parallelize_module</span><span class="p">,</span> <span class="n">ColwiseParallel</span><span class="p">,</span> <span class="n">RowwiseParallel</span><span class="p">,</span> <span class="n">SequenceParallel</span><span class="p">,</span> <span class="n">PrepareModuleInput</span>
<span class="kn">from</span> <span class="n">torch.distributed._tensor</span> <span class="kn">import</span> <span class="n">Shard</span><span class="p">,</span> <span class="n">Replicate</span><span class="p">,</span> <span class="n">distribute_tensor</span><span class="p">,</span> <span class="n">DTensor</span>

<span class="kn">from</span> <span class="n">llama2_model</span> <span class="kn">import</span> <span class="n">Transformer</span><span class="p">,</span> <span class="n">ModelArgs</span>


<span class="c1"># import warnings # ignore all warning messages
# warnings.filterwarnings("ignore")
</span>
<span class="c1"># Code for helping init process group
</span><span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="n">backend</span> <span class="o">=</span> <span class="sh">'</span><span class="s">nccl</span><span class="sh">'</span> <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">else</span> <span class="sh">'</span><span class="s">gloo</span><span class="sh">'</span>

<span class="n">rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">RANK</span><span class="sh">"</span><span class="p">])</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">WORLD_SIZE</span><span class="sh">"</span><span class="p">])</span>
<span class="n">local_rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">LOCAL_RANK</span><span class="sh">"</span><span class="p">])</span>

<span class="c1"># print('local rank', local_rank)
</span>
<span class="n">dist</span><span class="p">.</span><span class="nf">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

<span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>

<span class="n">simple_llama2_config</span> <span class="o">=</span> <span class="nc">ModelArgs</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">.</span><span class="nf">from_model_args</span><span class="p">(</span><span class="n">simple_llama2_config</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># model.init_weights()
</span>
<span class="c1"># """
# mesh = init_device_mesh('cpu', (2, 4), mesh_dim_names=["FSDP", "TP"])
</span>
<span class="n">mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">])</span>

<span class="n">layer_tp_plan</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># Now the input and output of SequenceParallel has Shard(1) layouts,
</span>    <span class="c1"># to represent the input/output tensors sharded on the sequence dimension
</span>    <span class="sh">"</span><span class="s">attention_norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">attention</span><span class="sh">"</span><span class="p">:</span> <span class="nc">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="nc">Replicate</span><span class="p">()),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Replicate</span><span class="p">(),</span> <span class="nc">Replicate</span><span class="p">()),</span>
    <span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wq</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wk</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wv</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wo</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
    <span class="sh">"</span><span class="s">ffn_norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">feed_forward</span><span class="sh">"</span><span class="p">:</span> <span class="nc">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Replicate</span><span class="p">(),),</span>
    <span class="p">),</span>
    <span class="sh">"</span><span class="s">feed_forward.w1</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">feed_forward.w2</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
    <span class="sh">"</span><span class="s">feed_forward.w3</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(),</span>
<span class="p">}</span>
<span class="c1"># Apply TP
</span><span class="k">for</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">transformer_block</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">):</span>
    <span class="c1"># layer_tp_plan = {...}  # i.e. the plan we just generated
</span>
    <span class="nf">parallelize_module</span><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="n">transformer_block</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">parallelize_plan</span><span class="o">=</span><span class="n">layer_tp_plan</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nf">parallelize_module</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">tok_embeddings</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">(),</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">),</span>
        <span class="sh">"</span><span class="s">norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
        <span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">()</span>
        <span class="p">),</span>
    <span class="p">}</span>
<span class="p">)</span>
</code></pre></div></div>

<p>you can pass input x through model() and inspect where the weights for each modules are stored and what the output will be.</p>

<p>All we need to change/understand is the parallelize_plan. let’s understand the sequence of parallelization in our model that we have created above with the help of Colwise Rowwise and Sequence Parallel.</p>

<p>Step 1</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="sh">"</span><span class="s">tok_embeddings</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">(),</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">),</span>
</code></pre></div></div>

<p>x = (B,T)
out = tok_embeddings(x)
tok_embeddings.weight is sharded by dim=0, output must be replicate but we apply output_layouts=Shard(1) cause next we apply SequenceParallel that expects Shard(1) as input</p>

<p>so out is of shape (B,T/2) its is sharded by dim=1 but its a torch.Tensor if GPU=2</p>

<p>Step 2</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="sh">"</span><span class="s">attention_norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
</code></pre></div></div>

<p>out = attention_norm(out)
converts out(B,T/2, C) which is a torch.Tensor to Shard(1) with prepare_input_fn. SequenceParallel is applied (no weight sharding), only applies LayerNorm/RMSNorm on T/2 tokens in each GPUs. no change of output layout i.e its still Shard(1) of shape B,T/2,C</p>

<p>Step 3</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="sh">"</span><span class="s">attention</span><span class="sh">"</span><span class="p">:</span> <span class="nc">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="nc">Replicate</span><span class="p">()),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Replicate</span><span class="p">(),</span> <span class="nc">Replicate</span><span class="p">()),</span>
    <span class="p">),</span>

</code></pre></div></div>

<p>attention expects whole sequence so convert Shard(1) to Replicate(). under the hood out is combined to form (B,T,C) from B,T/2, C and replicate to both GPUs. Thats what PrepareModuleInput does. (Shard(1), Replicate()) is for different inputs to the attention module Shard(1) is for x and Replicate() is for freq_cis</p>

<p>Step 4</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="sh">"</span><span class="s">attention.wq</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wk</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wv</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
</code></pre></div></div>

<p>apply Shard(0) to attention.wq.weight, but output is Shard(-1) (why? already explained above).</p>

<p>Step 5</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"</span><span class="s">attention.wo</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<p>Expects input to be Shard(-1) which is correct, should output replicate (after doing all reduce of the partial sum) but instead converts to Shard(1) because next we apply SequenceParallelism for ffn_norm.</p>

<p>Other steps should be straightforward, follow the sequence described below. Finally the output (B,T,vocab_size) is replicated across both the GPUs.</p>

<p>In pytorch’s Tensor Parallel docs they suggest also adding loss parallel, because replicating the output logits which is of size (B,T,vocab_size) can be expensive. The only change we need to do is keep the use_local_output which will make the output of that output module to be DTensor.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nf">parallelize_module</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">tp_mesh</span><span class="p">,</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">tok_embeddings</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">(),</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">),</span>
        <span class="sh">"</span><span class="s">norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
        <span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="c1"># use DTensor as the output
</span>            <span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">},</span>
<span class="p">)</span>
</code></pre></div></div>

<p>and apply loss_parallel() context.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">torch.distributed.tensor.parallel</span> <span class="kn">import</span> <span class="n">loss_parallel</span>

<span class="n">pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
<span class="k">with</span> <span class="nf">loss_parallel</span><span class="p">():</span>
    <span class="c1"># assuming pred and labels are of the shape [batch, seq, vocab]
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">pred</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
</code></pre></div></div>

<p>I don’t seem to understand this, because they don’t provide enough explanation on their <a href="https://docs.pytorch.org/tutorials/intermediate/TP_tutorial.html">page</a>.</p>

<p>My assumption is that each gpus will have (B,T,vocab_size/2) matrix, loss calculation depends on the full vocab_size cause we need to convert the logits into probabilities using softmax function. What they could be doing is calculate the local sum of all e^(x) in vocab_size do all_reduce and then calculate loss.</p>

<h3 id="device-mesh">Device mesh</h3>

<h4 id="2d-mesh">2D mesh</h4>

<p>It’s very important that we have no confusion on how device mesh works, and how TP and FSDP works using the device mesh. (it has haunted for a day, because I did not understand it properly)</p>

<p>let’s first start with a 2D mesh and apply tensor parallelism.
our original mesh will look like this</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[0,1],
[2,3]]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">FSDP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">])</span>

<span class="n">layer_tp_plan</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># Now the input and output of SequenceParallel has Shard(1) layouts,
</span>    <span class="c1"># to represent the input/output tensors sharded on the sequence dimension
</span>    <span class="sh">"</span><span class="s">attention_norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">attention</span><span class="sh">"</span><span class="p">:</span> <span class="nc">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="nc">Replicate</span><span class="p">()),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Replicate</span><span class="p">(),</span> <span class="nc">Replicate</span><span class="p">()),</span>
    <span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wq</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wk</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wv</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wo</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
    <span class="sh">"</span><span class="s">ffn_norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">feed_forward</span><span class="sh">"</span><span class="p">:</span> <span class="nc">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Replicate</span><span class="p">(),),</span>
    <span class="p">),</span>
    <span class="sh">"</span><span class="s">feed_forward.w1</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">feed_forward.w2</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
    <span class="sh">"</span><span class="s">feed_forward.w3</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(),</span>
<span class="p">}</span>
<span class="c1"># Apply TP
</span><span class="k">for</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">transformer_block</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">):</span>
    <span class="c1"># layer_tp_plan = {...}  # i.e. the plan we just generated
</span>
    <span class="nf">parallelize_module</span><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="n">transformer_block</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">parallelize_plan</span><span class="o">=</span><span class="n">layer_tp_plan</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nf">parallelize_module</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">tok_embeddings</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">(),</span>
            <span class="c1"># output_layouts=Shard(1),
</span>        <span class="p">),</span>
        <span class="sh">"</span><span class="s">norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
        <span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">()</span>
        <span class="p">),</span>
    <span class="p">}</span>
<span class="p">)</span>
</code></pre></div></div>

<p>we apply TP on only the TP mesh <code class="language-plaintext highlighter-rouge">mesh['TP']</code>
but what is the output of <code class="language-plaintext highlighter-rouge">mesh['TP']</code> ?</p>

<p>there could be different ways to understand this, but I understand it this way.
Assume we are in process 0, i.e
our original mesh is <code class="language-plaintext highlighter-rouge">mesh['FSDP','TP']</code>, we try to get <code class="language-plaintext highlighter-rouge">mesh['TP']</code> where FSDP is not there so lets draw a line across ‘FSDP’ dimensions (0th and 1st) and see which TP sub-matrix falls in the drawn line. since we were in process 0, submatrix <code class="language-plaintext highlighter-rouge">[0,1]</code> is chosen and our process 0 will only <code class="language-plaintext highlighter-rouge">[0,1]</code> when we output <code class="language-plaintext highlighter-rouge">mesh['TP']</code> and ignore others. We’ll get the same when we are on process 1, but we’ll get <code class="language-plaintext highlighter-rouge">[2,3]</code> when we’re on either 2 or 3 processes.</p>

<p>so the simple logic is to draw line across dimension that is not in the specified mesh i.e FSDP in this case, and see on which submatrix our current process falls. It’s better to get a pen and paper and understand it deeply.
<img src="/assets/images/2025-09-01-Distributed-RL-training-step/fsdp_mat.png" alt="fsdp_mat" /></p>

<p>We pass this mesh to parallelize_module that takes in only 1D mesh and applies the Sharding plan across that 1D mesh, For example, if we are in process 0, the weights of size (8,8) are sharded across 0 and 1 GPUs so each gpu will get (4,8) (assume Shard(0)) and if we’re on process 2 weights are sharded across 2,3.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="nf">parallelize_module</span><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="n">transformer_block</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">parallelize_plan</span><span class="o">=</span><span class="n">layer_tp_plan</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></div>

<p>now let’s apply the FSDP sharding</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
	<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
		<span class="nf">fully_shard</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">])</span>

	<span class="nf">fully_shard</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div>

<p>As we did above (drawing line and getting submatrix), process 0 will see mesh[‘FSDP’] = [0,2]
and process 1 will see [1,3].</p>

<p>fully_shard takes two types of mesh 1D, and 2D mesh. Since we have 1D mesh in this case, it will shard the weights across the 1D mesh with Shard(0) placement. i.e let’s say we’re on process 0, 0 already had (4,8) matrix, and when we apply fully_shard the dimension 0 is again split and process 0 will get (2,8) matrix and it’s other half will be stored in process 2. If you’re confused please bear with me, we’ll see with examples on 3D mesh below.</p>

<h4 id="3d-mesh">3D mesh</h4>

<p><img src="/assets/images/2025-09-01-Distributed-RL-training-step/matrix-mul.png" alt="matrix-mul" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
<span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]]</span>

<span class="p">[[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span>
<span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">]]]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">DDP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">FSDP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">])</span>
</code></pre></div></div>

<p>We apply the same TP plan with mesh[‘TP’], similar to above, 0,1 will see mesh [0,1] 2,3 will see mesh [2,3] and so on. Let’s see one worked out example, we initially have attention_wq matrix’s weight of size (8,8) arranged in order. We only apply TP (no FSDP). So each TP axis should have same sharded attention_wq matrix i.e for TP axis 0, rank 0,2,4,6 will have same matrix and for TP axis 1, ranks 1,2,5,7 will have same matrix.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.distributed.device_mesh</span> <span class="kn">import</span> <span class="n">init_device_mesh</span>
<span class="c1"># from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy
</span><span class="kn">from</span> <span class="n">torch.distributed._composable.fsdp</span> <span class="kn">import</span> <span class="n">fully_shard</span>
<span class="kn">from</span> <span class="n">torch.distributed.tensor.parallel</span> <span class="kn">import</span> <span class="n">parallelize_module</span><span class="p">,</span> <span class="n">ColwiseParallel</span><span class="p">,</span> <span class="n">RowwiseParallel</span><span class="p">,</span> <span class="n">SequenceParallel</span><span class="p">,</span> <span class="n">PrepareModuleInput</span>
<span class="kn">from</span> <span class="n">torch.distributed._tensor</span> <span class="kn">import</span> <span class="n">Shard</span><span class="p">,</span> <span class="n">Replicate</span><span class="p">,</span> <span class="n">distribute_tensor</span><span class="p">,</span> <span class="n">DTensor</span>

<span class="kn">from</span> <span class="n">llama2_model</span> <span class="kn">import</span> <span class="n">Transformer</span><span class="p">,</span> <span class="n">ModelArgs</span>


<span class="c1"># import warnings # ignore all warning messages
# warnings.filterwarnings("ignore")
</span>
<span class="c1"># Code for helping init process group
</span><span class="n">device</span> <span class="o">=</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="n">backend</span> <span class="o">=</span> <span class="sh">'</span><span class="s">gloo</span><span class="sh">'</span>

<span class="n">rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">RANK</span><span class="sh">"</span><span class="p">])</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">WORLD_SIZE</span><span class="sh">"</span><span class="p">])</span>
<span class="n">local_rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">LOCAL_RANK</span><span class="sh">"</span><span class="p">])</span>

<span class="c1"># print('local rank', local_rank)
</span>
<span class="n">dist</span><span class="p">.</span><span class="nf">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

<span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>

<span class="n">simple_llama2_config</span> <span class="o">=</span> <span class="nc">ModelArgs</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">.</span><span class="nf">from_model_args</span><span class="p">(</span><span class="n">simple_llama2_config</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># model.tok_embeddings.weight = nn.Parameter(torch.arange(1.,33.).reshape(8,4))
</span>
<span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">attention</span><span class="p">.</span><span class="n">wq</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">65.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>



<span class="n">mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">DDP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">FSDP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">])</span>
<span class="n">layer_tp_plan</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># Now the input and output of SequenceParallel has Shard(1) layouts,
</span>    <span class="c1"># to represent the input/output tensors sharded on the sequence dimension
</span>    <span class="sh">"</span><span class="s">attention_norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">attention</span><span class="sh">"</span><span class="p">:</span> <span class="nc">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="nc">Replicate</span><span class="p">()),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Replicate</span><span class="p">(),</span> <span class="nc">Replicate</span><span class="p">()),</span>
    <span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wq</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wk</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wv</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wo</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
    <span class="sh">"</span><span class="s">ffn_norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">feed_forward</span><span class="sh">"</span><span class="p">:</span> <span class="nc">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Replicate</span><span class="p">(),),</span>
    <span class="p">),</span>
    <span class="sh">"</span><span class="s">feed_forward.w1</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">feed_forward.w2</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
    <span class="sh">"</span><span class="s">feed_forward.w3</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(),</span>
<span class="p">}</span>
<span class="c1"># Apply TP
</span><span class="k">for</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">transformer_block</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">):</span>
    <span class="c1"># layer_tp_plan = {...}  # i.e. the plan we just generated
</span>
    <span class="nf">parallelize_module</span><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="n">transformer_block</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">parallelize_plan</span><span class="o">=</span><span class="n">layer_tp_plan</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nf">parallelize_module</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">tok_embeddings</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">(),</span>
            <span class="c1"># output_layouts=Shard(1),
</span>        <span class="p">),</span>
        <span class="sh">"</span><span class="s">norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
        <span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">()</span>
        <span class="p">),</span>
    <span class="p">}</span>
<span class="p">)</span>


<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">17</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">16</span><span class="p">)</span>

<span class="k">if</span> <span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">MESHDIM </span><span class="si">{</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">DDP</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s"> TP MESH </span><span class="si">{</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># import time
# time.sleep(5)
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Global rank: </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s">, attn_wq </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">attention</span><span class="p">.</span><span class="n">wq</span><span class="p">.</span><span class="n">weight</span><span class="si">}</span><span class="s"> shape: </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">attention</span><span class="p">.</span><span class="n">wq</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="nf">to_local</span><span class="p">().</span><span class="n">shape</span><span class="si">}</span><span class="s"> </span><span class="se">\n\n</span><span class="sh">'</span><span class="p">)</span>
<span class="c1"># print(f'Global rank: {dist.get_rank()}, \n\n OUTPUT:\n {model.tok_embeddings(x).shape}')
</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>output
Global rank: 5, tok_embeddings DTensor(local_tensor=tensor([[33., 34., 35., 36., 37., 38., 39., 40.],
        [41., 42., 43., 44., 45., 46., 47., 48.],
        [49., 50., 51., 52., 53., 54., 55., 56.],
        [57., 58., 59., 60., 61., 62., 63., 64.]]), device_mesh=DeviceMesh('cpu', [4, 5], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])


Global rank: 4, tok_embeddings DTensor(local_tensor=tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12., 13., 14., 15., 16.],
        [17., 18., 19., 20., 21., 22., 23., 24.],
        [25., 26., 27., 28., 29., 30., 31., 32.]]), device_mesh=DeviceMesh('cpu', [4, 5], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])


MESHDIM DeviceMesh('cpu', [[0, 2], [4, 6]], mesh_dim_names=('DDP', 'FSDP')) TP MESH DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',))

Global rank: 1, tok_embeddings DTensor(local_tensor=tensor([[33., 34., 35., 36., 37., 38., 39., 40.],
        [41., 42., 43., 44., 45., 46., 47., 48.],
        [49., 50., 51., 52., 53., 54., 55., 56.],
        [57., 58., 59., 60., 61., 62., 63., 64.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])


Global rank: 0, tok_embeddings DTensor(local_tensor=tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12., 13., 14., 15., 16.],
        [17., 18., 19., 20., 21., 22., 23., 24.],
        [25., 26., 27., 28., 29., 30., 31., 32.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])


Global rank: 7, tok_embeddings DTensor(local_tensor=tensor([[33., 34., 35., 36., 37., 38., 39., 40.],
        [41., 42., 43., 44., 45., 46., 47., 48.],
        [49., 50., 51., 52., 53., 54., 55., 56.],
        [57., 58., 59., 60., 61., 62., 63., 64.]]), device_mesh=DeviceMesh('cpu', [6, 7], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])


Global rank: 6, tok_embeddings DTensor(local_tensor=tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12., 13., 14., 15., 16.],
        [17., 18., 19., 20., 21., 22., 23., 24.],
        [25., 26., 27., 28., 29., 30., 31., 32.]]), device_mesh=DeviceMesh('cpu', [6, 7], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])


Global rank: 3, tok_embeddings DTensor(local_tensor=tensor([[33., 34., 35., 36., 37., 38., 39., 40.],
        [41., 42., 43., 44., 45., 46., 47., 48.],
        [49., 50., 51., 52., 53., 54., 55., 56.],
        [57., 58., 59., 60., 61., 62., 63., 64.]]), device_mesh=DeviceMesh('cpu', [2, 3], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])


Global rank: 2, tok_embeddings DTensor(local_tensor=tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12., 13., 14., 15., 16.],
        [17., 18., 19., 20., 21., 22., 23., 24.],
        [25., 26., 27., 28., 29., 30., 31., 32.]]), device_mesh=DeviceMesh('cpu', [2, 3], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])
</code></pre></div></div>

<p>Now let’s apply the FSDP on mesh[‘DDP’, ‘FSDP’]</p>

<p>but let’s first understand what will our processes. see when we simply print mesh[‘DDP’, ‘FSDP’]. As explained above, apply the rule above, which dimension is not there, draw the line and erase its brackets. In this case, ‘TP’ dimension is not there so if we are on process 0, it will see matrix <code class="language-plaintext highlighter-rouge">[[0,2], [4,6]]</code> which I’ve already printed in the output above as</p>

<p><code class="language-plaintext highlighter-rouge">MESHDIM DeviceMesh('cpu', [[0, 2], [4, 6]], mesh_dim_names=('DDP', 'FSDP')) TP MESH DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)) </code></p>

<p>but when we’re on process 1 it will see <code class="language-plaintext highlighter-rouge">[[1,3], [5,7]]</code> and so on.</p>

<p>let’s apply FSDP with mesh[‘DDP’, ‘FSDP’]</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
	<span class="nf">fully_shard</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">DDP</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">])</span>

<span class="nf">fully_shard</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">DDP</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div>

<p>the sharding rule isn’t the same when we pass 2D mesh to fully_shard.</p>

<blockquote>
  <p><strong>mesh</strong> (<em>Optional__[</em><a href="https://docs.pytorch.org/docs/stable/distributed.html#torch.distributed.device_mesh.DeviceMesh" title="torch.distributed.device_mesh.DeviceMesh"><em>DeviceMesh</em></a><em>]</em>) – This data parallel mesh defines the sharding and device. If 1D, then parameters are fully sharded across the 1D mesh (FSDP) with <code class="language-plaintext highlighter-rouge">(Shard(0),)</code> placement. If 2D, then parameters are sharded across the 1st dim and replicated across the 0th dim (HSDP) with <code class="language-plaintext highlighter-rouge">(Replicate(), Shard(0))</code> placement. The mesh’s device type gives the device type used for communication; if a CUDA or CUDA-like device type, then we use the current device.</p>
</blockquote>

<p>so assuming we’re on process 0, and it has attention_wq matrix of size (4,8). This (4,8) matrix will be across 1st dimension i.e 0 will get (2,8) matrix and 2 will get the other half (2,8) and this is replicated across 0th dimension i.e process 0 will have the same (2,8) matrix as process 4 and 2 will have same (2,8) matrix as 6.</p>

<p>Here’s the output</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Global rank: 3, tok_embeddings DTensor(local_tensor=tensor([[49., 50., 51., 52., 53., 54., 55., 56.],
        [57., 58., 59., 60., 61., 62., 63., 64.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])


Global rank: 2, tok_embeddings DTensor(local_tensor=tensor([[17., 18., 19., 20., 21., 22., 23., 24.],
        [25., 26., 27., 28., 29., 30., 31., 32.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])


Global rank: 4, tok_embeddings DTensor(local_tensor=tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12., 13., 14., 15., 16.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])


Global rank: 5, tok_embeddings DTensor(local_tensor=tensor([[33., 34., 35., 36., 37., 38., 39., 40.],
        [41., 42., 43., 44., 45., 46., 47., 48.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])


Global rank: 7, tok_embeddings DTensor(local_tensor=tensor([[49., 50., 51., 52., 53., 54., 55., 56.],
        [57., 58., 59., 60., 61., 62., 63., 64.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])


MESHDIM DeviceMesh('cpu', [[0, 2], [4, 6]], mesh_dim_names=('DDP', 'FSDP')) TP MESH DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',))

Global rank: 6, tok_embeddings DTensor(local_tensor=tensor([[17., 18., 19., 20., 21., 22., 23., 24.],
        [25., 26., 27., 28., 29., 30., 31., 32.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])


Global rank: 0, tok_embeddings DTensor(local_tensor=tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12., 13., 14., 15., 16.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])


Global rank: 1, tok_embeddings DTensor(local_tensor=tensor([[33., 34., 35., 36., 37., 38., 39., 40.],
        [41., 42., 43., 44., 45., 46., 47., 48.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])

</code></pre></div></div>

<p>That’s it.</p>

<p>If we want to shard computation increase TP size, if we want to shard weights increase FSDP size, and if we want to process more batch increase DDP size.</p>

<h3 id="data-parallelism-with-fsdp-and-tp">Data-parallelism with FSDP and TP</h3>

<p>There might be some confusion about how data parallelism works with these sharding techniques. The confusion might be that we only pass different batches to DDP axis, for example: if we have 3D mesh ddp,fsdp,tp = (2,2,2), we might think that we pass one minibatch to ranks from first ddp axis and another to ranks from second ddp axis. BUT no, we can pass different batch across FSDP groups but with a strict condition that they are same across tp groups. For the 3D mesh above we have tp groups <code class="language-plaintext highlighter-rouge">[0,1] [2,3], [4,5], [6,7]</code>
and FSDP group <code class="language-plaintext highlighter-rouge">[0,2],[1,3],[4,6], [5,7]</code> so we can distribute one batch to rank 0 and another to rank 2 and 0 will broadcast the same data to it’s rank in the same tp group i.e rank 1, and similarly 2 will broadcast the same data to it’s rank in the same tp group i.e rank 3.</p>

<p>so each TP group will be have one distinct batch they can process. One important thing we might forget here is that FSDP will average the gradients across it’s group. Ex (gradient-for-one-batch-at-rank-0 + gradient-for-one-batch-at-rank-2)/2 (which is the number of ranks in the same FSDP group)</p>

<p>here’s is the worked out example to understand it better</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span><span class="o">*</span><span class="mi">2</span><span class="p">]</span>

<span class="nd">@spawn_threads_and_init_comms</span>
<span class="k">def</span> <span class="nf">shard_big_tensor</span><span class="p">(</span><span class="n">world_size</span><span class="p">):</span>

    <span class="n">device</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
    <span class="n">model_device_mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">])</span>

    <span class="n">device_mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">dp</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">tp</span><span class="sh">'</span><span class="p">])</span>

    <span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
            <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
            <span class="n">self</span><span class="p">.</span><span class="n">layer0</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer0</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="nc">Model</span><span class="p">()</span>

    <span class="n">model</span><span class="p">.</span><span class="n">layer0</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">17.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

    <span class="nf">parallelize_module</span><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">model_device_mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">parallelize_plan</span><span class="o">=</span><span class="p">{</span>
            <span class="sh">"</span><span class="s">layer0</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">()</span> <span class="c1"># since rowwise parallel will replicate the output layouts, each tp rank will get the same outptu.
</span>        <span class="p">}</span>
    <span class="p">)</span>

    <span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

    <span class="n">optim</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">model_device_mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">].</span><span class="nf">get_local_rank</span><span class="p">()</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span> <span class="c1"># give one data
</span>        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">rank </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> x = </span><span class="si">{</span><span class="n">data_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">out0</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data_list</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1">#         print(out0)
</span>        <span class="n">dist</span><span class="p">.</span><span class="nf">barrier</span><span class="p">()</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
        <span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">out0</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">out0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">rank </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="n">dist</span><span class="p">.</span><span class="nf">barrier</span><span class="p">()</span>
<span class="c1">#     print(f' RANK {dist.get_rank()} before second weights = {(model.layer0.weight.grad)} \n\n')
</span>
    <span class="k">if</span> <span class="n">model_device_mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">].</span><span class="nf">get_local_rank</span><span class="p">()</span> <span class="o">==</span><span class="mi">1</span><span class="p">:</span> <span class="c1"># give one data
</span>        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">rank </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> x = </span><span class="si">{</span><span class="n">data_list</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

        <span class="n">out1</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data_list</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="c1">#         print(out1)
</span>        <span class="n">dist</span><span class="p">.</span><span class="nf">barrier</span><span class="p">()</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
        <span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">out1</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">out1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">rank </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="n">dist</span><span class="p">.</span><span class="nf">barrier</span><span class="p">()</span>

<span class="c1">#     print(f' RANK {dist.get_rank()} after second weights = {model.layer0.weight.grad} \n\n')
</span>

    <span class="n">dt</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">layer0</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="nf">to_local</span><span class="p">()</span>

    <span class="n">dist</span><span class="p">.</span><span class="nf">all_reduce</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="p">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">model_device_mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">].</span><span class="nf">get_group</span><span class="p">())</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s"> RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> after second weights = </span><span class="si">{</span><span class="n">dt</span><span class="o">/</span><span class="mi">2</span><span class="si">}</span><span class="s"> </span><span class="se">\n\n</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">return</span>
</code></pre></div></div>

<p>In the code above, I don’t split the model across FSDP dimension and try to replicate what FSDP does under the hood. In this code we have two different data consider, torch.ones to be one batch and torch.ones x 2 to be another. Initially, I apply TP across TP groups, so two tp groups here <code class="language-plaintext highlighter-rouge">[0,1] and [2,3]</code> will have the same weights, then I distribute batch 1 to rank 0 and batch 2 to rank 2 (why? I’m trying to replicate the behaviour of fsdp here). Similarly, rank 1 will have batch 1 and rank 3 will have batch 2 (why? cause ranks across same tp group must have same data).</p>

<p>after calculating the gradient independently, I sum the gradients using all_reduce and print the averaged gradient.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RANK 1 reduced weights = tensor([[102.3585, 102.3585],
        [260.1725, 260.1725],
        [412.6720, 412.6720],
        [581.2230, 581.2230]])


 RANK 2 reduced weights = tensor([[102.3585, 102.3585],
        [260.1725, 260.1725],
        [412.6720, 412.6720],
        [581.2230, 581.2230]])


 RANK 3 reduced weights = tensor([[102.3585, 102.3585],
        [260.1725, 260.1725],
        [412.6720, 412.6720],
        [581.2230, 581.2230]])


 RANK 0 reduced weights = tensor([[102.3585, 102.3585],
        [260.1725, 260.1725],
        [412.6720, 412.6720],
        [581.2230, 581.2230]])
</code></pre></div></div>

<p>You might be wondering why different tp ranks have the same weights, it because RowwiseParallel calls Replicate() and and it has to do partial sum for output across tp ranks.</p>

<p>Now let’s see if FSDP does the same or not.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span><span class="o">*</span><span class="mi">2</span><span class="p">]</span>

<span class="nd">@spawn_threads_and_init_comms</span>
<span class="k">def</span> <span class="nf">shard_big_tensor</span><span class="p">(</span><span class="n">world_size</span><span class="p">):</span>

    <span class="n">device</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
    <span class="n">model_device_mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">])</span>

    <span class="n">device_mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">dp</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">tp</span><span class="sh">'</span><span class="p">])</span>

    <span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
            <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
            <span class="n">self</span><span class="p">.</span><span class="n">layer0</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer0</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="nc">Model</span><span class="p">()</span>

    <span class="n">model</span><span class="p">.</span><span class="n">layer0</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">17.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

    <span class="nf">parallelize_module</span><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">model_device_mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">parallelize_plan</span><span class="o">=</span><span class="p">{</span>
            <span class="sh">"</span><span class="s">layer0</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">()</span> <span class="c1"># since rowwise parallel will replicate the output layouts, each tp rank will get the same outptu.
</span>        <span class="p">}</span>
    <span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="nf">split_data_list</span><span class="p">(</span><span class="n">data_list</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">dp</span><span class="sh">'</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s"> RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> x= </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s"> </span><span class="se">\n\n</span><span class="sh">'</span><span class="p">)</span>


    <span class="nf">fully_shard</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="n">model_device_mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">])</span>

    <span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

    <span class="n">optim</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">dist</span><span class="p">.</span><span class="nf">barrier</span><span class="p">()</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">out</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">rank </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> loss </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s"> </span><span class="sh">'</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>


    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s"> RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> weights = </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">layer0</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">grad</span><span class="si">}</span><span class="s"> </span><span class="se">\n\n</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">return</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> RANK 2 weights = DTensor(local_tensor=tensor([[412.6720, 412.6720],
        [581.2230, 581.2230]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(Shard(dim=0), Shard(dim=1)))


 RANK 0 weights = DTensor(local_tensor=tensor([[102.3585, 102.3585],
        [260.1725, 260.1725]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(Shard(dim=0), Shard(dim=1)))


 RANK 1 weights = DTensor(local_tensor=tensor([[102.3585, 102.3585],
        [260.1725, 260.1725]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(Shard(dim=0), Shard(dim=1)))


 RANK 3 weights = DTensor(local_tensor=tensor([[412.6720, 412.6720],
        [581.2230, 581.2230]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(Shard(dim=0), Shard(dim=1)))
</code></pre></div></div>

<p>Since we also apply FSDP here, the weights are row-wise sharded across in this output, but if you unshard the FSDP (i.e combine weights across FSDP group), you will see they have the same weights, i.e stack the rank 0,2 across dimension 0.</p>

<p>let’s also see if we get the same process two data on only one machine (no distributed training)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span><span class="o">*</span><span class="mi">2</span><span class="p">]</span>
<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer0</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer0</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">Model</span><span class="p">()</span>


<span class="nf">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="n">layer0</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">17.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>



<span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

<span class="n">optim</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">out0</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data_list</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">out0</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">out0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>

<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

<span class="n">out1</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data_list</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">out1</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">out1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>

<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>


<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">weights = </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">layer0</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">grad</span><span class="o">/</span><span class="mi">2</span><span class="si">}</span><span class="s"> </span><span class="se">\n\n</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>weights = tensor([[102.3585, 102.3585, 102.3585, 102.3585],
        [260.1725, 260.1725, 260.1725, 260.1725],
        [412.6720, 412.6720, 412.6720, 412.6720],
        [581.2230, 581.2230, 581.2230, 581.2230]])
</code></pre></div></div>

<p>I’ll be writing more on how all these techniques are used in distributed RL training in the next part.</p>

<p>While using parallelism, we need to be very careful to not break the computation graph, because collectives are not differentiable.</p>

<p>For example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@spawn_threads_and_init_comms</span>
<span class="k">def</span> <span class="nf">shard_big_tensor</span><span class="p">(</span><span class="n">world_size</span><span class="p">):</span>
    <span class="n">logsumexp</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="n">device_mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span>
        <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">DDP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">FSDP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="n">logsumexp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span> <span class="o">*</span> <span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span>
    <span class="n">logsumexp</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>

    <span class="n">logsumexps</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">logsumexp</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">device_mesh</span><span class="p">[</span><span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">].</span><span class="nf">size</span><span class="p">())]</span>
    <span class="n">dist</span><span class="p">.</span><span class="nf">all_gather</span><span class="p">(</span><span class="n">logsumexps</span><span class="p">,</span> <span class="n">logsumexp</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">[</span><span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">].</span><span class="nf">get_group</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">device_mesh</span><span class="p">[</span><span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">].</span><span class="nf">get_local_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">---detached ----</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">logsumexps</span><span class="p">)</span>

    <span class="n">dist</span><span class="p">.</span><span class="nf">barrier</span><span class="p">()</span>
    <span class="n">logsumexps</span><span class="p">[</span><span class="n">device_mesh</span><span class="p">[</span><span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">].</span><span class="nf">get_local_rank</span><span class="p">()]</span> <span class="o">=</span> <span class="n">logsumexp</span>

    <span class="k">if</span> <span class="n">device_mesh</span><span class="p">[</span><span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">].</span><span class="nf">get_local_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">----not detached----</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">logsumexps</span><span class="p">)</span>

    <span class="c1"># new_logsumexp = torch.stack(logsumexps, dim=-1).logsumexp(dim=-1)
</span>
    <span class="c1"># print(f'rank {dist.get_rank()} logsumexps {new_logsumexp}')
</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## output
---detached ---- [tensor([[0., 0., 0., 0.], [0., 0., 0., 0.]]), tensor([[1., 1., 1., 1.], [1., 1., 1., 1.]])]
---detached ---- [tensor([[2., 2., 2., 2.], [2., 2., 2., 2.]]), tensor([[3., 3., 3., 3.], [3., 3., 3., 3.]])]

----not detached---- [tensor([[0., 0., 0., 0.], [0., 0., 0., 0.]], requires_grad=True), tensor([[1., 1., 1., 1.], [1., 1., 1., 1.]])]

----not detached---- [tensor([[2., 2., 2., 2.], [2., 2., 2., 2.]], requires_grad=True), tensor([[3., 3., 3., 3.], [3., 3., 3., 3.]])]
</code></pre></div></div>

<p>The main culprit is the dist.all_gather which fills in the detached values of logsumexp into the logsumexps (as you can see no requires_grad in the elements in the logsumexps list), so when we do loss.backward() the gradient stops moving from this detached points and won’t travel to the downstream parameters.</p>

<p>But, using this line <code class="language-plaintext highlighter-rouge">logsumexps[device_mesh["TP"].get_local_rank()] = logsumexp</code> we fill in the local rank’s logsumexp in the logsumexps list, so now gradient’s will propagate through it.</p>

<p>Similar to that we need to use something similar while using all_reduce operation. During the forward pass the values will be the reduced one, and during backward, gradients can flow through the tensor variable.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">differentiable_all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">):</span>
    <span class="n">detached_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">detach</span><span class="p">()</span>
    <span class="n">dist</span><span class="p">.</span><span class="nf">all_reduce</span><span class="p">(</span>
        <span class="n">detached_tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="p">.</span><span class="n">ReduceOp</span><span class="p">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">device_mesh</span><span class="p">.</span><span class="nf">get_group</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">tens</span>                                                                                                                                                                                                                              <span class="ow">or</span> <span class="o">+</span> <span class="n">detached_tensor</span> <span class="o">-</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">detach</span><span class="p">()</span>

</code></pre></div></div>


<!-- <article class="blog-post">
  <header>
    <h1>Distributed Rl Training Step</h1>
    <time datetime="2025-09-01T00:00:00+05:45">
      September 01, 2025
    </time>
  </header>

  <div class="post-content"><p>I was learning how we can do distributed RL training, saw karpathy posting <a href="https://x.com/karpathy/status/1952076108565991588">this</a> and thought why not make a complete blog about what I learned so here it is.</p>

<p>The end goal of this blog is to explain clearly how to do distributed RL training, right now it contains explanations about fundamentals of distributed training, such as data parallelism, model parallelism, and tensor parallelism. Consider this as a part 1, where in the next blog I’ll be explaining how we apply the techniques learned in the blog.</p>

<p>Doing RL is simply not easy from the resource/computation standpoint because there isn’t just one model/optimizer states and gradients that we need to care of but the same model can be an actor, critic, reference policy and so on and we need to store them in our GPU. Doing RL even with 0.5B model with decent sequence length takes a LOT of GPU memory, which led me to understand how GPUs can work in a distributed manner (the part which I had always been ignoring).</p>

<h2 id="manual-distributed-data-parallel">Manual Distributed Data Parallel</h2>

<p>The simplest form of distributed training is distributed data parallel where <strong>each GPU</strong> has a copy of a model, but each GPU has different batches of data. Lets say we have batch_size = 64 and 2 GPUs, and a model. The model’s parameters will be replicated across both GPU. GPU1 will get (1, batch_size/2) and GPU2 will get (batch_size/2 , batch_size) data, each GPU will process on its own, find it’s own loss and find its own gradients and then sum those gradients over both the GPUs and average it (if needed), the averaged gradients will be replicated across both the GPUs and each will perform optimizer.step() on its own and end up with same set of parameters.</p>

<p>Let’s do manual distributed data parallel to see how it works under the hood with the help of a toy model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="c1">## ----------------- SETUP ----------------------------
</span><span class="n">rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">RANK</span><span class="sh">"</span><span class="p">])</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">WORLD_SIZE</span><span class="sh">"</span><span class="p">])</span> <span class="c1"># total number of GPUs
</span><span class="n">local_rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">LOCAL_RANK</span><span class="sh">"</span><span class="p">])</span> <span class="c1"># unique id for this GPU
</span><span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="n">backend</span> <span class="o">=</span> <span class="sh">'</span><span class="s">nccl</span><span class="sh">'</span> <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">else</span> <span class="sh">'</span><span class="s">gloo</span><span class="sh">'</span>
<span class="n">dist</span><span class="p">.</span><span class="nf">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>  <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>
<span class="c1">#torch.cuda.set_device(local_rank) # turn this on if you have GPU
## ----------------- SETUP ----------------------------
</span>
<span class="n">inp</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">9.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">9.</span><span class="p">,</span><span class="mf">17.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)]</span>


<span class="k">class</span> <span class="nc">ToyModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">net1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">5.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="n">self</span><span class="p">.</span><span class="n">net1</span><span class="p">.</span><span class="n">T</span>

<span class="n">local_data</span> <span class="o">=</span> <span class="n">inp</span><span class="p">[</span><span class="n">local_rank</span><span class="p">]</span> <span class="c1"># this way different gpus will get different data
</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">ToyModel</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">predicted</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">local_data</span><span class="p">)</span>
    <span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">predicted</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">/=</span> <span class="n">predicted</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s"> RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span> <span class="si">}</span><span class="se">\n</span><span class="s"> parameters</span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">net1</span><span class="p">.</span><span class="n">grad</span><span class="si">}</span><span class="s"> </span><span class="sh">'</span><span class="p">)</span>
    <span class="n">dist</span><span class="p">.</span><span class="nf">all_reduce</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">net1</span><span class="p">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="p">.</span><span class="n">ReduceOp</span><span class="p">.</span><span class="n">SUM</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">net1</span><span class="p">.</span><span class="n">grad</span> <span class="o">/=</span> <span class="n">world_size</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s"> AFTER ALL REDUCE </span><span class="se">\n</span><span class="s"> RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span> <span class="si">}</span><span class="se">\n</span><span class="s"> parameters</span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">net1</span><span class="p">.</span><span class="n">grad</span><span class="si">}</span><span class="s"> </span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># do the optimizer.step
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
<span class="n">dist</span><span class="p">.</span><span class="nf">destroy_process_group</span><span class="p">()</span>
</code></pre></div></div>

<p>run the code with <code class="language-plaintext highlighter-rouge">torchrun --node_rank=0 --nproc_per_node=2 --nnodes=1 --standalone test2.py</code>
<code class="language-plaintext highlighter-rouge">torchrun</code> will start different processes equals to the number specified in –nproc_per_node i.e 2 and this line <code class="language-plaintext highlighter-rouge">dist.init_process_group(backend=backend,  world_size=world_size)</code> will create a process group where ranks in the groups communicate with each other by using collectives (explained below). Each process will also get one rank, we can access it’s rank using <code class="language-plaintext highlighter-rouge">local_rank = int(os.environ["LOCAL_RANK"])</code> and can assign gpu to each process using <code class="language-plaintext highlighter-rouge">torch.cuda.set_device(local_rank)</code> for now I’ll be commenting our because, we can carry out these simple codes in CPU.</p>

<p>Now, we define our model structure, it’s a simply matrix multiplication (similar to nn.Linear with bias=False) and assign different data to different ranks using <code class="language-plaintext highlighter-rouge">local_data = inp[local_rank]</code>, we process different data on different process calculate loss, do backward and do
` dist.all_reduce(model.net1.grad, op=dist.ReduceOp.SUM)` it will first collect all the model.net1.grad from all the GPUs and sum it (provided by dist.ReduceOp.SUM) and transfer the summed result to each GPUs (thus the name reduce). The operation all_reduce should be intuitive by now.</p>

<p>This all_reduce is a collective operation performed by the <code class="language-plaintext highlighter-rouge">backend</code> i.e either ‘nccl’ or ‘gloo’.
‘nccl’ is mostly used for GPUs whereas ‘gloo’ can run in CPUs as well. Under the hood these backend implement the logic on how these collectives operations need to be implemented. all_reduce is one of many collective operations provided by the backend. Please late a look at the picture below to understand how these work.
<img src="/assets/images/2025-09-01-Distributed-RL-training-step/coll1.png" alt="coll1.png" />
<img src="/assets/images/2025-09-01-Distributed-RL-training-step/coll2.png" alt="coll2" />
Image source: <a href="https://docs.pytorch.org/tutorials/intermediate/dist_tuto.html">Writing Distributed Applications with PyTorch</a></p>

<p>You can take a look at this <a href="https://docs.pytorch.org/tutorials/intermediate/dist_tuto.html">source</a> if you want to understand how these collectives work under the hood.</p>

<h3 id="pytorchs-distributeddataparallel">Pytorch’s DistributedDataParallel</h3>

<p>The same function can be carried out by wrapping our model within pytorch’s DistributedDataParallel and we don’t have to call all_reduce manually, pytorch automatically takes care of that when we call loss.backward().</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>
<span class="c1">## ----------------- SETUP ----------------------------
</span><span class="n">rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">RANK</span><span class="sh">"</span><span class="p">])</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">WORLD_SIZE</span><span class="sh">"</span><span class="p">])</span>
<span class="n">local_rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">LOCAL_RANK</span><span class="sh">"</span><span class="p">])</span>
<span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="n">backend</span> <span class="o">=</span> <span class="sh">'</span><span class="s">nccl</span><span class="sh">'</span> <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">else</span> <span class="sh">'</span><span class="s">gloo</span><span class="sh">'</span>
<span class="n">dist</span><span class="p">.</span><span class="nf">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>  <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>
<span class="c1">## ----------------- SETUP ----------------------------
</span>
<span class="n">inp</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">9.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">9.</span><span class="p">,</span><span class="mf">17.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)]</span>


<span class="k">class</span> <span class="nc">ToyModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">net1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">5.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="n">self</span><span class="p">.</span><span class="n">net1</span><span class="p">.</span><span class="n">T</span>

<span class="n">local_data</span> <span class="o">=</span> <span class="n">inp</span><span class="p">[</span><span class="n">local_rank</span><span class="p">]</span> <span class="c1"># will get data based on its rank
</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">ToyModel</span><span class="p">()</span>
<span class="n">ddp_model</span> <span class="o">=</span> <span class="nc">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">ddp_model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">predicted</span> <span class="o">=</span> <span class="nf">ddp_model</span><span class="p">(</span><span class="n">local_data</span><span class="p">)</span>
    <span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="n">local_data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">predicted</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">/=</span> <span class="n">predicted</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span>

    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="c1"># dist.all_reduce(model.net1.grad, op=dist.ReduceOp.SUM)
</span>    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s"> AFTER ALL REDUCE </span><span class="se">\n</span><span class="s"> RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span> <span class="si">}</span><span class="se">\n</span><span class="s"> parameters</span><span class="se">\n</span><span class="s"> </span><span class="si">{</span> <span class="n">ddp_model</span><span class="p">.</span><span class="n">module</span><span class="p">.</span><span class="n">net1</span><span class="p">.</span><span class="n">grad</span><span class="si">}</span><span class="s"> </span><span class="sh">'</span><span class="p">)</span>
    <span class="c1"># do the optimizer.step
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
<span class="n">dist</span><span class="p">.</span><span class="nf">destroy_process_group</span><span class="p">()</span>
</code></pre></div></div>

<p>We need to take care of saving and loading the models which can be referred to <a href="https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html">this link</a></p>

<h3 id="fully-sharded-data-parallel-fsdp">Fully Sharded Data Parallel (FSDP)</h3>

<p>Let’s first remind ourselves about the drawbacks of DDP. DDP has a copy of whole model weights, gradients, and optimizer in each GPU. When models get bigger it won’t completely fit in One GPU’s memory. We don’t want to only fit model’s weight on a GPU, we need to fit it’s gradients, and the biiiiiig optimizer which contains twice the parameters as model’s weight.</p>

<p>How about distributing parameters on different GPUs and then gathering the required parameters when doing the specific operation. This is exactly what Fully Sharded Data Parallel does.</p>

<p>Let’s consider we have a layer that has a flat weight matrix and 3 GPUs, we divide 1/3 of the weights into each GPU as shown in the picture below and pass different batch to each GPU so that it can perform computation simultaneously.</p>

<p><img src="/assets/images/2025-09-01-Distributed-RL-training-step/fsdp1.png" alt="fsdp1" />
Image source: <a href="https://www.youtube.com/watch?v=6pVn6khIgiI">The SECRET Behind ChatGPT’s Training That Nobody Talks About | FSDP Explained</a></p>

<p>Data reaches the GPU but GPU doesn’t have the full weight to complete the computation, so each GPU calls all_gather collective to collect weights from all the GPUs. After all gather all 3 GPUs will have the full weight <code class="language-plaintext highlighter-rouge">[0..1024]</code> and it can do the computation as shown in the figure below.
<img src="/assets/images/2025-09-01-Distributed-RL-training-step/fsdp2.png" alt="fsdp2" />
After computing the activations it instantly frees up the GPU memory. Next, is the case of backward pass.</p>

<p>Similar to image above, it first all gathers the weights and compute the gradients, and reduce scatter so that each GPU only has gradients for their respective shard and then when we do optimizer.step() it will only update it’s respective shard of optimizer. See how we shard weights, gradients, and optimizer across GPUs but only gather weights, this helps a lot. BUT a GPU should have memory to at least store model’s full weights.
If you want to visually see how FSDP work, this <a href="https://www.youtube.com/watch?v=6pVn6khIgiI">video</a> might be very helpful.</p>

<p>We’ll see how FSDP shards weight in detail with code below along with Tensor Parallelism explained in the next section.</p>

<h3 id="tensor-parallelism">Tensor Parallelism</h3>

<p>Now let’s shard the computation. Previously, we sharded weights but in order to do computation (matrix multiplication), we all gathered all the parameters in each GPU then do the computation. Lets say we have very big parameter in our <code class="language-plaintext highlighter-rouge">net1</code>in the above example, so we shard that parameter into available GPUs, but when we do the .forward() call all weights need to be gathered inorder to do computation, so there’s a requirement that <code class="language-plaintext highlighter-rouge">net1</code>’s weight <strong>MUST</strong> fit in each GPU, what if it’s too big that it can’t fit in a single GPU. There’s a solution: Tensor Parallelism</p>

<p>As the name suggest we shard the matrix either row-wise or column-wise. Before understanding it would be easier to understand Tensor Parallelism if we remember some properties of matrix multiplication.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A = [  1   2   3   4
       5   6   7   8
       9  10  11  12
      13  14  15  16 ]

X = [ 1  0  0  0
      0  1  0  0 ]

# Shard A column wise, suppose we have 2 GPUS each half is stored in each GPU

A1 = [  1   2
        5   6
        9  10
       13  14 ]


A2 = [  3   4
        7   8
       11  12
       15  16 ]

X @ A1 = [ 1   2
           5   6 ]
X @ A2 = [ 3   4
           7   8 ]

# we can simply concatenate the two halves to get the full output

X @ A = [ 1   2    3   4
	      5   6   7   8 ]


</code></pre></div></div>

<p>Similarly, when we shard A row-wise,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
X @ A = [X @ A1 + X @ A2] # NOTE: "A" should be sharded such that its halves can be multiplied with X.
</code></pre></div></div>

<p>Let’s understand this with the help of this famous as well as the crucial implementation.
This is a sample code for FFN used in almost all the models these days. Since we apply gating mechanism here instead of using two weights we use three weights.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># forward in the FeedForward layer
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">w2</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">silu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">w1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="nf">w3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>

<p>Assume we have 2 GPUs. We have three weights.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>w1's shape = w3's shape =  (768, 1024)
w2's shape = (1024, 768)
</code></pre></div></div>

<p>We first shard w1 and w3’s weight column-wise and w2 row wise</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>GPU1 will get w1's (768, 512). # call it gpu1_w1
GPU1 will get w3's (768, 512). # call it gpu1_w3 and so on.
GPU1 will get w2's (512, 768)

------------------------------
GPU2 will get w1's (768, 512).
GPU2 will get w3's (768, 512).
GPU2 will get w2's (512, 768)

</code></pre></div></div>

<p>we have input x of shape (3,768) that is replicated across both the GPUs.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In GPU1, we can perform these operations below simultaneously,

a = x @ gpu1_w1
b = x @ gpu1_w3

since there was element-wise multplication between a and b ( from F.silu(self.w1(x)) * self.w3(x))

GPU1 can simply do c = a * b

then do

d1 = c @ gpu1_w2

--------------------------------------------------
While GPU1 was doing all these operations above, GPU2 was doing the same operation (at the same time) for it's own weights

a = x @ gpu2_w1
b = x @ gpu2_w3

since there was element-wise multplication between a and b ( from F.silu(self.w1(x)) * self.w3(x))

GPU1 can simply do c = a * b

then do

d2 = c @ gpu2_w2

</code></pre></div></div>

<p>Afterwards, we can simply do all_reduce that will add the d1 + d2 and provide GPU1 and GPU2 with the summed result.</p>

<p>Similarly, we can do row-wise and column-wise sharding for attention as well which can be seen in the illustration below.</p>

<p><img src="/assets/images/2025-09-01-Distributed-RL-training-step/attn1-n.png" alt="attn1-n" />
<img src="/assets/images/2025-09-01-Distributed-RL-training-step/attn2-n.png" alt="att2-n" /></p>

<p>Both should be easy to understand, if not please take a look at the <a href="https://insujang.github.io/2024-01-11/tensor-parallelism-and-sequence-parallelism-detailed-analysis/">source</a></p>

<p>Enough of theory, how do we do that in using pytorch?
——–&gt; <strong>DTensor</strong> &lt;——–</p>

<p>What is DTensor?</p>

<blockquote>
  <p>PyTorch DTensor offers simple and flexible tensor sharding primitives that transparently handles distributed logic,</p>
</blockquote>

<p>Basically, it aids sharding/replicating tensors in pytorch.
This is what we use under the hood in pytorch’s Tensor Parallelism and FSDP2.</p>

<p>Couple of things we need to understand properly are: device_mesh and placements.</p>

<p>For now lets consider we have 2 GPUs, with DeviceMesh of simple 1D mesh [0,1]. It simply denotes how GPUs are structured in a grid (more on 2D, 3D mesh later) and both 0 and 1 GPU will be used for Tensor Parallelism.</p>

<h4 id="placements">Placements</h4>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Shard</code>: Tensor sharded on the tensor dimension <code class="language-plaintext highlighter-rouge">dim</code> on the devices of the <code class="language-plaintext highlighter-rouge">DeviceMesh</code> dimension</li>
  <li><code class="language-plaintext highlighter-rouge">Replicate</code>: Tensor replicated on the devices of the <code class="language-plaintext highlighter-rouge">DeviceMesh</code> dimension</li>
  <li><code class="language-plaintext highlighter-rouge">Partial</code>: Tensor is pending reduction on the devices of the <code class="language-plaintext highlighter-rouge">DeviceMesh</code> dimension</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.distributed.device_mesh</span> <span class="kn">import</span> <span class="n">init_device_mesh</span>
<span class="c1"># from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy
</span><span class="kn">from</span> <span class="n">torch.distributed._composable.fsdp</span> <span class="kn">import</span> <span class="n">fully_shard</span>
<span class="kn">from</span> <span class="n">torch.distributed.tensor.parallel</span> <span class="kn">import</span> <span class="n">parallelize_module</span><span class="p">,</span> <span class="n">ColwiseParallel</span><span class="p">,</span> <span class="n">RowwiseParallel</span><span class="p">,</span> <span class="n">SequenceParallel</span><span class="p">,</span> <span class="n">PrepareModuleInput</span>
<span class="kn">from</span> <span class="n">torch.distributed._tensor</span> <span class="kn">import</span> <span class="n">Shard</span><span class="p">,</span> <span class="n">Replicate</span><span class="p">,</span> <span class="n">distribute_tensor</span><span class="p">,</span> <span class="n">DTensor</span>


<span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="n">backend</span> <span class="o">=</span> <span class="sh">'</span><span class="s">nccl</span><span class="sh">'</span> <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">else</span> <span class="sh">'</span><span class="s">gloo</span><span class="sh">'</span>

<span class="n">rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">RANK</span><span class="sh">"</span><span class="p">])</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">WORLD_SIZE</span><span class="sh">"</span><span class="p">])</span>
<span class="n">local_rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">LOCAL_RANK</span><span class="sh">"</span><span class="p">])</span>


<span class="n">dist</span><span class="p">.</span><span class="nf">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

<span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>


<span class="n">mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">17</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>

<span class="n">x_local</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Replicate</span><span class="p">()])</span>

<span class="n">y_local</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Replicate</span><span class="p">()])</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> X local </span><span class="si">{</span><span class="n">x_local</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> Y local </span><span class="si">{</span><span class="n">y_local</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> MM_local </span><span class="si">{</span><span class="n">x_local</span> <span class="o">@</span> <span class="n">y_local</span><span class="p">.</span><span class="n">T</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div>

<p>x and y are of type torch.Tensor and with distrbute_tensor they will be sharded/replicated based on the placements type over device_mesh. As you can see, we’ve replicated x and y so both of these will have the same output in both GPUs</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
RANK 1
 X local DTensor(local_tensor=tensor([[1, 2, 3, 4],
        [5, 6, 7, 8]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))
 Y local DTensor(local_tensor=tensor([[ 9, 10, 11, 12],
        [13, 14, 15, 16]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))
 MM_local DTensor(local_tensor=tensor([[110, 150],
        [278, 382]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))

RANK 0
 X local DTensor(local_tensor=tensor([[1, 2, 3, 4],
        [5, 6, 7, 8]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))
 Y local DTensor(local_tensor=tensor([[ 9, 10, 11, 12],
        [13, 14, 15, 16]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))
 MM_local DTensor(local_tensor=tensor([[110, 150],
        [278, 382]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))
</code></pre></div></div>

<p>Take this code now.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">17</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>

<span class="n">x_local</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Replicate</span><span class="p">()])</span>

<span class="n">y_local</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">0</span><span class="p">)])</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> Y local </span><span class="si">{</span><span class="n">y_local</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> MM_local </span><span class="si">{</span><span class="n">x_local</span> <span class="o">@</span> <span class="n">y_local</span><span class="p">.</span><span class="n">T</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div>

<p>we replicate x but we shard y so, y will be halved and saved with type Shard(0) in two GPUs with different value and x will be same in both GPUs. we multiply both and the output will be of type Shard(1) (why? because of transpose, the shard type is changed) remember this, this might be helpful later on. Here’s the output</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RANK 1 Y local DTensor(local_tensor=tensor([[13, 14, 15, 16]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=0),))
 MM_local DTensor(local_tensor=tensor([[150],
        [382]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=1),))

RANK 0 Y local DTensor(local_tensor=tensor([[ 9, 10, 11, 12]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=0),))
 MM_local DTensor(local_tensor=tensor([[110],
        [278]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=1),))
</code></pre></div></div>

<p>Let’s do some example similar to this code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">w2</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">silu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">w1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="nf">w3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">x</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Replicate</span><span class="p">()])</span>

<span class="n">w1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">w3</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">17.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">w1_local</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Shard</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)])</span>
<span class="n">w3_local</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">w3</span><span class="p">,</span> <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Shard</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)])</span>
<span class="n">w2_local</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">w2</span><span class="p">,</span> <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">0</span><span class="p">)])</span>


<span class="c1"># print(f'RANK {dist.get_rank()} \n X local {x_local} \n Y local {y_local} \n MM_local {x_local @ y_local.T}')
</span><span class="n">c</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">w1_local</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">w3_local</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">c</span> <span class="o">@</span> <span class="n">w2_local</span>

<span class="c1"># collect d from both GPUs
</span><span class="n">d_collect</span> <span class="o">=</span> <span class="n">d</span><span class="p">.</span><span class="nf">redistribute</span><span class="p">(</span><span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Replicate</span><span class="p">()])</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> w1.shape </span><span class="si">{</span><span class="n">w1_local</span><span class="p">.</span><span class="nf">to_local</span><span class="p">().</span><span class="n">shape</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s">  w3.shape </span><span class="si">{</span><span class="n">w3_local</span><span class="p">.</span><span class="nf">to_local</span><span class="p">().</span><span class="n">shape</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> w2.shape </span><span class="si">{</span><span class="n">w2_local</span><span class="p">.</span><span class="nf">to_local</span><span class="p">().</span><span class="n">shape</span><span class="si">}</span><span class="s"> c values </span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s">, </span><span class="se">\n</span><span class="s"> d values </span><span class="se">\n</span><span class="si">{</span><span class="n">d</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> d collect </span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">d_collect</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RANK 1 w1.shape torch.Size([2, 2])
  w3.shape torch.Size([2, 2])
 w2.shape torch.Size([2, 2]) c values
 DTensor(local_tensor=tensor([[260., 336.],
        [260., 336.],
        [260., 336.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=1),)),
 d values
DTensor(local_tensor=tensor([[596., 596.],
        [596., 596.],
        [596., 596.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Partial(sum),))
 d collect
 DTensor(local_tensor=tensor([[788., 596.],
        [788., 596.],
        [788., 596.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))
        ----------------------------------------------------------
RANK 0 w1.shape torch.Size([2, 2])
  w3.shape torch.Size([2, 2])
 w2.shape torch.Size([2, 2]) c values
 DTensor(local_tensor=tensor([[132., 192.],
        [132., 192.],
        [132., 192.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=1),)),
 d values
DTensor(local_tensor=tensor([[192.,   0.],
        [192.,   0.],
        [192.,   0.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Partial(sum),))
 d collect
 DTensor(local_tensor=tensor([[788., 596.],
        [788., 596.],
        [788., 596.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))
</code></pre></div></div>

<p>as you can see d has two different values with placement type Partial and when we call redistribute on d with Replicate() type the result is all gathered (summed) and will be same on both GPU.</p>

<p>This was all manual Tensor Parallelism. We don’t go on writing these for all the matrix multiplications so we use some different method which implements the same logic under the hood.</p>

<h4 id="colwiseparallel">ColwiseParallel()</h4>

<blockquote>
  <p>Partition a compatible nn.Module in a column-wise fashion.</p>
</blockquote>

<p>NOTE: ColwiseParallel() only operates on nn.Linear() and nn.Embedding() module.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">ToyModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ToyModel</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">w1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">w3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">w2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">self</span><span class="p">.</span><span class="n">w1</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span> <span class="c1"># cause weight will have shape opposite of whats specified in nn.Linear()
</span>            <span class="n">self</span><span class="p">.</span><span class="n">w3</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">17.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">w2</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">w2</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">w1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="nf">w3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">ToyModel</span><span class="p">()</span>

<span class="n">tp_plan</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">w1</span><span class="sh">"</span> <span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">w2</span><span class="sh">"</span> <span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">w3</span><span class="sh">"</span> <span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">()</span>
<span class="p">}</span>

<span class="n">model</span> <span class="o">=</span> <span class="nf">parallelize_module</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">tp_plan</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Replicate</span><span class="p">()])</span> <span class="c1"># this is simply a tensor because ColwiseParallel will automatically convert it's input from torch.Tensor to torch.DTensor
</span><span class="n">out</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> w1.weight </span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">w1</span><span class="p">.</span><span class="n">weight</span><span class="si">}</span><span class="s">, </span><span class="se">\n</span><span class="s"> MM values </span><span class="se">\n</span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="nf">w1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">model</span><span class="p">.</span><span class="nf">w3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s"> output values </span><span class="se">\n</span><span class="si">{</span><span class="n">out</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RANK 1 w1.weight
 DTensor(local_tensor=tensor([[5., 6.],
        [7., 8.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)),
 MM values
tensor([[297., 465.],
        [297., 465.],
        [297., 465.]], grad_fn=&lt;MulBackward0&gt;) output values
AsyncCollectiveTensor(tensor([[ 0., 57.],
        [ 0., 57.],
        [ 0., 57.]]))

RANK 0 w1.weight
 DTensor(local_tensor=tensor([[1., 2.],
        [3., 4.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)),
 MM values
tensor([[ 57., 161.],
        [ 57., 161.],
        [ 57., 161.]], grad_fn=&lt;MulBackward0&gt;) output values
AsyncCollectiveTensor(tensor([[ 0., 57.],
        [ 0., 57.],
        [ 0., 57.]])
</code></pre></div></div>

<p>This code tries to exactly do the task that we did previously, the logic is the same but the output is different because of how the arranged weights is transposed and sharded.</p>

<p>Let’s me explain how ColwiseParallel works internally.</p>

<ul>
  <li>It takes input x, if it’s a tensor it will make it a DTensor and if not in the desired layout it will redistribute it to be the desired layout.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="n">self</span><span class="p">.</span><span class="n">input_layouts</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_layouts</span> <span class="ow">or</span> <span class="nc">Replicate</span><span class="p">(),)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_layouts</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_layouts</span> <span class="ow">or</span> <span class="nc">Shard</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">desired_input_layouts</span> <span class="o">=</span> <span class="p">(</span><span class="nc">Replicate</span><span class="p">(),)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">_prepare_input_fn</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="p">,</span> <span class="n">desired_input_layouts</span><span class="p">,</span> <span class="n">mod</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">device_mesh</span>
    <span class="p">):</span>
        <span class="c1"># TODO: figure out dynamo support for instance method and switch this to instance method
</span>
        <span class="c1"># annotate module input placements/sharding with input_layouts
</span>        <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">DTensor</span><span class="p">):</span>
            <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">DTensor</span><span class="p">.</span><span class="nf">from_local</span><span class="p">(</span>
                <span class="n">input_tensor</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">,</span> <span class="n">input_layouts</span><span class="p">,</span> <span class="n">run_check</span><span class="o">=</span><span class="bp">False</span>
            <span class="p">)</span>

        <span class="c1"># transform the input layouts to the desired layouts of ColwiseParallel
</span>        <span class="k">if</span> <span class="n">input_layouts</span> <span class="o">!=</span> <span class="n">desired_input_layouts</span><span class="p">:</span>
            <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="p">.</span><span class="nf">redistribute</span><span class="p">(</span>
                <span class="n">placements</span><span class="o">=</span><span class="n">desired_input_layouts</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="bp">True</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">input_tenso</span>
</code></pre></div></div>

<ul>
  <li>ColwiseParallel will shard the module’s weights on 0’th dimension not 1, why??</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    <span class="k">def</span> <span class="nf">_partition_linear_fn</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">):</span>
        <span class="c1"># colwise shard weight/bias to Shard(0), weight be Shard(0)
</span>        <span class="c1"># means Colwise as Linear is input * weight^T + bias, where
</span>        <span class="c1"># weight would become Shard(1)
</span>        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">module</span><span class="p">.</span><span class="nf">named_parameters</span><span class="p">():</span>
            <span class="n">dist_param</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span>
                <span class="nf">distribute_tensor</span><span class="p">(</span>
                    <span class="n">param</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">,</span> <span class="p">[</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">0</span><span class="p">)],</span> <span class="n">src_data_rank</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">src_data_rank</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">module</span><span class="p">.</span><span class="nf">register_parameter</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">dist_param</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>because under the hood in nn.Linear, the operation is x @ w.T, the weights are transposed, so</li>
  <li>weights are sharded on dim=0, but output will be Shard(-1) because we multiply with the transpose weight (see example above where we implemented MM with transposed weights), and by default use_local_output is True which will convert the DTensor back to torch.Tensor</li>
</ul>

<p>This is a part of code from pytorch’s ColwiseParallel() <a href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/tensor/parallel/style.py#L89">here</a> shows the default input and output layouts for ColwiseParallel().</p>

<h4 id="rowwiseparallel">RowwiseParallel()</h4>

<p>it works exactly the opposite of ColwiseParallel(), it shards weights on dim=1, i.e Shard(dim=1)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="n">self</span><span class="p">.</span><span class="n">input_layouts</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_layouts</span> <span class="ow">or</span> <span class="nc">Shard</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_layouts</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_layouts</span> <span class="ow">or</span> <span class="nc">Replicate</span><span class="p">(),</span>
</code></pre></div></div>

<p>and it’s desired layout is Shard(-1).</p>

<p>That’s it, we need to take care of three things.</p>

<ol>
  <li>what input it expects (i.e Shard() or Replicate())</li>
  <li>what it outputs (i.e Shar() or Replicate())</li>
  <li>How it shards it’s module weights.</li>
</ol>

<h4 id="sequenceparallel">SequenceParallel()</h4>

<p>It generally operates on the sequence dimension. Say the activations are of shape B,T,C = batch_size, sequence_length, embedding dimension.</p>

<p>It will shard the sequence dimension T. There is no sharding of weight in this SequenceParallel(). It is applied to the LayerNorm()/RMSNorm() it will split on T and calculate mean/std based on the given T, there’s no need for communication because mean/std are calculated on C’th dimension and don’t depend on T.</p>

<p>However, the <strong>output</strong> of LayerNorm/RMSNorm is typically <strong>all-gathered</strong> across the <strong>T</strong> shards before it is consumed by the next layer (e.g., an attention or MLP block) (more on this below)
self.sequence_sharding = (Shard(sequence_dim)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">_prepare_input_fn</span><span class="p">(</span><span class="n">sequence_sharding</span><span class="p">,</span> <span class="n">mod</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">):</span>
        <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">DTensor</span><span class="p">):</span>
            <span class="c1"># if the passed in input DTensor is not sharded on the sequence dim, we need to redistribute it
</span>            <span class="k">if</span> <span class="n">input_tensor</span><span class="p">.</span><span class="n">placements</span> <span class="o">!=</span> <span class="n">sequence_sharding</span><span class="p">:</span>
                <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="p">.</span><span class="n">redistribute</span><span class="p">(</span>
                    <span class="n">placements</span><span class="o">=</span><span class="n">sequence_sharding</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="bp">True</span>
		<span class="k">return</span> <span class="n">input_tens</span>
</code></pre></div></div>

<h4 id="tensor-parallelism-on-transformers-model">Tensor Parallelism on Transformers Model</h4>

<p>Let’s apply what we learned above in a transformers model. I won’t be providing the output for this but will only explain what it’s doing. The code below is taken from <a href="https://docs.pytorch.org/tutorials/intermediate/TP_tutorial.html">here</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.distributed.device_mesh</span> <span class="kn">import</span> <span class="n">init_device_mesh</span>
<span class="c1"># from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy
</span><span class="kn">from</span> <span class="n">torch.distributed._composable.fsdp</span> <span class="kn">import</span> <span class="n">fully_shard</span>
<span class="kn">from</span> <span class="n">torch.distributed.tensor.parallel</span> <span class="kn">import</span> <span class="n">parallelize_module</span><span class="p">,</span> <span class="n">ColwiseParallel</span><span class="p">,</span> <span class="n">RowwiseParallel</span><span class="p">,</span> <span class="n">SequenceParallel</span><span class="p">,</span> <span class="n">PrepareModuleInput</span>
<span class="kn">from</span> <span class="n">torch.distributed._tensor</span> <span class="kn">import</span> <span class="n">Shard</span><span class="p">,</span> <span class="n">Replicate</span><span class="p">,</span> <span class="n">distribute_tensor</span><span class="p">,</span> <span class="n">DTensor</span>

<span class="kn">from</span> <span class="n">llama2_model</span> <span class="kn">import</span> <span class="n">Transformer</span><span class="p">,</span> <span class="n">ModelArgs</span>


<span class="c1"># import warnings # ignore all warning messages
# warnings.filterwarnings("ignore")
</span>
<span class="c1"># Code for helping init process group
</span><span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="n">backend</span> <span class="o">=</span> <span class="sh">'</span><span class="s">nccl</span><span class="sh">'</span> <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">else</span> <span class="sh">'</span><span class="s">gloo</span><span class="sh">'</span>

<span class="n">rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">RANK</span><span class="sh">"</span><span class="p">])</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">WORLD_SIZE</span><span class="sh">"</span><span class="p">])</span>
<span class="n">local_rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">LOCAL_RANK</span><span class="sh">"</span><span class="p">])</span>

<span class="c1"># print('local rank', local_rank)
</span>
<span class="n">dist</span><span class="p">.</span><span class="nf">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

<span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>

<span class="n">simple_llama2_config</span> <span class="o">=</span> <span class="nc">ModelArgs</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">.</span><span class="nf">from_model_args</span><span class="p">(</span><span class="n">simple_llama2_config</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># model.init_weights()
</span>
<span class="c1"># """
# mesh = init_device_mesh('cpu', (2, 4), mesh_dim_names=["FSDP", "TP"])
</span>
<span class="n">mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">])</span>

<span class="n">layer_tp_plan</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># Now the input and output of SequenceParallel has Shard(1) layouts,
</span>    <span class="c1"># to represent the input/output tensors sharded on the sequence dimension
</span>    <span class="sh">"</span><span class="s">attention_norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">attention</span><span class="sh">"</span><span class="p">:</span> <span class="nc">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="nc">Replicate</span><span class="p">()),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Replicate</span><span class="p">(),</span> <span class="nc">Replicate</span><span class="p">()),</span>
    <span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wq</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wk</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wv</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wo</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
    <span class="sh">"</span><span class="s">ffn_norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">feed_forward</span><span class="sh">"</span><span class="p">:</span> <span class="nc">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Replicate</span><span class="p">(),),</span>
    <span class="p">),</span>
    <span class="sh">"</span><span class="s">feed_forward.w1</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">feed_forward.w2</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
    <span class="sh">"</span><span class="s">feed_forward.w3</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(),</span>
<span class="p">}</span>
<span class="c1"># Apply TP
</span><span class="k">for</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">transformer_block</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">):</span>
    <span class="c1"># layer_tp_plan = {...}  # i.e. the plan we just generated
</span>
    <span class="nf">parallelize_module</span><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="n">transformer_block</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">parallelize_plan</span><span class="o">=</span><span class="n">layer_tp_plan</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nf">parallelize_module</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">tok_embeddings</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">(),</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">),</span>
        <span class="sh">"</span><span class="s">norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
        <span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">()</span>
        <span class="p">),</span>
    <span class="p">}</span>
<span class="p">)</span>
</code></pre></div></div>

<p>you can pass input x through model() and inspect where the weights for each modules are stored and what the output will be.</p>

<p>All we need to change/understand is the parallelize_plan. let’s understand the sequence of parallelization in our model that we have created above with the help of Colwise Rowwise and Sequence Parallel.</p>

<p>Step 1</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="sh">"</span><span class="s">tok_embeddings</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">(),</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">),</span>
</code></pre></div></div>

<p>x = (B,T)
out = tok_embeddings(x)
tok_embeddings.weight is sharded by dim=0, output must be replicate but we apply output_layouts=Shard(1) cause next we apply SequenceParallel that expects Shard(1) as input</p>

<p>so out is of shape (B,T/2) its is sharded by dim=1 but its a torch.Tensor if GPU=2</p>

<p>Step 2</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="sh">"</span><span class="s">attention_norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
</code></pre></div></div>

<p>out = attention_norm(out)
converts out(B,T/2, C) which is a torch.Tensor to Shard(1) with prepare_input_fn. SequenceParallel is applied (no weight sharding), only applies LayerNorm/RMSNorm on T/2 tokens in each GPUs. no change of output layout i.e its still Shard(1) of shape B,T/2,C</p>

<p>Step 3</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="sh">"</span><span class="s">attention</span><span class="sh">"</span><span class="p">:</span> <span class="nc">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="nc">Replicate</span><span class="p">()),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Replicate</span><span class="p">(),</span> <span class="nc">Replicate</span><span class="p">()),</span>
    <span class="p">),</span>

</code></pre></div></div>

<p>attention expects whole sequence so convert Shard(1) to Replicate(). under the hood out is combined to form (B,T,C) from B,T/2, C and replicate to both GPUs. Thats what PrepareModuleInput does. (Shard(1), Replicate()) is for different inputs to the attention module Shard(1) is for x and Replicate() is for freq_cis</p>

<p>Step 4</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="sh">"</span><span class="s">attention.wq</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wk</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wv</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
</code></pre></div></div>

<p>apply Shard(0) to attention.wq.weight, but output is Shard(-1) (why? already explained above).</p>

<p>Step 5</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"</span><span class="s">attention.wo</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<p>Expects input to be Shard(-1) which is correct, should output replicate (after doing all reduce of the partial sum) but instead converts to Shard(1) because next we apply SequenceParallelism for ffn_norm.</p>

<p>Other steps should be straightforward, follow the sequence described below. Finally the output (B,T,vocab_size) is replicated across both the GPUs.</p>

<p>In pytorch’s Tensor Parallel docs they suggest also adding loss parallel, because replicating the output logits which is of size (B,T,vocab_size) can be expensive. The only change we need to do is keep the use_local_output which will make the output of that output module to be DTensor.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nf">parallelize_module</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">tp_mesh</span><span class="p">,</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">tok_embeddings</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">(),</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">),</span>
        <span class="sh">"</span><span class="s">norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
        <span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="c1"># use DTensor as the output
</span>            <span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">},</span>
<span class="p">)</span>
</code></pre></div></div>

<p>and apply loss_parallel() context.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">torch.distributed.tensor.parallel</span> <span class="kn">import</span> <span class="n">loss_parallel</span>

<span class="n">pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
<span class="k">with</span> <span class="nf">loss_parallel</span><span class="p">():</span>
    <span class="c1"># assuming pred and labels are of the shape [batch, seq, vocab]
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">pred</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
</code></pre></div></div>

<p>I don’t seem to understand this, because they don’t provide enough explanation on their <a href="https://docs.pytorch.org/tutorials/intermediate/TP_tutorial.html">page</a>.</p>

<p>My assumption is that each gpus will have (B,T,vocab_size/2) matrix, loss calculation depends on the full vocab_size cause we need to convert the logits into probabilities using softmax function. What they could be doing is calculate the local sum of all e^(x) in vocab_size do all_reduce and then calculate loss.</p>

<h3 id="device-mesh">Device mesh</h3>

<h4 id="2d-mesh">2D mesh</h4>

<p>It’s very important that we have no confusion on how device mesh works, and how TP and FSDP works using the device mesh. (it has haunted for a day, because I did not understand it properly)</p>

<p>let’s first start with a 2D mesh and apply tensor parallelism.
our original mesh will look like this</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[0,1],
[2,3]]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">FSDP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">])</span>

<span class="n">layer_tp_plan</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># Now the input and output of SequenceParallel has Shard(1) layouts,
</span>    <span class="c1"># to represent the input/output tensors sharded on the sequence dimension
</span>    <span class="sh">"</span><span class="s">attention_norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">attention</span><span class="sh">"</span><span class="p">:</span> <span class="nc">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="nc">Replicate</span><span class="p">()),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Replicate</span><span class="p">(),</span> <span class="nc">Replicate</span><span class="p">()),</span>
    <span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wq</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wk</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wv</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wo</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
    <span class="sh">"</span><span class="s">ffn_norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">feed_forward</span><span class="sh">"</span><span class="p">:</span> <span class="nc">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Replicate</span><span class="p">(),),</span>
    <span class="p">),</span>
    <span class="sh">"</span><span class="s">feed_forward.w1</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">feed_forward.w2</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
    <span class="sh">"</span><span class="s">feed_forward.w3</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(),</span>
<span class="p">}</span>
<span class="c1"># Apply TP
</span><span class="k">for</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">transformer_block</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">):</span>
    <span class="c1"># layer_tp_plan = {...}  # i.e. the plan we just generated
</span>
    <span class="nf">parallelize_module</span><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="n">transformer_block</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">parallelize_plan</span><span class="o">=</span><span class="n">layer_tp_plan</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nf">parallelize_module</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">tok_embeddings</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">(),</span>
            <span class="c1"># output_layouts=Shard(1),
</span>        <span class="p">),</span>
        <span class="sh">"</span><span class="s">norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
        <span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">()</span>
        <span class="p">),</span>
    <span class="p">}</span>
<span class="p">)</span>
</code></pre></div></div>

<p>we apply TP on only the TP mesh <code class="language-plaintext highlighter-rouge">mesh['TP']</code>
but what is the output of <code class="language-plaintext highlighter-rouge">mesh['TP']</code> ?</p>

<p>there could be different ways to understand this, but I understand it this way.
Assume we are in process 0, i.e
our original mesh is <code class="language-plaintext highlighter-rouge">mesh['FSDP','TP']</code>, we try to get <code class="language-plaintext highlighter-rouge">mesh['TP']</code> where FSDP is not there so lets draw a line across ‘FSDP’ dimensions (0th and 1st) and see which TP sub-matrix falls in the drawn line. since we were in process 0, submatrix <code class="language-plaintext highlighter-rouge">[0,1]</code> is chosen and our process 0 will only <code class="language-plaintext highlighter-rouge">[0,1]</code> when we output <code class="language-plaintext highlighter-rouge">mesh['TP']</code> and ignore others. We’ll get the same when we are on process 1, but we’ll get <code class="language-plaintext highlighter-rouge">[2,3]</code> when we’re on either 2 or 3 processes.</p>

<p>so the simple logic is to draw line across dimension that is not in the specified mesh i.e FSDP in this case, and see on which submatrix our current process falls. It’s better to get a pen and paper and understand it deeply.
<img src="/assets/images/2025-09-01-Distributed-RL-training-step/fsdp_mat.png" alt="fsdp_mat" /></p>

<p>We pass this mesh to parallelize_module that takes in only 1D mesh and applies the Sharding plan across that 1D mesh, For example, if we are in process 0, the weights of size (8,8) are sharded across 0 and 1 GPUs so each gpu will get (4,8) (assume Shard(0)) and if we’re on process 2 weights are sharded across 2,3.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="nf">parallelize_module</span><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="n">transformer_block</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">parallelize_plan</span><span class="o">=</span><span class="n">layer_tp_plan</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></div>

<p>now let’s apply the FSDP sharding</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
	<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
		<span class="nf">fully_shard</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">])</span>

	<span class="nf">fully_shard</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div>

<p>As we did above (drawing line and getting submatrix), process 0 will see mesh[‘FSDP’] = [0,2]
and process 1 will see [1,3].</p>

<p>fully_shard takes two types of mesh 1D, and 2D mesh. Since we have 1D mesh in this case, it will shard the weights across the 1D mesh with Shard(0) placement. i.e let’s say we’re on process 0, 0 already had (4,8) matrix, and when we apply fully_shard the dimension 0 is again split and process 0 will get (2,8) matrix and it’s other half will be stored in process 2. If you’re confused please bear with me, we’ll see with examples on 3D mesh below.</p>

<h4 id="3d-mesh">3D mesh</h4>

<p><img src="/assets/images/2025-09-01-Distributed-RL-training-step/matrix-mul.png" alt="matrix-mul" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
<span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]]</span>

<span class="p">[[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span>
<span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">]]]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">DDP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">FSDP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">])</span>
</code></pre></div></div>

<p>We apply the same TP plan with mesh[‘TP’], similar to above, 0,1 will see mesh [0,1] 2,3 will see mesh [2,3] and so on. Let’s see one worked out example, we initially have attention_wq matrix’s weight of size (8,8) arranged in order. We only apply TP (no FSDP). So each TP axis should have same sharded attention_wq matrix i.e for TP axis 0, rank 0,2,4,6 will have same matrix and for TP axis 1, ranks 1,2,5,7 will have same matrix.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.distributed.device_mesh</span> <span class="kn">import</span> <span class="n">init_device_mesh</span>
<span class="c1"># from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy
</span><span class="kn">from</span> <span class="n">torch.distributed._composable.fsdp</span> <span class="kn">import</span> <span class="n">fully_shard</span>
<span class="kn">from</span> <span class="n">torch.distributed.tensor.parallel</span> <span class="kn">import</span> <span class="n">parallelize_module</span><span class="p">,</span> <span class="n">ColwiseParallel</span><span class="p">,</span> <span class="n">RowwiseParallel</span><span class="p">,</span> <span class="n">SequenceParallel</span><span class="p">,</span> <span class="n">PrepareModuleInput</span>
<span class="kn">from</span> <span class="n">torch.distributed._tensor</span> <span class="kn">import</span> <span class="n">Shard</span><span class="p">,</span> <span class="n">Replicate</span><span class="p">,</span> <span class="n">distribute_tensor</span><span class="p">,</span> <span class="n">DTensor</span>

<span class="kn">from</span> <span class="n">llama2_model</span> <span class="kn">import</span> <span class="n">Transformer</span><span class="p">,</span> <span class="n">ModelArgs</span>


<span class="c1"># import warnings # ignore all warning messages
# warnings.filterwarnings("ignore")
</span>
<span class="c1"># Code for helping init process group
</span><span class="n">device</span> <span class="o">=</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="n">backend</span> <span class="o">=</span> <span class="sh">'</span><span class="s">gloo</span><span class="sh">'</span>

<span class="n">rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">RANK</span><span class="sh">"</span><span class="p">])</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">WORLD_SIZE</span><span class="sh">"</span><span class="p">])</span>
<span class="n">local_rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">LOCAL_RANK</span><span class="sh">"</span><span class="p">])</span>

<span class="c1"># print('local rank', local_rank)
</span>
<span class="n">dist</span><span class="p">.</span><span class="nf">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

<span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>

<span class="n">simple_llama2_config</span> <span class="o">=</span> <span class="nc">ModelArgs</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">.</span><span class="nf">from_model_args</span><span class="p">(</span><span class="n">simple_llama2_config</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># model.tok_embeddings.weight = nn.Parameter(torch.arange(1.,33.).reshape(8,4))
</span>
<span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">attention</span><span class="p">.</span><span class="n">wq</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">65.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>



<span class="n">mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">DDP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">FSDP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">])</span>
<span class="n">layer_tp_plan</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># Now the input and output of SequenceParallel has Shard(1) layouts,
</span>    <span class="c1"># to represent the input/output tensors sharded on the sequence dimension
</span>    <span class="sh">"</span><span class="s">attention_norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">attention</span><span class="sh">"</span><span class="p">:</span> <span class="nc">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="nc">Replicate</span><span class="p">()),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Replicate</span><span class="p">(),</span> <span class="nc">Replicate</span><span class="p">()),</span>
    <span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wq</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wk</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wv</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wo</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
    <span class="sh">"</span><span class="s">ffn_norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">feed_forward</span><span class="sh">"</span><span class="p">:</span> <span class="nc">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Replicate</span><span class="p">(),),</span>
    <span class="p">),</span>
    <span class="sh">"</span><span class="s">feed_forward.w1</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">feed_forward.w2</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
    <span class="sh">"</span><span class="s">feed_forward.w3</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(),</span>
<span class="p">}</span>
<span class="c1"># Apply TP
</span><span class="k">for</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">transformer_block</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">):</span>
    <span class="c1"># layer_tp_plan = {...}  # i.e. the plan we just generated
</span>
    <span class="nf">parallelize_module</span><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="n">transformer_block</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">parallelize_plan</span><span class="o">=</span><span class="n">layer_tp_plan</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nf">parallelize_module</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">tok_embeddings</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">(),</span>
            <span class="c1"># output_layouts=Shard(1),
</span>        <span class="p">),</span>
        <span class="sh">"</span><span class="s">norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
        <span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">()</span>
        <span class="p">),</span>
    <span class="p">}</span>
<span class="p">)</span>


<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">17</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">16</span><span class="p">)</span>

<span class="k">if</span> <span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">MESHDIM </span><span class="si">{</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">DDP</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s"> TP MESH </span><span class="si">{</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># import time
# time.sleep(5)
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Global rank: </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s">, attn_wq </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">attention</span><span class="p">.</span><span class="n">wq</span><span class="p">.</span><span class="n">weight</span><span class="si">}</span><span class="s"> shape: </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">attention</span><span class="p">.</span><span class="n">wq</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="nf">to_local</span><span class="p">().</span><span class="n">shape</span><span class="si">}</span><span class="s"> </span><span class="se">\n\n</span><span class="sh">'</span><span class="p">)</span>
<span class="c1"># print(f'Global rank: {dist.get_rank()}, \n\n OUTPUT:\n {model.tok_embeddings(x).shape}')
</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>output
Global rank: 5, tok_embeddings DTensor(local_tensor=tensor([[33., 34., 35., 36., 37., 38., 39., 40.],
        [41., 42., 43., 44., 45., 46., 47., 48.],
        [49., 50., 51., 52., 53., 54., 55., 56.],
        [57., 58., 59., 60., 61., 62., 63., 64.]]), device_mesh=DeviceMesh('cpu', [4, 5], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])


Global rank: 4, tok_embeddings DTensor(local_tensor=tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12., 13., 14., 15., 16.],
        [17., 18., 19., 20., 21., 22., 23., 24.],
        [25., 26., 27., 28., 29., 30., 31., 32.]]), device_mesh=DeviceMesh('cpu', [4, 5], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])


MESHDIM DeviceMesh('cpu', [[0, 2], [4, 6]], mesh_dim_names=('DDP', 'FSDP')) TP MESH DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',))

Global rank: 1, tok_embeddings DTensor(local_tensor=tensor([[33., 34., 35., 36., 37., 38., 39., 40.],
        [41., 42., 43., 44., 45., 46., 47., 48.],
        [49., 50., 51., 52., 53., 54., 55., 56.],
        [57., 58., 59., 60., 61., 62., 63., 64.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])


Global rank: 0, tok_embeddings DTensor(local_tensor=tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12., 13., 14., 15., 16.],
        [17., 18., 19., 20., 21., 22., 23., 24.],
        [25., 26., 27., 28., 29., 30., 31., 32.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])


Global rank: 7, tok_embeddings DTensor(local_tensor=tensor([[33., 34., 35., 36., 37., 38., 39., 40.],
        [41., 42., 43., 44., 45., 46., 47., 48.],
        [49., 50., 51., 52., 53., 54., 55., 56.],
        [57., 58., 59., 60., 61., 62., 63., 64.]]), device_mesh=DeviceMesh('cpu', [6, 7], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])


Global rank: 6, tok_embeddings DTensor(local_tensor=tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12., 13., 14., 15., 16.],
        [17., 18., 19., 20., 21., 22., 23., 24.],
        [25., 26., 27., 28., 29., 30., 31., 32.]]), device_mesh=DeviceMesh('cpu', [6, 7], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])


Global rank: 3, tok_embeddings DTensor(local_tensor=tensor([[33., 34., 35., 36., 37., 38., 39., 40.],
        [41., 42., 43., 44., 45., 46., 47., 48.],
        [49., 50., 51., 52., 53., 54., 55., 56.],
        [57., 58., 59., 60., 61., 62., 63., 64.]]), device_mesh=DeviceMesh('cpu', [2, 3], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])


Global rank: 2, tok_embeddings DTensor(local_tensor=tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12., 13., 14., 15., 16.],
        [17., 18., 19., 20., 21., 22., 23., 24.],
        [25., 26., 27., 28., 29., 30., 31., 32.]]), device_mesh=DeviceMesh('cpu', [2, 3], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])
</code></pre></div></div>

<p>Now let’s apply the FSDP on mesh[‘DDP’, ‘FSDP’]</p>

<p>but let’s first understand what will our processes. see when we simply print mesh[‘DDP’, ‘FSDP’]. As explained above, apply the rule above, which dimension is not there, draw the line and erase its brackets. In this case, ‘TP’ dimension is not there so if we are on process 0, it will see matrix <code class="language-plaintext highlighter-rouge">[[0,2], [4,6]]</code> which I’ve already printed in the output above as</p>

<p><code class="language-plaintext highlighter-rouge">MESHDIM DeviceMesh('cpu', [[0, 2], [4, 6]], mesh_dim_names=('DDP', 'FSDP')) TP MESH DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)) </code></p>

<p>but when we’re on process 1 it will see <code class="language-plaintext highlighter-rouge">[[1,3], [5,7]]</code> and so on.</p>

<p>let’s apply FSDP with mesh[‘DDP’, ‘FSDP’]</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
	<span class="nf">fully_shard</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">DDP</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">])</span>

<span class="nf">fully_shard</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">DDP</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div>

<p>the sharding rule isn’t the same when we pass 2D mesh to fully_shard.</p>

<blockquote>
  <p><strong>mesh</strong> (<em>Optional__[</em><a href="https://docs.pytorch.org/docs/stable/distributed.html#torch.distributed.device_mesh.DeviceMesh" title="torch.distributed.device_mesh.DeviceMesh"><em>DeviceMesh</em></a><em>]</em>) – This data parallel mesh defines the sharding and device. If 1D, then parameters are fully sharded across the 1D mesh (FSDP) with <code class="language-plaintext highlighter-rouge">(Shard(0),)</code> placement. If 2D, then parameters are sharded across the 1st dim and replicated across the 0th dim (HSDP) with <code class="language-plaintext highlighter-rouge">(Replicate(), Shard(0))</code> placement. The mesh’s device type gives the device type used for communication; if a CUDA or CUDA-like device type, then we use the current device.</p>
</blockquote>

<p>so assuming we’re on process 0, and it has attention_wq matrix of size (4,8). This (4,8) matrix will be across 1st dimension i.e 0 will get (2,8) matrix and 2 will get the other half (2,8) and this is replicated across 0th dimension i.e process 0 will have the same (2,8) matrix as process 4 and 2 will have same (2,8) matrix as 6.</p>

<p>Here’s the output</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Global rank: 3, tok_embeddings DTensor(local_tensor=tensor([[49., 50., 51., 52., 53., 54., 55., 56.],
        [57., 58., 59., 60., 61., 62., 63., 64.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])


Global rank: 2, tok_embeddings DTensor(local_tensor=tensor([[17., 18., 19., 20., 21., 22., 23., 24.],
        [25., 26., 27., 28., 29., 30., 31., 32.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])


Global rank: 4, tok_embeddings DTensor(local_tensor=tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12., 13., 14., 15., 16.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])


Global rank: 5, tok_embeddings DTensor(local_tensor=tensor([[33., 34., 35., 36., 37., 38., 39., 40.],
        [41., 42., 43., 44., 45., 46., 47., 48.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])


Global rank: 7, tok_embeddings DTensor(local_tensor=tensor([[49., 50., 51., 52., 53., 54., 55., 56.],
        [57., 58., 59., 60., 61., 62., 63., 64.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])


MESHDIM DeviceMesh('cpu', [[0, 2], [4, 6]], mesh_dim_names=('DDP', 'FSDP')) TP MESH DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',))

Global rank: 6, tok_embeddings DTensor(local_tensor=tensor([[17., 18., 19., 20., 21., 22., 23., 24.],
        [25., 26., 27., 28., 29., 30., 31., 32.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])


Global rank: 0, tok_embeddings DTensor(local_tensor=tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12., 13., 14., 15., 16.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])


Global rank: 1, tok_embeddings DTensor(local_tensor=tensor([[33., 34., 35., 36., 37., 38., 39., 40.],
        [41., 42., 43., 44., 45., 46., 47., 48.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])

</code></pre></div></div>

<p>That’s it.</p>

<p>If we want to shard computation increase TP size, if we want to shard weights increase FSDP size, and if we want to process more batch increase DDP size.</p>

<h3 id="data-parallelism-with-fsdp-and-tp">Data-parallelism with FSDP and TP</h3>

<p>There might be some confusion about how data parallelism works with these sharding techniques. The confusion might be that we only pass different batches to DDP axis, for example: if we have 3D mesh ddp,fsdp,tp = (2,2,2), we might think that we pass one minibatch to ranks from first ddp axis and another to ranks from second ddp axis. BUT no, we can pass different batch across FSDP groups but with a strict condition that they are same across tp groups. For the 3D mesh above we have tp groups <code class="language-plaintext highlighter-rouge">[0,1] [2,3], [4,5], [6,7]</code>
and FSDP group <code class="language-plaintext highlighter-rouge">[0,2],[1,3],[4,6], [5,7]</code> so we can distribute one batch to rank 0 and another to rank 2 and 0 will broadcast the same data to it’s rank in the same tp group i.e rank 1, and similarly 2 will broadcast the same data to it’s rank in the same tp group i.e rank 3.</p>

<p>so each TP group will be have one distinct batch they can process. One important thing we might forget here is that FSDP will average the gradients across it’s group. Ex (gradient-for-one-batch-at-rank-0 + gradient-for-one-batch-at-rank-2)/2 (which is the number of ranks in the same FSDP group)</p>

<p>here’s is the worked out example to understand it better</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span><span class="o">*</span><span class="mi">2</span><span class="p">]</span>

<span class="nd">@spawn_threads_and_init_comms</span>
<span class="k">def</span> <span class="nf">shard_big_tensor</span><span class="p">(</span><span class="n">world_size</span><span class="p">):</span>

    <span class="n">device</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
    <span class="n">model_device_mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">])</span>

    <span class="n">device_mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">dp</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">tp</span><span class="sh">'</span><span class="p">])</span>

    <span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
            <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
            <span class="n">self</span><span class="p">.</span><span class="n">layer0</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer0</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="nc">Model</span><span class="p">()</span>

    <span class="n">model</span><span class="p">.</span><span class="n">layer0</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">17.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

    <span class="nf">parallelize_module</span><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">model_device_mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">parallelize_plan</span><span class="o">=</span><span class="p">{</span>
            <span class="sh">"</span><span class="s">layer0</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">()</span> <span class="c1"># since rowwise parallel will replicate the output layouts, each tp rank will get the same outptu.
</span>        <span class="p">}</span>
    <span class="p">)</span>

    <span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

    <span class="n">optim</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">model_device_mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">].</span><span class="nf">get_local_rank</span><span class="p">()</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span> <span class="c1"># give one data
</span>        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">rank </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> x = </span><span class="si">{</span><span class="n">data_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">out0</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data_list</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1">#         print(out0)
</span>        <span class="n">dist</span><span class="p">.</span><span class="nf">barrier</span><span class="p">()</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
        <span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">out0</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">out0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">rank </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="n">dist</span><span class="p">.</span><span class="nf">barrier</span><span class="p">()</span>
<span class="c1">#     print(f' RANK {dist.get_rank()} before second weights = {(model.layer0.weight.grad)} \n\n')
</span>
    <span class="k">if</span> <span class="n">model_device_mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">].</span><span class="nf">get_local_rank</span><span class="p">()</span> <span class="o">==</span><span class="mi">1</span><span class="p">:</span> <span class="c1"># give one data
</span>        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">rank </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> x = </span><span class="si">{</span><span class="n">data_list</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

        <span class="n">out1</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data_list</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="c1">#         print(out1)
</span>        <span class="n">dist</span><span class="p">.</span><span class="nf">barrier</span><span class="p">()</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
        <span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">out1</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">out1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">rank </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="n">dist</span><span class="p">.</span><span class="nf">barrier</span><span class="p">()</span>

<span class="c1">#     print(f' RANK {dist.get_rank()} after second weights = {model.layer0.weight.grad} \n\n')
</span>

    <span class="n">dt</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">layer0</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="nf">to_local</span><span class="p">()</span>

    <span class="n">dist</span><span class="p">.</span><span class="nf">all_reduce</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="p">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">model_device_mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">].</span><span class="nf">get_group</span><span class="p">())</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s"> RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> after second weights = </span><span class="si">{</span><span class="n">dt</span><span class="o">/</span><span class="mi">2</span><span class="si">}</span><span class="s"> </span><span class="se">\n\n</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">return</span>
</code></pre></div></div>

<p>In the code above, I don’t split the model across FSDP dimension and try to replicate what FSDP does under the hood. In this code we have two different data consider, torch.ones to be one batch and torch.ones x 2 to be another. Initially, I apply TP across TP groups, so two tp groups here <code class="language-plaintext highlighter-rouge">[0,1] and [2,3]</code> will have the same weights, then I distribute batch 1 to rank 0 and batch 2 to rank 2 (why? I’m trying to replicate the behaviour of fsdp here). Similarly, rank 1 will have batch 1 and rank 3 will have batch 2 (why? cause ranks across same tp group must have same data).</p>

<p>after calculating the gradient independently, I sum the gradients using all_reduce and print the averaged gradient.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RANK 1 reduced weights = tensor([[102.3585, 102.3585],
        [260.1725, 260.1725],
        [412.6720, 412.6720],
        [581.2230, 581.2230]])


 RANK 2 reduced weights = tensor([[102.3585, 102.3585],
        [260.1725, 260.1725],
        [412.6720, 412.6720],
        [581.2230, 581.2230]])


 RANK 3 reduced weights = tensor([[102.3585, 102.3585],
        [260.1725, 260.1725],
        [412.6720, 412.6720],
        [581.2230, 581.2230]])


 RANK 0 reduced weights = tensor([[102.3585, 102.3585],
        [260.1725, 260.1725],
        [412.6720, 412.6720],
        [581.2230, 581.2230]])
</code></pre></div></div>

<p>You might be wondering why different tp ranks have the same weights, it because RowwiseParallel calls Replicate() and and it has to do partial sum for output across tp ranks.</p>

<p>Now let’s see if FSDP does the same or not.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span><span class="o">*</span><span class="mi">2</span><span class="p">]</span>

<span class="nd">@spawn_threads_and_init_comms</span>
<span class="k">def</span> <span class="nf">shard_big_tensor</span><span class="p">(</span><span class="n">world_size</span><span class="p">):</span>

    <span class="n">device</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
    <span class="n">model_device_mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">])</span>

    <span class="n">device_mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">dp</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">tp</span><span class="sh">'</span><span class="p">])</span>

    <span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
            <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
            <span class="n">self</span><span class="p">.</span><span class="n">layer0</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer0</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="nc">Model</span><span class="p">()</span>

    <span class="n">model</span><span class="p">.</span><span class="n">layer0</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">17.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

    <span class="nf">parallelize_module</span><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">model_device_mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">parallelize_plan</span><span class="o">=</span><span class="p">{</span>
            <span class="sh">"</span><span class="s">layer0</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">()</span> <span class="c1"># since rowwise parallel will replicate the output layouts, each tp rank will get the same outptu.
</span>        <span class="p">}</span>
    <span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="nf">split_data_list</span><span class="p">(</span><span class="n">data_list</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">dp</span><span class="sh">'</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s"> RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> x= </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s"> </span><span class="se">\n\n</span><span class="sh">'</span><span class="p">)</span>


    <span class="nf">fully_shard</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="n">model_device_mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">])</span>

    <span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

    <span class="n">optim</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">dist</span><span class="p">.</span><span class="nf">barrier</span><span class="p">()</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">out</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">rank </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> loss </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s"> </span><span class="sh">'</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>


    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s"> RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> weights = </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">layer0</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">grad</span><span class="si">}</span><span class="s"> </span><span class="se">\n\n</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">return</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> RANK 2 weights = DTensor(local_tensor=tensor([[412.6720, 412.6720],
        [581.2230, 581.2230]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(Shard(dim=0), Shard(dim=1)))


 RANK 0 weights = DTensor(local_tensor=tensor([[102.3585, 102.3585],
        [260.1725, 260.1725]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(Shard(dim=0), Shard(dim=1)))


 RANK 1 weights = DTensor(local_tensor=tensor([[102.3585, 102.3585],
        [260.1725, 260.1725]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(Shard(dim=0), Shard(dim=1)))


 RANK 3 weights = DTensor(local_tensor=tensor([[412.6720, 412.6720],
        [581.2230, 581.2230]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(Shard(dim=0), Shard(dim=1)))
</code></pre></div></div>

<p>Since we also apply FSDP here, the weights are row-wise sharded across in this output, but if you unshard the FSDP (i.e combine weights across FSDP group), you will see they have the same weights, i.e stack the rank 0,2 across dimension 0.</p>

<p>let’s also see if we get the same process two data on only one machine (no distributed training)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span><span class="o">*</span><span class="mi">2</span><span class="p">]</span>
<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer0</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer0</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">Model</span><span class="p">()</span>


<span class="nf">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="n">layer0</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">17.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>



<span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

<span class="n">optim</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">out0</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data_list</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">out0</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">out0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>

<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

<span class="n">out1</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data_list</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">out1</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">out1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>

<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>


<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">weights = </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">layer0</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">grad</span><span class="o">/</span><span class="mi">2</span><span class="si">}</span><span class="s"> </span><span class="se">\n\n</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>weights = tensor([[102.3585, 102.3585, 102.3585, 102.3585],
        [260.1725, 260.1725, 260.1725, 260.1725],
        [412.6720, 412.6720, 412.6720, 412.6720],
        [581.2230, 581.2230, 581.2230, 581.2230]])
</code></pre></div></div>

<p>I’ll be writing more on how all these techniques are used in distributed RL training in the next part.</p>

<p>While using parallelism, we need to be very careful to not break the computation graph, because collectives are not differentiable.</p>

<p>For example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@spawn_threads_and_init_comms</span>
<span class="k">def</span> <span class="nf">shard_big_tensor</span><span class="p">(</span><span class="n">world_size</span><span class="p">):</span>
    <span class="n">logsumexp</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="n">device_mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span>
        <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">DDP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">FSDP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="n">logsumexp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span> <span class="o">*</span> <span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span>
    <span class="n">logsumexp</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>

    <span class="n">logsumexps</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">logsumexp</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">device_mesh</span><span class="p">[</span><span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">].</span><span class="nf">size</span><span class="p">())]</span>
    <span class="n">dist</span><span class="p">.</span><span class="nf">all_gather</span><span class="p">(</span><span class="n">logsumexps</span><span class="p">,</span> <span class="n">logsumexp</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">[</span><span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">].</span><span class="nf">get_group</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">device_mesh</span><span class="p">[</span><span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">].</span><span class="nf">get_local_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">---detached ----</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">logsumexps</span><span class="p">)</span>

    <span class="n">dist</span><span class="p">.</span><span class="nf">barrier</span><span class="p">()</span>
    <span class="n">logsumexps</span><span class="p">[</span><span class="n">device_mesh</span><span class="p">[</span><span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">].</span><span class="nf">get_local_rank</span><span class="p">()]</span> <span class="o">=</span> <span class="n">logsumexp</span>

    <span class="k">if</span> <span class="n">device_mesh</span><span class="p">[</span><span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">].</span><span class="nf">get_local_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">----not detached----</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">logsumexps</span><span class="p">)</span>

    <span class="c1"># new_logsumexp = torch.stack(logsumexps, dim=-1).logsumexp(dim=-1)
</span>
    <span class="c1"># print(f'rank {dist.get_rank()} logsumexps {new_logsumexp}')
</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## output
---detached ---- [tensor([[0., 0., 0., 0.], [0., 0., 0., 0.]]), tensor([[1., 1., 1., 1.], [1., 1., 1., 1.]])]
---detached ---- [tensor([[2., 2., 2., 2.], [2., 2., 2., 2.]]), tensor([[3., 3., 3., 3.], [3., 3., 3., 3.]])]

----not detached---- [tensor([[0., 0., 0., 0.], [0., 0., 0., 0.]], requires_grad=True), tensor([[1., 1., 1., 1.], [1., 1., 1., 1.]])]

----not detached---- [tensor([[2., 2., 2., 2.], [2., 2., 2., 2.]], requires_grad=True), tensor([[3., 3., 3., 3.], [3., 3., 3., 3.]])]
</code></pre></div></div>

<p>The main culprit is the dist.all_gather which fills in the detached values of logsumexp into the logsumexps (as you can see no requires_grad in the elements in the logsumexps list), so when we do loss.backward() the gradient stops moving from this detached points and won’t travel to the downstream parameters.</p>

<p>But, using this line <code class="language-plaintext highlighter-rouge">logsumexps[device_mesh["TP"].get_local_rank()] = logsumexp</code> we fill in the local rank’s logsumexp in the logsumexps list, so now gradient’s will propagate through it.</p>

<p>Similar to that we need to use something similar while using all_reduce operation. During the forward pass the values will be the reduced one, and during backward, gradients can flow through the tensor variable.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">differentiable_all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">):</span>
    <span class="n">detached_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">detach</span><span class="p">()</span>
    <span class="n">dist</span><span class="p">.</span><span class="nf">all_reduce</span><span class="p">(</span>
        <span class="n">detached_tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="p">.</span><span class="n">ReduceOp</span><span class="p">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">device_mesh</span><span class="p">.</span><span class="nf">get_group</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">tens</span>                                                                                                                                                                                                                              <span class="ow">or</span> <span class="o">+</span> <span class="n">detached_tensor</span> <span class="o">-</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">detach</span><span class="p">()</span>

</code></pre></div></div>
</div>

  
  <div class="tags">
    
    <span class="tag">blog</span>
    
    <span class="tag">RL</span>
    
    <span class="tag">distributed</span>
    
  </div>
  
</article> -->
</main>
  </body>
</html>
