<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Why We Need Regularization - </title>
    <link rel="stylesheet" href="/assets/css/main.css" />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Zalando+Sans+Expanded:ital,wght@0,200..900;1,200..900&family=Zalando+Sans:ital,wght@0,200..900;1,200..900&display=swap"
      rel="stylesheet"
    />

    <!-- MathJax for LaTeX -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
          displayMath: [
            ["$$", "$$"],
            ["\\[", "\\]"],
          ],
          processEscapes: true,
          processEnvironments: true,
        },
        options: {
          skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"],
        },
      };
    </script>
    <script
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
      id="MathJax-script"
      async
    ></script>
  </head>
  <body>
    <nav>
      <a href="/">Home</a>
      <a href="/posts">Posts</a>
      <a href="/notes">Notes</a>
      <a href="/blogs">Blogs</a>
    </nav>

    <main><h1>Why We Need Regularization</h1>
<p>08 Dec 2024 - cohlem</p>

<ul>
  <li>it penalizes the weights, and prioritizes uniformity in weights.</li>
</ul>

<h4 id="how-does-it-penalize-the-weights">How does it penalize the weights?</h4>

<p><img src="/assets/images/2024-12-08-why-we-need-regularization/one.png" alt="one" /></p>

<p>Now when we do the backprop and gradient descent.</p>

<p>The gradient of loss w.r.t some weights become
<img src="/assets/images/2024-12-08-why-we-need-regularization/two.png" alt="one" /></p>

<p><img src="/assets/images/2024-12-08-why-we-need-regularization/three.png" alt="one" />
as we can see it penalizes the weight by reducing the weights’s value by some higher amount compared to the some minimial weight update when we only used loss function.</p>

<p>So overall, the model tries to balance the Loss (L) as well as keep the weights small.
This balance prevents the model from relying excessively on any particular weight.</p>


<!-- <article class="blog-post">
  <header>
    <h1>Why We Need Regularization</h1>
    <time datetime="2024-12-08T00:00:00+05:45">
      December 08, 2024
    </time>
  </header>

  <div class="post-content"><ul>
  <li>it penalizes the weights, and prioritizes uniformity in weights.</li>
</ul>

<h4 id="how-does-it-penalize-the-weights">How does it penalize the weights?</h4>

<p><img src="/assets/images/2024-12-08-why-we-need-regularization/one.png" alt="one" /></p>

<p>Now when we do the backprop and gradient descent.</p>

<p>The gradient of loss w.r.t some weights become
<img src="/assets/images/2024-12-08-why-we-need-regularization/two.png" alt="one" /></p>

<p><img src="/assets/images/2024-12-08-why-we-need-regularization/three.png" alt="one" />
as we can see it penalizes the weight by reducing the weights’s value by some higher amount compared to the some minimial weight update when we only used loss function.</p>

<p>So overall, the model tries to balance the Loss (L) as well as keep the weights small.
This balance prevents the model from relying excessively on any particular weight.</p>
</div>

  
  <div class="tags">
    
    <span class="tag">loss-function</span>
    
  </div>
  
</article> -->
</main>
  </body>
</html>
