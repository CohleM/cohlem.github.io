<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Backpropagation From Scratch - </title>
    <link rel="stylesheet" href="/assets/css/main.css" />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Zalando+Sans+Expanded:ital,wght@0,200..900;1,200..900&family=Zalando+Sans:ital,wght@0,200..900;1,200..900&display=swap"
      rel="stylesheet"
    />

    <!-- MathJax for LaTeX -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
          displayMath: [
            ["$$", "$$"],
            ["\\[", "\\]"],
          ],
          processEscapes: true,
          processEnvironments: true,
        },
        options: {
          skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"],
        },
      };
    </script>
    <script
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
      id="MathJax-script"
      async
    ></script>
  </head>
  <body>
    <nav>
      <a href="/">Home</a>
      <a href="/posts">Posts</a>
      <a href="/notes">Notes</a>
      <a href="/blogs">Blogs</a>
    </nav>

    <main><h1>Backpropagation From Scratch</h1>
<p>08 Dec 2024 - cohlem</p>

<p>Source: <a href="https://www.youtube.com/watch?v=VMj-3S1tku0&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">The spelled-out intro to neural networks and backpropagation: building
micrograd</a></p>

<h2 id="backpropagation-on-paper">Backpropagation on paper</h2>

<p>It implements backpropagation for two arithmetic operation (multiplication and addition) which are quite straightforward.</p>

<p>Implementation is for this equation.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a = Value(2.0, label='a')
b = Value(-3.0, label='b')
c = Value(10.0, label='c')
e = a*b; e.label = 'e'
d = e + c; d.label = 'd'
f = Value(-2.0, label='f')
L = d * f; L.label = 'L'
L
</code></pre></div></div>

<p><img src="backprop.jpg" alt="backprop" />
The most important thing to note here is the gradient accumulation step (shown at the bottom-left). If a node takes part two times building up to the final node. The gradient for that node is accumulated. For instance, in the figure node b takes part two times. First, it is involved in equation e = a * b, and another is e = b + m (not in the equation above).</p>

<p>Code</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class Value:
    def __init__(self,data, _children = (), _op = '', label=None):
        self.data = data
        self.label = label
        self.grad = 0.0
        self._prev = set(_children)
        self._op = _op
        self._backward = lambda: None

    def __repr__(self):
        return f"Value({self.label}, {self.data})"

    def __add__(self, other):

        result = Value(self.data + other.data, (self, other), '+')

        def _backward():
            self.grad = 1.0 * result.grad
            other.grad = 1.0 * result.grad

        result._backward = _backward

        return result

    def __sub__(self,other):
        result = Value(self.data - other.data)
        result._prev = [self, other]
        result._op = '-'
        return result

    def __mul__(self,other):
        result = Value(self.data * other.data, (self, other), '*')
        def _backward():
            self.grad = other.data * result.grad
            other.grad = self.data * result.grad


        result._backward = _backward
        return result

    def backward(self):
        topo = []
        visited = set()
        self.grad = 1
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._prev:
                    build_topo(child)
                topo.append(v)
        build_topo(self)
        topo = list(reversed(topo))


        print('gg', topo)
        for i in topo:
            print(i)
            i._backward()



</code></pre></div></div>


<!-- <article class="blog-post">
  <header>
    <h1>Backpropagation From Scratch</h1>
    <time datetime="2024-12-08T00:00:00+05:45">
      December 08, 2024
    </time>
  </header>

  <div class="post-content"><p>Source: <a href="https://www.youtube.com/watch?v=VMj-3S1tku0&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">The spelled-out intro to neural networks and backpropagation: building
micrograd</a></p>

<h2 id="backpropagation-on-paper">Backpropagation on paper</h2>

<p>It implements backpropagation for two arithmetic operation (multiplication and addition) which are quite straightforward.</p>

<p>Implementation is for this equation.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a = Value(2.0, label='a')
b = Value(-3.0, label='b')
c = Value(10.0, label='c')
e = a*b; e.label = 'e'
d = e + c; d.label = 'd'
f = Value(-2.0, label='f')
L = d * f; L.label = 'L'
L
</code></pre></div></div>

<p><img src="backprop.jpg" alt="backprop" />
The most important thing to note here is the gradient accumulation step (shown at the bottom-left). If a node takes part two times building up to the final node. The gradient for that node is accumulated. For instance, in the figure node b takes part two times. First, it is involved in equation e = a * b, and another is e = b + m (not in the equation above).</p>

<p>Code</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class Value:
    def __init__(self,data, _children = (), _op = '', label=None):
        self.data = data
        self.label = label
        self.grad = 0.0
        self._prev = set(_children)
        self._op = _op
        self._backward = lambda: None

    def __repr__(self):
        return f"Value({self.label}, {self.data})"

    def __add__(self, other):

        result = Value(self.data + other.data, (self, other), '+')

        def _backward():
            self.grad = 1.0 * result.grad
            other.grad = 1.0 * result.grad

        result._backward = _backward

        return result

    def __sub__(self,other):
        result = Value(self.data - other.data)
        result._prev = [self, other]
        result._op = '-'
        return result

    def __mul__(self,other):
        result = Value(self.data * other.data, (self, other), '*')
        def _backward():
            self.grad = other.data * result.grad
            other.grad = self.data * result.grad


        result._backward = _backward
        return result

    def backward(self):
        topo = []
        visited = set()
        self.grad = 1
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._prev:
                    build_topo(child)
                topo.append(v)
        build_topo(self)
        topo = list(reversed(topo))


        print('gg', topo)
        for i in topo:
            print(i)
            i._backward()



</code></pre></div></div>
</div>

  
  <div class="tags">
    
    <span class="tag">backpropagation</span>
    
  </div>
  
</article> -->
</main>

    <footer>
      <p>&copy; 2025 </p>
    </footer>
  </body>
</html>
