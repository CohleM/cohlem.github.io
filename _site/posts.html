<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title> - </title>
    <link rel="stylesheet" href="/assets/css/main.css" />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Zalando+Sans+Expanded:ital,wght@0,200..900;1,200..900&family=Zalando+Sans:ital,wght@0,200..900;1,200..900&display=swap"
      rel="stylesheet"
    />

    <!-- MathJax for LaTeX -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
          displayMath: [
            ["$$", "$$"],
            ["\\[", "\\]"],
          ],
          processEscapes: true,
          processEnvironments: true,
        },
        options: {
          skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"],
        },
      };
    </script>
    <script
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
      id="MathJax-script"
      async
    ></script>
  </head>
  <body>
    <nav>
      <a href="/">Home</a>
      <a href="/posts">Posts</a>
      <a href="/notes">Notes</a>
      <a href="/blogs">Blogs</a>
    </nav>

    <main><div class="home-container">
    <!-- <section class="blogs">
  <h2>Blogs</h2>
  <ul>
    <li>Skill 1 </li>
    <li>Skill 2</li>
    <li>Skill 3</li>
  </ul>
</section> -->

<h1>Latest Posts</h1>
<p>All unstructured blogs/posts/notes</p>
<section class="blogs">
  <ul>
    
    <a href="/2025/09/18/RL-papers.html">
      <li>
        <h2>Rl Papers</h2>
        Skywork Open Reasoner 1 Technical Report
      </li></a
    >
    
    <a href="/2025/09/01/Distributed-RL-training-step.html">
      <li>
        <h2>Distributed Rl Training Step</h2>
        I was learning how we can do distributed RL training, saw karpathy posting this and thought why not make a complete blog about what I learned so here it is.
      </li></a
    >
    
    <a href="/2025/06/09/python.html">
      <li>
        <h2>Python</h2>
        Unpacking over indexing
      </li></a
    >
    
    <a href="/2025/04/28/Multi-head-latent-attention.html">
      <li>
        <h2>Multi Head Latent Attention</h2>
        Scaled-dot product Attention
      </li></a
    >
    
    <a href="/2025/04/07/LoRA.html">
      <li>
        <h2>Lora</h2>
        LoRA
      </li></a
    >
    
    <a href="/2025/03/03/interpretability.html">
      <li>
        <h2>Interpretability</h2>
        Induction circuits
      </li></a
    >
    
    <a href="/2025/02/24/RLHF.html">
      <li>
        <h2>Rlhf</h2>
        Before starting, it’s advisable to first complete David Silver’s Course on RL and read Lilian’s notes on RL which explains/provides notes on the David’s cour...
      </li></a
    >
    
    <a href="/2025/02/11/flops-calculation.html">
      <li>
        <h2>Flops Calculation</h2>
        Calculation of FLOPs
      </li></a
    >
    
    <a href="/2025/02/06/post-training-strategies.html">
      <li>
        <h2>Post Training Strategies</h2>
        After training, we generally perform alignment i.e teaching the model how to behave/act in desired manner. Post training mainly consists 1) Supervised Fine-t...
      </li></a
    >
    
    <a href="/2025/01/29/pytorch.html">
      <li>
        <h2>Pytorch</h2>
        torch.stack(tensors, dim)
      </li></a
    >
    
    <a href="/2025/01/29/building-lillm.html">
      <li>
        <h2>Building Lillm</h2>
        Pre-training
      </li></a
    >
    
    <a href="/2025/01/22/tokenization.html">
      <li>
        <h2>Tokenization</h2>
        Unicode
      </li></a
    >
    
    <a href="/2025/01/21/paper-summaries.html">
      <li>
        <h2>Paper Summaries</h2>
        Papers that I’ve read with their respective notes.
      </li></a
    >
    
    <a href="/2025/01/18/KV-cache-GQA.html">
      <li>
        <h2>Kv Cache Gqa</h2>
        KV Cache
      </li></a
    >
    
    <a href="/2025/01/15/RoPE.html">
      <li>
        <h2>Rope</h2>
        Recap of Absolute PE
      </li></a
    >
    
    <a href="/2025/01/15/RMSNorm.html">
      <li>
        <h2>Rmsnorm</h2>
        Recap of LayerNorm
      </li></a
    >
    
    <a href="/2025/01/08/GPUs.html">
      <li>
        <h2>Gpus</h2>
        GPU physcial structure
      </li></a
    >
    
    <a href="/2025/01/05/mixture-of-experts.html">
      <li>
        <h2>Mixture Of Experts</h2>
        Image Source:https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts
      </li></a
    >
    
    <a href="/2025/01/03/gradient-accumulation.html">
      <li>
        <h2>Gradient Accumulation</h2>
        Gradient Accumulation
      </li></a
    >
    
    <a href="/2025/01/03/DDP.html">
      <li>
        <h2>Ddp</h2>
        When we have enough resources we would want to train our neural networks in parallel, the way to do this is to train our NN with different data (different ba...
      </li></a
    >
    
    <a href="/2025/01/02/training-speed-optimization.html">
      <li>
        <h2>Training Speed Optimization</h2>
        Precision
      </li></a
    >
    
    <a href="/2024/12/30/skip-connections.html">
      <li>
        <h2>Skip Connections</h2>
        Skip connections are simply skipping the layers by adding the identity of input to it’s output as shown in the figure below.
      </li></a
    >
    
    <a href="/2024/12/27/optimization-algorithms.html">
      <li>
        <h2>Optimization Algorithms</h2>
        The simplest algorithm is the gradient descent in which we simply calculate loss over all the training data and then update our parameters, but it would be t...
      </li></a
    >
    
    <a href="/2024/12/24/manual-backpropagation-on-tensors.html">
      <li>
        <h2>Manual Backpropagation On Tensors</h2>
        Main code
      </li></a
    >
    
    <a href="/2024/12/24/Matrix-Visualization.html">
      <li>
        <h2>Matrix Visualization</h2>
        In deep learning, it’s important to visualize a matrix and how it is represented in a dimension space because the operations that we perform on those matrix ...
      </li></a
    >
    
    <a href="/2024/12/24/GPT-implementation.html">
      <li>
        <h2>Gpt Implementation</h2>
        # We always start with a dataset to train on. Let's download the tiny shakespeare dataset!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/dat...
      </li></a
    >
    
    <a href="/2024/12/20/Diagnostic-tool-while-training-nn.html">
      <li>
        <h2>Diagnostic Tool While Training Nn</h2>
        source:  Building makemore Part 3: Activations &amp; Gradients, BatchNorm
      </li></a
    >
    
    <a href="/2024/12/19/optimizing-loss.html">
      <li>
        <h2>Optimizing Loss</h2>
        Problem
      </li></a
    >
    
    <a href="/2024/12/19/BatchNormalization.html">
      <li>
        <h2>Batchnormalization</h2>
        As we saw in our previous note how important it is to have the pre-activation values to be roughly gaussian (0 mean, and unit std). We saw how we can initial...
      </li></a
    >
    
    <a href="/2024/12/16/maximum-likelihood-estimate-as-loss.html">
      <li>
        <h2>Maximum Likelihood Estimate As Loss</h2>
        
      </li></a
    >
    
    <a href="/2024/12/08/why-we-need-regularization.html">
      <li>
        <h2>Why We Need Regularization</h2>
          it penalizes the weights, and prioritizes uniformity in weights.
      </li></a
    >
    
    <a href="/2024/12/08/backpropagation-from-scratch.html">
      <li>
        <h2>Backpropagation From Scratch</h2>
        Source: The spelled-out intro to neural networks and backpropagation: buildingmicrograd
      </li></a
    >
    
    <a href="/2023/08/12/Using-Genetic-Algorithm-for-Weights-Optimization.html">
      <li>
        <h2>Using Genetic Algorithm For Weights Optimization</h2>
        BackStory
      </li></a
    >
    
  </ul>
</section>

</div>
</main>
  </body>
</html>
