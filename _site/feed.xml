<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-10-19T22:54:09+05:45</updated><id>http://localhost:4000/feed.xml</id><entry><title type="html">Rl Papers</title><link href="http://localhost:4000/2025/09/18/RL-papers.html" rel="alternate" type="text/html" title="Rl Papers" /><published>2025-09-18T00:00:00+05:45</published><updated>2025-09-18T00:00:00+05:45</updated><id>http://localhost:4000/2025/09/18/RL-papers</id><content type="html" xml:base="http://localhost:4000/2025/09/18/RL-papers.html"><![CDATA[<h3 id="skywork-open-reasoner-1-technical-report">Skywork Open Reasoner 1 Technical Report</h3>

<h4 id="multi-stage-training">Multi-stage training</h4>

<p>First, set context_length = T, train, wait for convergence, at some point afterwards set context_length=2T</p>

<p>There seems to be no problem with truncated responses, cause actor learns to produce accurate but shorter responses.
<img src="skywork-tr.png" alt="skywork-tr" /></p>

<h4 id="high-temperature-sampling">High temperature Sampling</h4>

<p>temperatue 1.0 provides more room for exploration, whereas lower temperature might cause early entropy collapse.
<img src="temp-sampling.png" alt="temp-sampling.png" /></p>

<h4 id="adaptive-entropy-control">Adaptive Entropy Control</h4>

<p>turn off entropy coefficient when entropy is already greater than target entropy else, increase it by del.
<img src="entropy_control.png" alt="entropy_control" /></p>

<h3 id="acereason-nemotron-11-advancing-math-and-code-reasoning-through-sft-and-rl-synergy">AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy</h3>

<h4 id="which-one-results-in-better-performance--large-no-of-unique-prompts-or-more-responses-per-prompt">Which one results in better performance ? large no. of unique prompts or more responses per prompt.</h4>

<p>Generally, large no. of unique prompts is better, but at some point obtaining those become difficult, afterwards, we can include more responses per prompt in our dataset. More epochs helps as well.</p>

<p><img src="ace-nemo.png" alt="ace-nemo" /></p>

<h4 id="is-stage-1-necessary">Is stage-1 necessary?</h4>

<p><img src="overall-trn.png" alt="overall-trn" />
stage-1 training seems to be necessary cause, it allows the model to compress it’s reasoning at early stage, not doing so results in accuracy that plateaus see fig below.</p>

<p><img src="stage-1.png" alt="stage-1" /></p>

<h4 id="exploration-vs-exploitation-ie-temperature-controlled-entropy">Exploration vs exploitation (i.e temperature controlled entropy)</h4>

<p>extremely high temp or low temp cause reward to dampen, thus noisy training.</p>

<p><img src="temp-exp.png" alt="temp-exp" /></p>

<h4 id="with-overlong-filtering-or-without-overlong-filtering">with overlong filtering or without overlong filtering</h4>

<p>with overlong filtering = don’t use the trajectory for reward calculation
without overlong filtering = use the trajectory and provide negative reward for truncated responses</p>

<p><img src="overlong.png" alt="overlong" /></p>

<p>overlong filtering helps during first stages (i.e when context lengths are small) otherwise, when context length is sufficient, it doesn’t help.</p>

<h4 id="performance-with-increase-in-k-value-passk">Performance with increase in K value (pass@K)</h4>

<p>The performance gain decreases for some benchmarks(AIME25) as we increase K. We can find answer within the SFT model if the K is sufficiently large</p>

<p><img src="passk.png" alt="passk" /></p>

<h3 id="something">Something</h3>

<h4 id="which-ones-better-group-normalization-grpo-batch-normalization-use-same-equation-as-grpo-but-taken-mean-and-std-between-whole-rollout-batch">which one’s better? group normalization (GRPO), batch normalization (use same equation as GRPO but taken mean and std between whole rollout batch)</h4>

<ul>
  <li>group normalization seems to be better.</li>
</ul>

<h4 id="do-we-need-std-normalization">Do we need std normalization?</h4>

<p>when problems are too easy for actor, the rewards are almost all correct, the std will be low, division by low std results in high advanrages causing higher gradients (sometimes exploding gradients) but when problems are medium-hard there’s not much of a difference between normalizing by std or just simply subtracting the mean from the reward. (i.e no use of std)</p>

<p><img src="std-use.png" alt="std-use" /></p>

<h4 id="reward-shaping-works-best-when-use-divide-by-global-std">Reward shaping works best when use divide by global std</h4>

<p><img src="global-std.png" alt="global-std" /></p>

<h4 id="liteppo">LitePPO</h4>

<p>simply using token-level loss i.e sum all tokens loss then divide by total tokens in the batch and using division by global std improves the overall performance.</p>

<h1 id="for-sop">FOR SOP</h1>

<h3 id="writing-zero">Writing zero</h3>

<p>train pairwise GenRM,</p>

<ol>
  <li>Gather pairwise data</li>
  <li>pass claude code with those pairwise data, generate critiques and final answer.</li>
  <li>remove the trajectories which has contradictory answer or doesn’t match the ground truth (we already know the real preferences)</li>
  <li>do cold-start SFT with those trajectories.</li>
  <li>do GRPO training with Reward = 1 if Sc &gt; Sr</li>
</ol>

<p>afterwards,
do BRPO (modified GRPO) i.e a model will try to generate best answers.
exp do rollout of 8 exp, select one randomly as a reference data, pass current answer and the reference answer to GenRM, GenRM provides scores S1,S2. If R = 1 if S1 &gt; S2 else -1. thats it.</p>

<h3 id="next-story-generation">Next story generation</h3>

<p>simply train the reasoning model not the generation model. i.e learn to generate intermediate p_hat, do this using GRPO, reward given by this formula ![[Screenshot 2025-10-13 at 11.48.44 AM.png]]</p>

<p>I is given by this formula
![[Screenshot 2025-10-13 at 11.49.14 AM.png]]</p>

<p>reminder, we need the ground truth answer for training, and PPL simply mean that we find the avg of negative log likelihood of all the tokens and exponentiate that avg and put - sign in front.</p>

<p><a href="https://arxiv.org/pdf/2503.19618">Beyond Verifiable Rewards: Scaling Reinforcement Learning for Language Models to Unverifiable Data</a></p>

<p>https://chatgpt.com/share/68ecbaf2-35e4-800c-8dcf-db3b183741ed</p>

<p>Given a prompt x, the intuitive role of chain-of-thought is such that it makes the marginal likelihood of the ground truth answer a∗ higher. As such, we can interpret chain-of- thought as a latent variable and formulate the optimization of chain-of-thought as latent variable modeling
![[Screenshot 2025-10-13 at 2.23.40 PM.png]]</p>

<p>the first term promotes CoT chains that produce best answer, whereas the second term promotes answer given the CoT chain, third term is simply the KL term.</p>

<p>So it’s kind of like providing the implicit rewards using RL + SFT</p>

<p>in</p>]]></content><author><name>cohlem</name></author><summary type="html"><![CDATA[Skywork Open Reasoner 1 Technical Report]]></summary></entry><entry><title type="html">Distributed Rl Training Step</title><link href="http://localhost:4000/2025/09/01/Distributed-RL-training-step.html" rel="alternate" type="text/html" title="Distributed Rl Training Step" /><published>2025-09-01T00:00:00+05:45</published><updated>2025-09-01T00:00:00+05:45</updated><id>http://localhost:4000/2025/09/01/Distributed-RL-training-step</id><content type="html" xml:base="http://localhost:4000/2025/09/01/Distributed-RL-training-step.html"><![CDATA[<p>I was learning how we can do distributed RL training, saw karpathy posting <a href="https://x.com/karpathy/status/1952076108565991588">this</a> and thought why not make a complete blog about what I learned so here it is.</p>

<p>The end goal of this blog is to explain clearly how to do distributed RL training, right now it contains explanations about fundamentals of distributed training, such as data parallelism, model parallelism, and tensor parallelism. Consider this as a part 1, where in the next blog I’ll be explaining how we apply the techniques learned in the blog.</p>

<p>Doing RL is simply not easy from the resource/computation standpoint because there isn’t just one model/optimizer states and gradients that we need to care of but the same model can be an actor, critic, reference policy and so on and we need to store them in our GPU. Doing RL even with 0.5B model with decent sequence length takes a LOT of GPU memory, which led me to understand how GPUs can work in a distributed manner (the part which I had always been ignoring).</p>

<h2 id="manual-distributed-data-parallel">Manual Distributed Data Parallel</h2>

<p>The simplest form of distributed training is distributed data parallel where <strong>each GPU</strong> has a copy of a model, but each GPU has different batches of data. Lets say we have batch_size = 64 and 2 GPUs, and a model. The model’s parameters will be replicated across both GPU. GPU1 will get (1, batch_size/2) and GPU2 will get (batch_size/2 , batch_size) data, each GPU will process on its own, find it’s own loss and find its own gradients and then sum those gradients over both the GPUs and average it (if needed), the averaged gradients will be replicated across both the GPUs and each will perform optimizer.step() on its own and end up with same set of parameters.</p>

<p>Let’s do manual distributed data parallel to see how it works under the hood with the help of a toy model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="c1">## ----------------- SETUP ----------------------------
</span><span class="n">rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">RANK</span><span class="sh">"</span><span class="p">])</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">WORLD_SIZE</span><span class="sh">"</span><span class="p">])</span> <span class="c1"># total number of GPUs
</span><span class="n">local_rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">LOCAL_RANK</span><span class="sh">"</span><span class="p">])</span> <span class="c1"># unique id for this GPU
</span><span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="n">backend</span> <span class="o">=</span> <span class="sh">'</span><span class="s">nccl</span><span class="sh">'</span> <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">else</span> <span class="sh">'</span><span class="s">gloo</span><span class="sh">'</span>
<span class="n">dist</span><span class="p">.</span><span class="nf">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>  <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>
<span class="c1">#torch.cuda.set_device(local_rank) # turn this on if you have GPU
## ----------------- SETUP ----------------------------
</span>
<span class="n">inp</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">9.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">9.</span><span class="p">,</span><span class="mf">17.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)]</span>


<span class="k">class</span> <span class="nc">ToyModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">net1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">5.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="n">self</span><span class="p">.</span><span class="n">net1</span><span class="p">.</span><span class="n">T</span>

<span class="n">local_data</span> <span class="o">=</span> <span class="n">inp</span><span class="p">[</span><span class="n">local_rank</span><span class="p">]</span> <span class="c1"># this way different gpus will get different data
</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">ToyModel</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">predicted</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">local_data</span><span class="p">)</span>
    <span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">predicted</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">/=</span> <span class="n">predicted</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s"> RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span> <span class="si">}</span><span class="se">\n</span><span class="s"> parameters</span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">net1</span><span class="p">.</span><span class="n">grad</span><span class="si">}</span><span class="s"> </span><span class="sh">'</span><span class="p">)</span>
    <span class="n">dist</span><span class="p">.</span><span class="nf">all_reduce</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">net1</span><span class="p">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="p">.</span><span class="n">ReduceOp</span><span class="p">.</span><span class="n">SUM</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">net1</span><span class="p">.</span><span class="n">grad</span> <span class="o">/=</span> <span class="n">world_size</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s"> AFTER ALL REDUCE </span><span class="se">\n</span><span class="s"> RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span> <span class="si">}</span><span class="se">\n</span><span class="s"> parameters</span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">net1</span><span class="p">.</span><span class="n">grad</span><span class="si">}</span><span class="s"> </span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># do the optimizer.step
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
<span class="n">dist</span><span class="p">.</span><span class="nf">destroy_process_group</span><span class="p">()</span>
</code></pre></div></div>

<p>run the code with <code class="language-plaintext highlighter-rouge">torchrun --node_rank=0 --nproc_per_node=2 --nnodes=1 --standalone test2.py</code>
<code class="language-plaintext highlighter-rouge">torchrun</code> will start different processes equals to the number specified in –nproc_per_node i.e 2 and this line <code class="language-plaintext highlighter-rouge">dist.init_process_group(backend=backend,  world_size=world_size)</code> will create a process group where ranks in the groups communicate with each other by using collectives (explained below). Each process will also get one rank, we can access it’s rank using <code class="language-plaintext highlighter-rouge">local_rank = int(os.environ["LOCAL_RANK"])</code> and can assign gpu to each process using <code class="language-plaintext highlighter-rouge">torch.cuda.set_device(local_rank)</code> for now I’ll be commenting our because, we can carry out these simple codes in CPU.</p>

<p>Now, we define our model structure, it’s a simply matrix multiplication (similar to nn.Linear with bias=False) and assign different data to different ranks using <code class="language-plaintext highlighter-rouge">local_data = inp[local_rank]</code>, we process different data on different process calculate loss, do backward and do
` dist.all_reduce(model.net1.grad, op=dist.ReduceOp.SUM)` it will first collect all the model.net1.grad from all the GPUs and sum it (provided by dist.ReduceOp.SUM) and transfer the summed result to each GPUs (thus the name reduce). The operation all_reduce should be intuitive by now.</p>

<p>This all_reduce is a collective operation performed by the <code class="language-plaintext highlighter-rouge">backend</code> i.e either ‘nccl’ or ‘gloo’.
‘nccl’ is mostly used for GPUs whereas ‘gloo’ can run in CPUs as well. Under the hood these backend implement the logic on how these collectives operations need to be implemented. all_reduce is one of many collective operations provided by the backend. Please late a look at the picture below to understand how these work.
<img src="coll1.png" alt="coll1.png" />
<img src="coll2.png" alt="coll2" />
Image source: <a href="https://docs.pytorch.org/tutorials/intermediate/dist_tuto.html">Writing Distributed Applications with PyTorch</a></p>

<p>You can take a look at this <a href="https://docs.pytorch.org/tutorials/intermediate/dist_tuto.html">source</a> if you want to understand how these collectives work under the hood.</p>

<h3 id="pytorchs-distributeddataparallel">Pytorch’s DistributedDataParallel</h3>

<p>The same function can be carried out by wrapping our model within pytorch’s DistributedDataParallel and we don’t have to call all_reduce manually, pytorch automatically takes care of that when we call loss.backward().</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>
<span class="c1">## ----------------- SETUP ----------------------------
</span><span class="n">rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">RANK</span><span class="sh">"</span><span class="p">])</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">WORLD_SIZE</span><span class="sh">"</span><span class="p">])</span>
<span class="n">local_rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">LOCAL_RANK</span><span class="sh">"</span><span class="p">])</span>
<span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="n">backend</span> <span class="o">=</span> <span class="sh">'</span><span class="s">nccl</span><span class="sh">'</span> <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">else</span> <span class="sh">'</span><span class="s">gloo</span><span class="sh">'</span>
<span class="n">dist</span><span class="p">.</span><span class="nf">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>  <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>
<span class="c1">## ----------------- SETUP ----------------------------
</span>
<span class="n">inp</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">9.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">9.</span><span class="p">,</span><span class="mf">17.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)]</span>


<span class="k">class</span> <span class="nc">ToyModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">net1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">5.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="n">self</span><span class="p">.</span><span class="n">net1</span><span class="p">.</span><span class="n">T</span>

<span class="n">local_data</span> <span class="o">=</span> <span class="n">inp</span><span class="p">[</span><span class="n">local_rank</span><span class="p">]</span> <span class="c1"># will get data based on its rank
</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">ToyModel</span><span class="p">()</span>
<span class="n">ddp_model</span> <span class="o">=</span> <span class="nc">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">ddp_model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">predicted</span> <span class="o">=</span> <span class="nf">ddp_model</span><span class="p">(</span><span class="n">local_data</span><span class="p">)</span>
    <span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="n">local_data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">predicted</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">/=</span> <span class="n">predicted</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span>

    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="c1"># dist.all_reduce(model.net1.grad, op=dist.ReduceOp.SUM)
</span>    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s"> AFTER ALL REDUCE </span><span class="se">\n</span><span class="s"> RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span> <span class="si">}</span><span class="se">\n</span><span class="s"> parameters</span><span class="se">\n</span><span class="s"> </span><span class="si">{</span> <span class="n">ddp_model</span><span class="p">.</span><span class="n">module</span><span class="p">.</span><span class="n">net1</span><span class="p">.</span><span class="n">grad</span><span class="si">}</span><span class="s"> </span><span class="sh">'</span><span class="p">)</span>
    <span class="c1"># do the optimizer.step
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
<span class="n">dist</span><span class="p">.</span><span class="nf">destroy_process_group</span><span class="p">()</span>
</code></pre></div></div>

<p>We need to take care of saving and loading the models which can be referred to <a href="https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html">this link</a></p>

<h3 id="fully-sharded-data-parallel-fsdp">Fully Sharded Data Parallel (FSDP)</h3>

<p>Let’s first remind ourselves about the drawbacks of DDP. DDP has a copy of whole model weights, gradients, and optimizer in each GPU. When models get bigger it won’t completely fit in One GPU’s memory. We don’t want to only fit model’s weight on a GPU, we need to fit it’s gradients, and the biiiiiig optimizer which contains twice the parameters as model’s weight.</p>

<p>How about distributing parameters on different GPUs and then gathering the required parameters when doing the specific operation. This is exactly what Fully Sharded Data Parallel does.</p>

<p>Let’s consider we have a layer that has a flat weight matrix and 3 GPUs, we divide 1/3 of the weights into each GPU as shown in the picture below and pass different batch to each GPU so that it can perform computation simultaneously.</p>

<p><img src="fsdp1.png" alt="fsdp1" />
Image source: <a href="https://www.youtube.com/watch?v=6pVn6khIgiI">The SECRET Behind ChatGPT’s Training That Nobody Talks About | FSDP Explained</a></p>

<p>Data reaches the GPU but GPU doesn’t have the full weight to complete the computation, so each GPU calls all_gather collective to collect weights from all the GPUs. After all gather all 3 GPUs will have the full weight <code class="language-plaintext highlighter-rouge">[0..1024]</code> and it can do the computation as shown in the figure below.
<img src="fsdp2.png" alt="fsdp2" />
After computing the activations it instantly frees up the GPU memory. Next, is the case of backward pass.</p>

<p>Similar to image above, it first all gathers the weights and compute the gradients, and reduce scatter so that each GPU only has gradients for their respective shard and then when we do optimizer.step() it will only update it’s respective shard of optimizer. See how we shard weights, gradients, and optimizer across GPUs but only gather weights, this helps a lot. BUT a GPU should have memory to at least store model’s full weights.
If you want to visually see how FSDP work, this <a href="https://www.youtube.com/watch?v=6pVn6khIgiI">video</a> might be very helpful.</p>

<p>We’ll see how FSDP shards weight in detail with code below along with Tensor Parallelism explained in the next section.</p>

<h3 id="tensor-parallelism">Tensor Parallelism</h3>

<p>Now let’s shard the computation. Previously, we sharded weights but in order to do computation (matrix multiplication), we all gathered all the parameters in each GPU then do the computation. Lets say we have very big parameter in our <code class="language-plaintext highlighter-rouge">net1</code>in the above example, so we shard that parameter into available GPUs, but when we do the .forward() call all weights need to be gathered inorder to do computation, so there’s a requirement that <code class="language-plaintext highlighter-rouge">net1</code>’s weight <strong>MUST</strong> fit in each GPU, what if it’s too big that it can’t fit in a single GPU. There’s a solution: Tensor Parallelism</p>

<p>As the name suggest we shard the matrix either row-wise or column-wise. Before understanding it would be easier to understand Tensor Parallelism if we remember some properties of matrix multiplication.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A = [  1   2   3   4
       5   6   7   8
       9  10  11  12
      13  14  15  16 ]

X = [ 1  0  0  0
      0  1  0  0 ]

# Shard A column wise, suppose we have 2 GPUS each half is stored in each GPU

A1 = [  1   2
        5   6
        9  10
       13  14 ]


A2 = [  3   4
        7   8
       11  12
       15  16 ]

X @ A1 = [ 1   2
           5   6 ]
X @ A2 = [ 3   4
           7   8 ]

# we can simply concatenate the two halves to get the full output

X @ A = [ 1   2    3   4
	      5   6   7   8 ]


</code></pre></div></div>

<p>Similarly, when we shard A row-wise,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
X @ A = [X @ A1 + X @ A2] # NOTE: "A" should be sharded such that its halves can be multiplied with X.
</code></pre></div></div>

<p>Let’s understand this with the help of this famous as well as the crucial implementation.
This is a sample code for FFN used in almost all the models these days. Since we apply gating mechanism here instead of using two weights we use three weights.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># forward in the FeedForward layer
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">w2</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">silu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">w1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="nf">w3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>

<p>Assume we have 2 GPUs. We have three weights.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>w1's shape = w3's shape =  (768, 1024)
w2's shape = (1024, 768)
</code></pre></div></div>

<p>We first shard w1 and w3’s weight column-wise and w2 row wise</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>GPU1 will get w1's (768, 512). # call it gpu1_w1
GPU1 will get w3's (768, 512). # call it gpu1_w3 and so on.
GPU1 will get w2's (512, 768)

------------------------------
GPU2 will get w1's (768, 512).
GPU2 will get w3's (768, 512).
GPU2 will get w2's (512, 768)

</code></pre></div></div>

<p>we have input x of shape (3,768) that is replicated across both the GPUs.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In GPU1, we can perform these operations below simultaneously,

a = x @ gpu1_w1
b = x @ gpu1_w3

since there was element-wise multplication between a and b ( from F.silu(self.w1(x)) * self.w3(x))

GPU1 can simply do c = a * b

then do

d1 = c @ gpu1_w2

--------------------------------------------------
While GPU1 was doing all these operations above, GPU2 was doing the same operation (at the same time) for it's own weights

a = x @ gpu2_w1
b = x @ gpu2_w3

since there was element-wise multplication between a and b ( from F.silu(self.w1(x)) * self.w3(x))

GPU1 can simply do c = a * b

then do

d2 = c @ gpu2_w2

</code></pre></div></div>

<p>Afterwards, we can simply do all_reduce that will add the d1 + d2 and provide GPU1 and GPU2 with the summed result.</p>

<p>Similarly, we can do row-wise and column-wise sharding for attention as well which can be seen in the illustration below.</p>

<p><img src="attn1-n.png" alt="attn1-n" />
<img src="attn2-n.png" alt="att2-n" /></p>

<p>Both should be easy to understand, if not please take a look at the <a href="https://insujang.github.io/2024-01-11/tensor-parallelism-and-sequence-parallelism-detailed-analysis/">source</a></p>

<p>Enough of theory, how do we do that in using pytorch?
——–&gt; <strong>DTensor</strong> &lt;——–</p>

<p>What is DTensor?</p>

<blockquote>
  <p>PyTorch DTensor offers simple and flexible tensor sharding primitives that transparently handles distributed logic,</p>
</blockquote>

<p>Basically, it aids sharding/replicating tensors in pytorch.
This is what we use under the hood in pytorch’s Tensor Parallelism and FSDP2.</p>

<p>Couple of things we need to understand properly are: device_mesh and placements.</p>

<p>For now lets consider we have 2 GPUs, with DeviceMesh of simple 1D mesh [0,1]. It simply denotes how GPUs are structured in a grid (more on 2D, 3D mesh later) and both 0 and 1 GPU will be used for Tensor Parallelism.</p>

<h4 id="placements">Placements</h4>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Shard</code>: Tensor sharded on the tensor dimension <code class="language-plaintext highlighter-rouge">dim</code> on the devices of the <code class="language-plaintext highlighter-rouge">DeviceMesh</code> dimension</li>
  <li><code class="language-plaintext highlighter-rouge">Replicate</code>: Tensor replicated on the devices of the <code class="language-plaintext highlighter-rouge">DeviceMesh</code> dimension</li>
  <li><code class="language-plaintext highlighter-rouge">Partial</code>: Tensor is pending reduction on the devices of the <code class="language-plaintext highlighter-rouge">DeviceMesh</code> dimension</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.distributed.device_mesh</span> <span class="kn">import</span> <span class="n">init_device_mesh</span>
<span class="c1"># from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy
</span><span class="kn">from</span> <span class="n">torch.distributed._composable.fsdp</span> <span class="kn">import</span> <span class="n">fully_shard</span>
<span class="kn">from</span> <span class="n">torch.distributed.tensor.parallel</span> <span class="kn">import</span> <span class="n">parallelize_module</span><span class="p">,</span> <span class="n">ColwiseParallel</span><span class="p">,</span> <span class="n">RowwiseParallel</span><span class="p">,</span> <span class="n">SequenceParallel</span><span class="p">,</span> <span class="n">PrepareModuleInput</span>
<span class="kn">from</span> <span class="n">torch.distributed._tensor</span> <span class="kn">import</span> <span class="n">Shard</span><span class="p">,</span> <span class="n">Replicate</span><span class="p">,</span> <span class="n">distribute_tensor</span><span class="p">,</span> <span class="n">DTensor</span>


<span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="n">backend</span> <span class="o">=</span> <span class="sh">'</span><span class="s">nccl</span><span class="sh">'</span> <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">else</span> <span class="sh">'</span><span class="s">gloo</span><span class="sh">'</span>

<span class="n">rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">RANK</span><span class="sh">"</span><span class="p">])</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">WORLD_SIZE</span><span class="sh">"</span><span class="p">])</span>
<span class="n">local_rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">LOCAL_RANK</span><span class="sh">"</span><span class="p">])</span>


<span class="n">dist</span><span class="p">.</span><span class="nf">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

<span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>


<span class="n">mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">17</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>

<span class="n">x_local</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Replicate</span><span class="p">()])</span>

<span class="n">y_local</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Replicate</span><span class="p">()])</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> X local </span><span class="si">{</span><span class="n">x_local</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> Y local </span><span class="si">{</span><span class="n">y_local</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> MM_local </span><span class="si">{</span><span class="n">x_local</span> <span class="o">@</span> <span class="n">y_local</span><span class="p">.</span><span class="n">T</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div>

<p>x and y are of type torch.Tensor and with distrbute_tensor they will be sharded/replicated based on the placements type over device_mesh. As you can see, we’ve replicated x and y so both of these will have the same output in both GPUs</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
RANK 1
 X local DTensor(local_tensor=tensor([[1, 2, 3, 4],
        [5, 6, 7, 8]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))
 Y local DTensor(local_tensor=tensor([[ 9, 10, 11, 12],
        [13, 14, 15, 16]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))
 MM_local DTensor(local_tensor=tensor([[110, 150],
        [278, 382]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))

RANK 0
 X local DTensor(local_tensor=tensor([[1, 2, 3, 4],
        [5, 6, 7, 8]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))
 Y local DTensor(local_tensor=tensor([[ 9, 10, 11, 12],
        [13, 14, 15, 16]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))
 MM_local DTensor(local_tensor=tensor([[110, 150],
        [278, 382]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))
</code></pre></div></div>

<p>Take this code now.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">17</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>

<span class="n">x_local</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Replicate</span><span class="p">()])</span>

<span class="n">y_local</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">0</span><span class="p">)])</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> Y local </span><span class="si">{</span><span class="n">y_local</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> MM_local </span><span class="si">{</span><span class="n">x_local</span> <span class="o">@</span> <span class="n">y_local</span><span class="p">.</span><span class="n">T</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div>

<p>we replicate x but we shard y so, y will be halved and saved with type Shard(0) in two GPUs with different value and x will be same in both GPUs. we multiply both and the output will be of type Shard(1) (why? because of transpose, the shard type is changed) remember this, this might be helpful later on. Here’s the output</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RANK 1 Y local DTensor(local_tensor=tensor([[13, 14, 15, 16]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=0),))
 MM_local DTensor(local_tensor=tensor([[150],
        [382]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=1),))

RANK 0 Y local DTensor(local_tensor=tensor([[ 9, 10, 11, 12]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=0),))
 MM_local DTensor(local_tensor=tensor([[110],
        [278]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=1),))
</code></pre></div></div>

<p>Let’s do some example similar to this code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">w2</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">silu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">w1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="nf">w3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">x</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Replicate</span><span class="p">()])</span>

<span class="n">w1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">w3</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">17.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">w1_local</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Shard</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)])</span>
<span class="n">w3_local</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">w3</span><span class="p">,</span> <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Shard</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)])</span>
<span class="n">w2_local</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">w2</span><span class="p">,</span> <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">0</span><span class="p">)])</span>


<span class="c1"># print(f'RANK {dist.get_rank()} \n X local {x_local} \n Y local {y_local} \n MM_local {x_local @ y_local.T}')
</span><span class="n">c</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">w1_local</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">w3_local</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">c</span> <span class="o">@</span> <span class="n">w2_local</span>

<span class="c1"># collect d from both GPUs
</span><span class="n">d_collect</span> <span class="o">=</span> <span class="n">d</span><span class="p">.</span><span class="nf">redistribute</span><span class="p">(</span><span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Replicate</span><span class="p">()])</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> w1.shape </span><span class="si">{</span><span class="n">w1_local</span><span class="p">.</span><span class="nf">to_local</span><span class="p">().</span><span class="n">shape</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s">  w3.shape </span><span class="si">{</span><span class="n">w3_local</span><span class="p">.</span><span class="nf">to_local</span><span class="p">().</span><span class="n">shape</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> w2.shape </span><span class="si">{</span><span class="n">w2_local</span><span class="p">.</span><span class="nf">to_local</span><span class="p">().</span><span class="n">shape</span><span class="si">}</span><span class="s"> c values </span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s">, </span><span class="se">\n</span><span class="s"> d values </span><span class="se">\n</span><span class="si">{</span><span class="n">d</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> d collect </span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">d_collect</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RANK 1 w1.shape torch.Size([2, 2])
  w3.shape torch.Size([2, 2])
 w2.shape torch.Size([2, 2]) c values
 DTensor(local_tensor=tensor([[260., 336.],
        [260., 336.],
        [260., 336.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=1),)),
 d values
DTensor(local_tensor=tensor([[596., 596.],
        [596., 596.],
        [596., 596.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Partial(sum),))
 d collect
 DTensor(local_tensor=tensor([[788., 596.],
        [788., 596.],
        [788., 596.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))
        ----------------------------------------------------------
RANK 0 w1.shape torch.Size([2, 2])
  w3.shape torch.Size([2, 2])
 w2.shape torch.Size([2, 2]) c values
 DTensor(local_tensor=tensor([[132., 192.],
        [132., 192.],
        [132., 192.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=1),)),
 d values
DTensor(local_tensor=tensor([[192.,   0.],
        [192.,   0.],
        [192.,   0.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Partial(sum),))
 d collect
 DTensor(local_tensor=tensor([[788., 596.],
        [788., 596.],
        [788., 596.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Replicate(),))
</code></pre></div></div>

<p>as you can see d has two different values with placement type Partial and when we call redistribute on d with Replicate() type the result is all gathered (summed) and will be same on both GPU.</p>

<p>This was all manual Tensor Parallelism. We don’t go on writing these for all the matrix multiplications so we use some different method which implements the same logic under the hood.</p>

<h4 id="colwiseparallel">ColwiseParallel()</h4>

<blockquote>
  <p>Partition a compatible nn.Module in a column-wise fashion.</p>
</blockquote>

<p>NOTE: ColwiseParallel() only operates on nn.Linear() and nn.Embedding() module.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">ToyModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ToyModel</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">w1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">w3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">w2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">self</span><span class="p">.</span><span class="n">w1</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span> <span class="c1"># cause weight will have shape opposite of whats specified in nn.Linear()
</span>            <span class="n">self</span><span class="p">.</span><span class="n">w3</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">17.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">w2</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">w2</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">w1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="nf">w3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">ToyModel</span><span class="p">()</span>

<span class="n">tp_plan</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">w1</span><span class="sh">"</span> <span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">w2</span><span class="sh">"</span> <span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">w3</span><span class="sh">"</span> <span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">()</span>
<span class="p">}</span>

<span class="n">model</span> <span class="o">=</span> <span class="nf">parallelize_module</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">tp_plan</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="nf">distribute_tensor</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span> <span class="n">placements</span><span class="o">=</span><span class="p">[</span><span class="nc">Replicate</span><span class="p">()])</span> <span class="c1"># this is simply a tensor because ColwiseParallel will automatically convert it's input from torch.Tensor to torch.DTensor
</span><span class="n">out</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> w1.weight </span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">w1</span><span class="p">.</span><span class="n">weight</span><span class="si">}</span><span class="s">, </span><span class="se">\n</span><span class="s"> MM values </span><span class="se">\n</span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="nf">w1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">model</span><span class="p">.</span><span class="nf">w3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s"> output values </span><span class="se">\n</span><span class="si">{</span><span class="n">out</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RANK 1 w1.weight
 DTensor(local_tensor=tensor([[5., 6.],
        [7., 8.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)),
 MM values
tensor([[297., 465.],
        [297., 465.],
        [297., 465.]], grad_fn=&lt;MulBackward0&gt;) output values
AsyncCollectiveTensor(tensor([[ 0., 57.],
        [ 0., 57.],
        [ 0., 57.]]))

RANK 0 w1.weight
 DTensor(local_tensor=tensor([[1., 2.],
        [3., 4.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)),
 MM values
tensor([[ 57., 161.],
        [ 57., 161.],
        [ 57., 161.]], grad_fn=&lt;MulBackward0&gt;) output values
AsyncCollectiveTensor(tensor([[ 0., 57.],
        [ 0., 57.],
        [ 0., 57.]])
</code></pre></div></div>

<p>This code tries to exactly do the task that we did previously, the logic is the same but the output is different because of how the arranged weights is transposed and sharded.</p>

<p>Let’s me explain how ColwiseParallel works internally.</p>

<ul>
  <li>It takes input x, if it’s a tensor it will make it a DTensor and if not in the desired layout it will redistribute it to be the desired layout.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="n">self</span><span class="p">.</span><span class="n">input_layouts</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_layouts</span> <span class="ow">or</span> <span class="nc">Replicate</span><span class="p">(),)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_layouts</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_layouts</span> <span class="ow">or</span> <span class="nc">Shard</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">desired_input_layouts</span> <span class="o">=</span> <span class="p">(</span><span class="nc">Replicate</span><span class="p">(),)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">_prepare_input_fn</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="p">,</span> <span class="n">desired_input_layouts</span><span class="p">,</span> <span class="n">mod</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">device_mesh</span>
    <span class="p">):</span>
        <span class="c1"># TODO: figure out dynamo support for instance method and switch this to instance method
</span>
        <span class="c1"># annotate module input placements/sharding with input_layouts
</span>        <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">DTensor</span><span class="p">):</span>
            <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">DTensor</span><span class="p">.</span><span class="nf">from_local</span><span class="p">(</span>
                <span class="n">input_tensor</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">,</span> <span class="n">input_layouts</span><span class="p">,</span> <span class="n">run_check</span><span class="o">=</span><span class="bp">False</span>
            <span class="p">)</span>

        <span class="c1"># transform the input layouts to the desired layouts of ColwiseParallel
</span>        <span class="k">if</span> <span class="n">input_layouts</span> <span class="o">!=</span> <span class="n">desired_input_layouts</span><span class="p">:</span>
            <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="p">.</span><span class="nf">redistribute</span><span class="p">(</span>
                <span class="n">placements</span><span class="o">=</span><span class="n">desired_input_layouts</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="bp">True</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">input_tenso</span>
</code></pre></div></div>

<ul>
  <li>ColwiseParallel will shard the module’s weights on 0’th dimension not 1, why??</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    <span class="k">def</span> <span class="nf">_partition_linear_fn</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">):</span>
        <span class="c1"># colwise shard weight/bias to Shard(0), weight be Shard(0)
</span>        <span class="c1"># means Colwise as Linear is input * weight^T + bias, where
</span>        <span class="c1"># weight would become Shard(1)
</span>        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">module</span><span class="p">.</span><span class="nf">named_parameters</span><span class="p">():</span>
            <span class="n">dist_param</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span>
                <span class="nf">distribute_tensor</span><span class="p">(</span>
                    <span class="n">param</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">,</span> <span class="p">[</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">0</span><span class="p">)],</span> <span class="n">src_data_rank</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">src_data_rank</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">module</span><span class="p">.</span><span class="nf">register_parameter</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">dist_param</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>because under the hood in nn.Linear, the operation is x @ w.T, the weights are transposed, so</li>
  <li>weights are sharded on dim=0, but output will be Shard(-1) because we multiply with the transpose weight (see example above where we implemented MM with transposed weights), and by default use_local_output is True which will convert the DTensor back to torch.Tensor</li>
</ul>

<p>This is a part of code from pytorch’s ColwiseParallel() <a href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/tensor/parallel/style.py#L89">here</a> shows the default input and output layouts for ColwiseParallel().</p>

<h4 id="rowwiseparallel">RowwiseParallel()</h4>

<p>it works exactly the opposite of ColwiseParallel(), it shards weights on dim=1, i.e Shard(dim=1)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="n">self</span><span class="p">.</span><span class="n">input_layouts</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_layouts</span> <span class="ow">or</span> <span class="nc">Shard</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_layouts</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_layouts</span> <span class="ow">or</span> <span class="nc">Replicate</span><span class="p">(),</span>
</code></pre></div></div>

<p>and it’s desired layout is Shard(-1).</p>

<p>That’s it, we need to take care of three things.</p>

<ol>
  <li>what input it expects (i.e Shard() or Replicate())</li>
  <li>what it outputs (i.e Shar() or Replicate())</li>
  <li>How it shards it’s module weights.</li>
</ol>

<h4 id="sequenceparallel">SequenceParallel()</h4>

<p>It generally operates on the sequence dimension. Say the activations are of shape B,T,C = batch_size, sequence_length, embedding dimension.</p>

<p>It will shard the sequence dimension T. There is no sharding of weight in this SequenceParallel(). It is applied to the LayerNorm()/RMSNorm() it will split on T and calculate mean/std based on the given T, there’s no need for communication because mean/std are calculated on C’th dimension and don’t depend on T.</p>

<p>However, the <strong>output</strong> of LayerNorm/RMSNorm is typically <strong>all-gathered</strong> across the <strong>T</strong> shards before it is consumed by the next layer (e.g., an attention or MLP block) (more on this below)
self.sequence_sharding = (Shard(sequence_dim)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">_prepare_input_fn</span><span class="p">(</span><span class="n">sequence_sharding</span><span class="p">,</span> <span class="n">mod</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">):</span>
        <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">DTensor</span><span class="p">):</span>
            <span class="c1"># if the passed in input DTensor is not sharded on the sequence dim, we need to redistribute it
</span>            <span class="k">if</span> <span class="n">input_tensor</span><span class="p">.</span><span class="n">placements</span> <span class="o">!=</span> <span class="n">sequence_sharding</span><span class="p">:</span>
                <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="p">.</span><span class="n">redistribute</span><span class="p">(</span>
                    <span class="n">placements</span><span class="o">=</span><span class="n">sequence_sharding</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="bp">True</span>
		<span class="k">return</span> <span class="n">input_tens</span>
</code></pre></div></div>

<h4 id="tensor-parallelism-on-transformers-model">Tensor Parallelism on Transformers Model</h4>

<p>Let’s apply what we learned above in a transformers model. I won’t be providing the output for this but will only explain what it’s doing. The code below is taken from <a href="https://docs.pytorch.org/tutorials/intermediate/TP_tutorial.html">here</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.distributed.device_mesh</span> <span class="kn">import</span> <span class="n">init_device_mesh</span>
<span class="c1"># from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy
</span><span class="kn">from</span> <span class="n">torch.distributed._composable.fsdp</span> <span class="kn">import</span> <span class="n">fully_shard</span>
<span class="kn">from</span> <span class="n">torch.distributed.tensor.parallel</span> <span class="kn">import</span> <span class="n">parallelize_module</span><span class="p">,</span> <span class="n">ColwiseParallel</span><span class="p">,</span> <span class="n">RowwiseParallel</span><span class="p">,</span> <span class="n">SequenceParallel</span><span class="p">,</span> <span class="n">PrepareModuleInput</span>
<span class="kn">from</span> <span class="n">torch.distributed._tensor</span> <span class="kn">import</span> <span class="n">Shard</span><span class="p">,</span> <span class="n">Replicate</span><span class="p">,</span> <span class="n">distribute_tensor</span><span class="p">,</span> <span class="n">DTensor</span>

<span class="kn">from</span> <span class="n">llama2_model</span> <span class="kn">import</span> <span class="n">Transformer</span><span class="p">,</span> <span class="n">ModelArgs</span>


<span class="c1"># import warnings # ignore all warning messages
# warnings.filterwarnings("ignore")
</span>
<span class="c1"># Code for helping init process group
</span><span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="n">backend</span> <span class="o">=</span> <span class="sh">'</span><span class="s">nccl</span><span class="sh">'</span> <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">else</span> <span class="sh">'</span><span class="s">gloo</span><span class="sh">'</span>

<span class="n">rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">RANK</span><span class="sh">"</span><span class="p">])</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">WORLD_SIZE</span><span class="sh">"</span><span class="p">])</span>
<span class="n">local_rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">LOCAL_RANK</span><span class="sh">"</span><span class="p">])</span>

<span class="c1"># print('local rank', local_rank)
</span>
<span class="n">dist</span><span class="p">.</span><span class="nf">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

<span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>

<span class="n">simple_llama2_config</span> <span class="o">=</span> <span class="nc">ModelArgs</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">.</span><span class="nf">from_model_args</span><span class="p">(</span><span class="n">simple_llama2_config</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># model.init_weights()
</span>
<span class="c1"># """
# mesh = init_device_mesh('cpu', (2, 4), mesh_dim_names=["FSDP", "TP"])
</span>
<span class="n">mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">])</span>

<span class="n">layer_tp_plan</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># Now the input and output of SequenceParallel has Shard(1) layouts,
</span>    <span class="c1"># to represent the input/output tensors sharded on the sequence dimension
</span>    <span class="sh">"</span><span class="s">attention_norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">attention</span><span class="sh">"</span><span class="p">:</span> <span class="nc">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="nc">Replicate</span><span class="p">()),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Replicate</span><span class="p">(),</span> <span class="nc">Replicate</span><span class="p">()),</span>
    <span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wq</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wk</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wv</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wo</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
    <span class="sh">"</span><span class="s">ffn_norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">feed_forward</span><span class="sh">"</span><span class="p">:</span> <span class="nc">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Replicate</span><span class="p">(),),</span>
    <span class="p">),</span>
    <span class="sh">"</span><span class="s">feed_forward.w1</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">feed_forward.w2</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
    <span class="sh">"</span><span class="s">feed_forward.w3</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(),</span>
<span class="p">}</span>
<span class="c1"># Apply TP
</span><span class="k">for</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">transformer_block</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">):</span>
    <span class="c1"># layer_tp_plan = {...}  # i.e. the plan we just generated
</span>
    <span class="nf">parallelize_module</span><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="n">transformer_block</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">parallelize_plan</span><span class="o">=</span><span class="n">layer_tp_plan</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nf">parallelize_module</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">tok_embeddings</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">(),</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">),</span>
        <span class="sh">"</span><span class="s">norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
        <span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">()</span>
        <span class="p">),</span>
    <span class="p">}</span>
<span class="p">)</span>
</code></pre></div></div>

<p>you can pass input x through model() and inspect where the weights for each modules are stored and what the output will be.</p>

<p>All we need to change/understand is the parallelize_plan. let’s understand the sequence of parallelization in our model that we have created above with the help of Colwise Rowwise and Sequence Parallel.</p>

<p>Step 1</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="sh">"</span><span class="s">tok_embeddings</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">(),</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">),</span>
</code></pre></div></div>

<p>x = (B,T)
out = tok_embeddings(x)
tok_embeddings.weight is sharded by dim=0, output must be replicate but we apply output_layouts=Shard(1) cause next we apply SequenceParallel that expects Shard(1) as input</p>

<p>so out is of shape (B,T/2) its is sharded by dim=1 but its a torch.Tensor if GPU=2</p>

<p>Step 2</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="sh">"</span><span class="s">attention_norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
</code></pre></div></div>

<p>out = attention_norm(out)
converts out(B,T/2, C) which is a torch.Tensor to Shard(1) with prepare_input_fn. SequenceParallel is applied (no weight sharding), only applies LayerNorm/RMSNorm on T/2 tokens in each GPUs. no change of output layout i.e its still Shard(1) of shape B,T/2,C</p>

<p>Step 3</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="sh">"</span><span class="s">attention</span><span class="sh">"</span><span class="p">:</span> <span class="nc">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="nc">Replicate</span><span class="p">()),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Replicate</span><span class="p">(),</span> <span class="nc">Replicate</span><span class="p">()),</span>
    <span class="p">),</span>

</code></pre></div></div>

<p>attention expects whole sequence so convert Shard(1) to Replicate(). under the hood out is combined to form (B,T,C) from B,T/2, C and replicate to both GPUs. Thats what PrepareModuleInput does. (Shard(1), Replicate()) is for different inputs to the attention module Shard(1) is for x and Replicate() is for freq_cis</p>

<p>Step 4</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="sh">"</span><span class="s">attention.wq</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wk</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wv</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
</code></pre></div></div>

<p>apply Shard(0) to attention.wq.weight, but output is Shard(-1) (why? already explained above).</p>

<p>Step 5</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"</span><span class="s">attention.wo</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<p>Expects input to be Shard(-1) which is correct, should output replicate (after doing all reduce of the partial sum) but instead converts to Shard(1) because next we apply SequenceParallelism for ffn_norm.</p>

<p>Other steps should be straightforward, follow the sequence described below. Finally the output (B,T,vocab_size) is replicated across both the GPUs.</p>

<p>In pytorch’s Tensor Parallel docs they suggest also adding loss parallel, because replicating the output logits which is of size (B,T,vocab_size) can be expensive. The only change we need to do is keep the use_local_output which will make the output of that output module to be DTensor.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nf">parallelize_module</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">tp_mesh</span><span class="p">,</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">tok_embeddings</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">(),</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">),</span>
        <span class="sh">"</span><span class="s">norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
        <span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="c1"># use DTensor as the output
</span>            <span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">},</span>
<span class="p">)</span>
</code></pre></div></div>

<p>and apply loss_parallel() context.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">torch.distributed.tensor.parallel</span> <span class="kn">import</span> <span class="n">loss_parallel</span>

<span class="n">pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
<span class="k">with</span> <span class="nf">loss_parallel</span><span class="p">():</span>
    <span class="c1"># assuming pred and labels are of the shape [batch, seq, vocab]
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">pred</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
</code></pre></div></div>

<p>I don’t seem to understand this, because they don’t provide enough explanation on their <a href="https://docs.pytorch.org/tutorials/intermediate/TP_tutorial.html">page</a>.</p>

<p>My assumption is that each gpus will have (B,T,vocab_size/2) matrix, loss calculation depends on the full vocab_size cause we need to convert the logits into probabilities using softmax function. What they could be doing is calculate the local sum of all e^(x) in vocab_size do all_reduce and then calculate loss.</p>

<h3 id="device-mesh">Device mesh</h3>

<h4 id="2d-mesh">2D mesh</h4>

<p>It’s very important that we have no confusion on how device mesh works, and how TP and FSDP works using the device mesh. (it has haunted for a day, because I did not understand it properly)</p>

<p>let’s first start with a 2D mesh and apply tensor parallelism.
our original mesh will look like this</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[0,1],
[2,3]]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">FSDP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">])</span>

<span class="n">layer_tp_plan</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># Now the input and output of SequenceParallel has Shard(1) layouts,
</span>    <span class="c1"># to represent the input/output tensors sharded on the sequence dimension
</span>    <span class="sh">"</span><span class="s">attention_norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">attention</span><span class="sh">"</span><span class="p">:</span> <span class="nc">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="nc">Replicate</span><span class="p">()),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Replicate</span><span class="p">(),</span> <span class="nc">Replicate</span><span class="p">()),</span>
    <span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wq</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wk</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wv</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wo</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
    <span class="sh">"</span><span class="s">ffn_norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">feed_forward</span><span class="sh">"</span><span class="p">:</span> <span class="nc">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Replicate</span><span class="p">(),),</span>
    <span class="p">),</span>
    <span class="sh">"</span><span class="s">feed_forward.w1</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">feed_forward.w2</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
    <span class="sh">"</span><span class="s">feed_forward.w3</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(),</span>
<span class="p">}</span>
<span class="c1"># Apply TP
</span><span class="k">for</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">transformer_block</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">):</span>
    <span class="c1"># layer_tp_plan = {...}  # i.e. the plan we just generated
</span>
    <span class="nf">parallelize_module</span><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="n">transformer_block</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">parallelize_plan</span><span class="o">=</span><span class="n">layer_tp_plan</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nf">parallelize_module</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">tok_embeddings</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">(),</span>
            <span class="c1"># output_layouts=Shard(1),
</span>        <span class="p">),</span>
        <span class="sh">"</span><span class="s">norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
        <span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">()</span>
        <span class="p">),</span>
    <span class="p">}</span>
<span class="p">)</span>
</code></pre></div></div>

<p>we apply TP on only the TP mesh <code class="language-plaintext highlighter-rouge">mesh['TP']</code>
but what is the output of <code class="language-plaintext highlighter-rouge">mesh['TP']</code> ?</p>

<p>there could be different ways to understand this, but I understand it this way.
Assume we are in process 0, i.e
our original mesh is <code class="language-plaintext highlighter-rouge">mesh['FSDP','TP']</code>, we try to get <code class="language-plaintext highlighter-rouge">mesh['TP']</code> where FSDP is not there so lets draw a line across ‘FSDP’ dimensions (0th and 1st) and see which TP sub-matrix falls in the drawn line. since we were in process 0, submatrix <code class="language-plaintext highlighter-rouge">[0,1]</code> is chosen and our process 0 will only <code class="language-plaintext highlighter-rouge">[0,1]</code> when we output <code class="language-plaintext highlighter-rouge">mesh['TP']</code> and ignore others. We’ll get the same when we are on process 1, but we’ll get <code class="language-plaintext highlighter-rouge">[2,3]</code> when we’re on either 2 or 3 processes.</p>

<p>so the simple logic is to draw line across dimension that is not in the specified mesh i.e FSDP in this case, and see on which submatrix our current process falls. It’s better to get a pen and paper and understand it deeply.
<img src="fsdp_mat.png" alt="fsdp_mat" /></p>

<p>We pass this mesh to parallelize_module that takes in only 1D mesh and applies the Sharding plan across that 1D mesh, For example, if we are in process 0, the weights of size (8,8) are sharded across 0 and 1 GPUs so each gpu will get (4,8) (assume Shard(0)) and if we’re on process 2 weights are sharded across 2,3.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="nf">parallelize_module</span><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="n">transformer_block</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">parallelize_plan</span><span class="o">=</span><span class="n">layer_tp_plan</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></div>

<p>now let’s apply the FSDP sharding</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
	<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
		<span class="nf">fully_shard</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">])</span>

	<span class="nf">fully_shard</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div>

<p>As we did above (drawing line and getting submatrix), process 0 will see mesh[‘FSDP’] = [0,2]
and process 1 will see [1,3].</p>

<p>fully_shard takes two types of mesh 1D, and 2D mesh. Since we have 1D mesh in this case, it will shard the weights across the 1D mesh with Shard(0) placement. i.e let’s say we’re on process 0, 0 already had (4,8) matrix, and when we apply fully_shard the dimension 0 is again split and process 0 will get (2,8) matrix and it’s other half will be stored in process 2. If you’re confused please bear with me, we’ll see with examples on 3D mesh below.</p>

<h4 id="3d-mesh">3D mesh</h4>

<p><img src="matrix-mul.png" alt="matrix-mul" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
<span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]]</span>

<span class="p">[[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span>
<span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">]]]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">DDP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">FSDP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">])</span>
</code></pre></div></div>

<p>We apply the same TP plan with mesh[‘TP’], similar to above, 0,1 will see mesh [0,1] 2,3 will see mesh [2,3] and so on. Let’s see one worked out example, we initially have attention_wq matrix’s weight of size (8,8) arranged in order. We only apply TP (no FSDP). So each TP axis should have same sharded attention_wq matrix i.e for TP axis 0, rank 0,2,4,6 will have same matrix and for TP axis 1, ranks 1,2,5,7 will have same matrix.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.distributed.device_mesh</span> <span class="kn">import</span> <span class="n">init_device_mesh</span>
<span class="c1"># from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy
</span><span class="kn">from</span> <span class="n">torch.distributed._composable.fsdp</span> <span class="kn">import</span> <span class="n">fully_shard</span>
<span class="kn">from</span> <span class="n">torch.distributed.tensor.parallel</span> <span class="kn">import</span> <span class="n">parallelize_module</span><span class="p">,</span> <span class="n">ColwiseParallel</span><span class="p">,</span> <span class="n">RowwiseParallel</span><span class="p">,</span> <span class="n">SequenceParallel</span><span class="p">,</span> <span class="n">PrepareModuleInput</span>
<span class="kn">from</span> <span class="n">torch.distributed._tensor</span> <span class="kn">import</span> <span class="n">Shard</span><span class="p">,</span> <span class="n">Replicate</span><span class="p">,</span> <span class="n">distribute_tensor</span><span class="p">,</span> <span class="n">DTensor</span>

<span class="kn">from</span> <span class="n">llama2_model</span> <span class="kn">import</span> <span class="n">Transformer</span><span class="p">,</span> <span class="n">ModelArgs</span>


<span class="c1"># import warnings # ignore all warning messages
# warnings.filterwarnings("ignore")
</span>
<span class="c1"># Code for helping init process group
</span><span class="n">device</span> <span class="o">=</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="n">backend</span> <span class="o">=</span> <span class="sh">'</span><span class="s">gloo</span><span class="sh">'</span>

<span class="n">rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">RANK</span><span class="sh">"</span><span class="p">])</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">WORLD_SIZE</span><span class="sh">"</span><span class="p">])</span>
<span class="n">local_rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">LOCAL_RANK</span><span class="sh">"</span><span class="p">])</span>

<span class="c1"># print('local rank', local_rank)
</span>
<span class="n">dist</span><span class="p">.</span><span class="nf">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

<span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>

<span class="n">simple_llama2_config</span> <span class="o">=</span> <span class="nc">ModelArgs</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">.</span><span class="nf">from_model_args</span><span class="p">(</span><span class="n">simple_llama2_config</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># model.tok_embeddings.weight = nn.Parameter(torch.arange(1.,33.).reshape(8,4))
</span>
<span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">attention</span><span class="p">.</span><span class="n">wq</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">65.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>



<span class="n">mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">DDP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">FSDP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">])</span>
<span class="n">layer_tp_plan</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># Now the input and output of SequenceParallel has Shard(1) layouts,
</span>    <span class="c1"># to represent the input/output tensors sharded on the sequence dimension
</span>    <span class="sh">"</span><span class="s">attention_norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">attention</span><span class="sh">"</span><span class="p">:</span> <span class="nc">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="nc">Replicate</span><span class="p">()),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Replicate</span><span class="p">(),</span> <span class="nc">Replicate</span><span class="p">()),</span>
    <span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wq</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wk</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wv</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span><span class="n">use_local_output</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">attention.wo</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
    <span class="sh">"</span><span class="s">ffn_norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">feed_forward</span><span class="sh">"</span><span class="p">:</span> <span class="nc">PrepareModuleInput</span><span class="p">(</span>
        <span class="n">input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),),</span>
        <span class="n">desired_input_layouts</span><span class="o">=</span><span class="p">(</span><span class="nc">Replicate</span><span class="p">(),),</span>
    <span class="p">),</span>
    <span class="sh">"</span><span class="s">feed_forward.w1</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">feed_forward.w2</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span><span class="n">output_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
    <span class="sh">"</span><span class="s">feed_forward.w3</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(),</span>
<span class="p">}</span>
<span class="c1"># Apply TP
</span><span class="k">for</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">transformer_block</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">):</span>
    <span class="c1"># layer_tp_plan = {...}  # i.e. the plan we just generated
</span>
    <span class="nf">parallelize_module</span><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="n">transformer_block</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">parallelize_plan</span><span class="o">=</span><span class="n">layer_tp_plan</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nf">parallelize_module</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">tok_embeddings</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">(),</span>
            <span class="c1"># output_layouts=Shard(1),
</span>        <span class="p">),</span>
        <span class="sh">"</span><span class="s">norm</span><span class="sh">"</span><span class="p">:</span> <span class="nc">SequenceParallel</span><span class="p">(),</span>
        <span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ColwiseParallel</span><span class="p">(</span>
            <span class="n">input_layouts</span><span class="o">=</span><span class="nc">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">output_layouts</span><span class="o">=</span><span class="nc">Replicate</span><span class="p">()</span>
        <span class="p">),</span>
    <span class="p">}</span>
<span class="p">)</span>


<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">17</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">16</span><span class="p">)</span>

<span class="k">if</span> <span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">MESHDIM </span><span class="si">{</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">DDP</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s"> TP MESH </span><span class="si">{</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># import time
# time.sleep(5)
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Global rank: </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s">, attn_wq </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">attention</span><span class="p">.</span><span class="n">wq</span><span class="p">.</span><span class="n">weight</span><span class="si">}</span><span class="s"> shape: </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">attention</span><span class="p">.</span><span class="n">wq</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="nf">to_local</span><span class="p">().</span><span class="n">shape</span><span class="si">}</span><span class="s"> </span><span class="se">\n\n</span><span class="sh">'</span><span class="p">)</span>
<span class="c1"># print(f'Global rank: {dist.get_rank()}, \n\n OUTPUT:\n {model.tok_embeddings(x).shape}')
</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>output
Global rank: 5, tok_embeddings DTensor(local_tensor=tensor([[33., 34., 35., 36., 37., 38., 39., 40.],
        [41., 42., 43., 44., 45., 46., 47., 48.],
        [49., 50., 51., 52., 53., 54., 55., 56.],
        [57., 58., 59., 60., 61., 62., 63., 64.]]), device_mesh=DeviceMesh('cpu', [4, 5], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])


Global rank: 4, tok_embeddings DTensor(local_tensor=tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12., 13., 14., 15., 16.],
        [17., 18., 19., 20., 21., 22., 23., 24.],
        [25., 26., 27., 28., 29., 30., 31., 32.]]), device_mesh=DeviceMesh('cpu', [4, 5], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])


MESHDIM DeviceMesh('cpu', [[0, 2], [4, 6]], mesh_dim_names=('DDP', 'FSDP')) TP MESH DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',))

Global rank: 1, tok_embeddings DTensor(local_tensor=tensor([[33., 34., 35., 36., 37., 38., 39., 40.],
        [41., 42., 43., 44., 45., 46., 47., 48.],
        [49., 50., 51., 52., 53., 54., 55., 56.],
        [57., 58., 59., 60., 61., 62., 63., 64.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])


Global rank: 0, tok_embeddings DTensor(local_tensor=tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12., 13., 14., 15., 16.],
        [17., 18., 19., 20., 21., 22., 23., 24.],
        [25., 26., 27., 28., 29., 30., 31., 32.]]), device_mesh=DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])


Global rank: 7, tok_embeddings DTensor(local_tensor=tensor([[33., 34., 35., 36., 37., 38., 39., 40.],
        [41., 42., 43., 44., 45., 46., 47., 48.],
        [49., 50., 51., 52., 53., 54., 55., 56.],
        [57., 58., 59., 60., 61., 62., 63., 64.]]), device_mesh=DeviceMesh('cpu', [6, 7], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])


Global rank: 6, tok_embeddings DTensor(local_tensor=tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12., 13., 14., 15., 16.],
        [17., 18., 19., 20., 21., 22., 23., 24.],
        [25., 26., 27., 28., 29., 30., 31., 32.]]), device_mesh=DeviceMesh('cpu', [6, 7], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])


Global rank: 3, tok_embeddings DTensor(local_tensor=tensor([[33., 34., 35., 36., 37., 38., 39., 40.],
        [41., 42., 43., 44., 45., 46., 47., 48.],
        [49., 50., 51., 52., 53., 54., 55., 56.],
        [57., 58., 59., 60., 61., 62., 63., 64.]]), device_mesh=DeviceMesh('cpu', [2, 3], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])


Global rank: 2, tok_embeddings DTensor(local_tensor=tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12., 13., 14., 15., 16.],
        [17., 18., 19., 20., 21., 22., 23., 24.],
        [25., 26., 27., 28., 29., 30., 31., 32.]]), device_mesh=DeviceMesh('cpu', [2, 3], mesh_dim_names=('TP',)), placements=(Shard(dim=0),)) shape: torch.Size([4, 8])
</code></pre></div></div>

<p>Now let’s apply the FSDP on mesh[‘DDP’, ‘FSDP’]</p>

<p>but let’s first understand what will our processes. see when we simply print mesh[‘DDP’, ‘FSDP’]. As explained above, apply the rule above, which dimension is not there, draw the line and erase its brackets. In this case, ‘TP’ dimension is not there so if we are on process 0, it will see matrix <code class="language-plaintext highlighter-rouge">[[0,2], [4,6]]</code> which I’ve already printed in the output above as</p>

<p><code class="language-plaintext highlighter-rouge">MESHDIM DeviceMesh('cpu', [[0, 2], [4, 6]], mesh_dim_names=('DDP', 'FSDP')) TP MESH DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',)) </code></p>

<p>but when we’re on process 1 it will see <code class="language-plaintext highlighter-rouge">[[1,3], [5,7]]</code> and so on.</p>

<p>let’s apply FSDP with mesh[‘DDP’, ‘FSDP’]</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
	<span class="nf">fully_shard</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">DDP</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">])</span>

<span class="nf">fully_shard</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">DDP</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div>

<p>the sharding rule isn’t the same when we pass 2D mesh to fully_shard.</p>

<blockquote>
  <p><strong>mesh</strong> (<em>Optional__[</em><a href="https://docs.pytorch.org/docs/stable/distributed.html#torch.distributed.device_mesh.DeviceMesh" title="torch.distributed.device_mesh.DeviceMesh"><em>DeviceMesh</em></a><em>]</em>) – This data parallel mesh defines the sharding and device. If 1D, then parameters are fully sharded across the 1D mesh (FSDP) with <code class="language-plaintext highlighter-rouge">(Shard(0),)</code> placement. If 2D, then parameters are sharded across the 1st dim and replicated across the 0th dim (HSDP) with <code class="language-plaintext highlighter-rouge">(Replicate(), Shard(0))</code> placement. The mesh’s device type gives the device type used for communication; if a CUDA or CUDA-like device type, then we use the current device.</p>
</blockquote>

<p>so assuming we’re on process 0, and it has attention_wq matrix of size (4,8). This (4,8) matrix will be across 1st dimension i.e 0 will get (2,8) matrix and 2 will get the other half (2,8) and this is replicated across 0th dimension i.e process 0 will have the same (2,8) matrix as process 4 and 2 will have same (2,8) matrix as 6.</p>

<p>Here’s the output</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Global rank: 3, tok_embeddings DTensor(local_tensor=tensor([[49., 50., 51., 52., 53., 54., 55., 56.],
        [57., 58., 59., 60., 61., 62., 63., 64.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])


Global rank: 2, tok_embeddings DTensor(local_tensor=tensor([[17., 18., 19., 20., 21., 22., 23., 24.],
        [25., 26., 27., 28., 29., 30., 31., 32.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])


Global rank: 4, tok_embeddings DTensor(local_tensor=tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12., 13., 14., 15., 16.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])


Global rank: 5, tok_embeddings DTensor(local_tensor=tensor([[33., 34., 35., 36., 37., 38., 39., 40.],
        [41., 42., 43., 44., 45., 46., 47., 48.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])


Global rank: 7, tok_embeddings DTensor(local_tensor=tensor([[49., 50., 51., 52., 53., 54., 55., 56.],
        [57., 58., 59., 60., 61., 62., 63., 64.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])


MESHDIM DeviceMesh('cpu', [[0, 2], [4, 6]], mesh_dim_names=('DDP', 'FSDP')) TP MESH DeviceMesh('cpu', [0, 1], mesh_dim_names=('TP',))

Global rank: 6, tok_embeddings DTensor(local_tensor=tensor([[17., 18., 19., 20., 21., 22., 23., 24.],
        [25., 26., 27., 28., 29., 30., 31., 32.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])


Global rank: 0, tok_embeddings DTensor(local_tensor=tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12., 13., 14., 15., 16.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])


Global rank: 1, tok_embeddings DTensor(local_tensor=tensor([[33., 34., 35., 36., 37., 38., 39., 40.],
        [41., 42., 43., 44., 45., 46., 47., 48.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]], [[4, 5], [6, 7]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), _StridedShard(dim=0, sf=2), Shard(dim=0))) shape: torch.Size([2, 8])

</code></pre></div></div>

<p>That’s it.</p>

<p>If we want to shard computation increase TP size, if we want to shard weights increase FSDP size, and if we want to process more batch increase DDP size.</p>

<h3 id="data-parallelism-with-fsdp-and-tp">Data-parallelism with FSDP and TP</h3>

<p>There might be some confusion about how data parallelism works with these sharding techniques. The confusion might be that we only pass different batches to DDP axis, for example: if we have 3D mesh ddp,fsdp,tp = (2,2,2), we might think that we pass one minibatch to ranks from first ddp axis and another to ranks from second ddp axis. BUT no, we can pass different batch across FSDP groups but with a strict condition that they are same across tp groups. For the 3D mesh above we have tp groups <code class="language-plaintext highlighter-rouge">[0,1] [2,3], [4,5], [6,7]</code>
and FSDP group <code class="language-plaintext highlighter-rouge">[0,2],[1,3],[4,6], [5,7]</code> so we can distribute one batch to rank 0 and another to rank 2 and 0 will broadcast the same data to it’s rank in the same tp group i.e rank 1, and similarly 2 will broadcast the same data to it’s rank in the same tp group i.e rank 3.</p>

<p>so each TP group will be have one distinct batch they can process. One important thing we might forget here is that FSDP will average the gradients across it’s group. Ex (gradient-for-one-batch-at-rank-0 + gradient-for-one-batch-at-rank-2)/2 (which is the number of ranks in the same FSDP group)</p>

<p>here’s is the worked out example to understand it better</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span><span class="o">*</span><span class="mi">2</span><span class="p">]</span>

<span class="nd">@spawn_threads_and_init_comms</span>
<span class="k">def</span> <span class="nf">shard_big_tensor</span><span class="p">(</span><span class="n">world_size</span><span class="p">):</span>

    <span class="n">device</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
    <span class="n">model_device_mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">])</span>

    <span class="n">device_mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">dp</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">tp</span><span class="sh">'</span><span class="p">])</span>

    <span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
            <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
            <span class="n">self</span><span class="p">.</span><span class="n">layer0</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer0</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="nc">Model</span><span class="p">()</span>

    <span class="n">model</span><span class="p">.</span><span class="n">layer0</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">17.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

    <span class="nf">parallelize_module</span><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">model_device_mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">parallelize_plan</span><span class="o">=</span><span class="p">{</span>
            <span class="sh">"</span><span class="s">layer0</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">()</span> <span class="c1"># since rowwise parallel will replicate the output layouts, each tp rank will get the same outptu.
</span>        <span class="p">}</span>
    <span class="p">)</span>

    <span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

    <span class="n">optim</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">model_device_mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">].</span><span class="nf">get_local_rank</span><span class="p">()</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span> <span class="c1"># give one data
</span>        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">rank </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> x = </span><span class="si">{</span><span class="n">data_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">out0</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data_list</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1">#         print(out0)
</span>        <span class="n">dist</span><span class="p">.</span><span class="nf">barrier</span><span class="p">()</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
        <span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">out0</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">out0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">rank </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="n">dist</span><span class="p">.</span><span class="nf">barrier</span><span class="p">()</span>
<span class="c1">#     print(f' RANK {dist.get_rank()} before second weights = {(model.layer0.weight.grad)} \n\n')
</span>
    <span class="k">if</span> <span class="n">model_device_mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">].</span><span class="nf">get_local_rank</span><span class="p">()</span> <span class="o">==</span><span class="mi">1</span><span class="p">:</span> <span class="c1"># give one data
</span>        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">rank </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> x = </span><span class="si">{</span><span class="n">data_list</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

        <span class="n">out1</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data_list</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="c1">#         print(out1)
</span>        <span class="n">dist</span><span class="p">.</span><span class="nf">barrier</span><span class="p">()</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
        <span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">out1</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">out1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">rank </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="n">dist</span><span class="p">.</span><span class="nf">barrier</span><span class="p">()</span>

<span class="c1">#     print(f' RANK {dist.get_rank()} after second weights = {model.layer0.weight.grad} \n\n')
</span>

    <span class="n">dt</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">layer0</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="nf">to_local</span><span class="p">()</span>

    <span class="n">dist</span><span class="p">.</span><span class="nf">all_reduce</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="p">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">model_device_mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">].</span><span class="nf">get_group</span><span class="p">())</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s"> RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> after second weights = </span><span class="si">{</span><span class="n">dt</span><span class="o">/</span><span class="mi">2</span><span class="si">}</span><span class="s"> </span><span class="se">\n\n</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">return</span>
</code></pre></div></div>

<p>In the code above, I don’t split the model across FSDP dimension and try to replicate what FSDP does under the hood. In this code we have two different data consider, torch.ones to be one batch and torch.ones x 2 to be another. Initially, I apply TP across TP groups, so two tp groups here <code class="language-plaintext highlighter-rouge">[0,1] and [2,3]</code> will have the same weights, then I distribute batch 1 to rank 0 and batch 2 to rank 2 (why? I’m trying to replicate the behaviour of fsdp here). Similarly, rank 1 will have batch 1 and rank 3 will have batch 2 (why? cause ranks across same tp group must have same data).</p>

<p>after calculating the gradient independently, I sum the gradients using all_reduce and print the averaged gradient.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RANK 1 reduced weights = tensor([[102.3585, 102.3585],
        [260.1725, 260.1725],
        [412.6720, 412.6720],
        [581.2230, 581.2230]])


 RANK 2 reduced weights = tensor([[102.3585, 102.3585],
        [260.1725, 260.1725],
        [412.6720, 412.6720],
        [581.2230, 581.2230]])


 RANK 3 reduced weights = tensor([[102.3585, 102.3585],
        [260.1725, 260.1725],
        [412.6720, 412.6720],
        [581.2230, 581.2230]])


 RANK 0 reduced weights = tensor([[102.3585, 102.3585],
        [260.1725, 260.1725],
        [412.6720, 412.6720],
        [581.2230, 581.2230]])
</code></pre></div></div>

<p>You might be wondering why different tp ranks have the same weights, it because RowwiseParallel calls Replicate() and and it has to do partial sum for output across tp ranks.</p>

<p>Now let’s see if FSDP does the same or not.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span><span class="o">*</span><span class="mi">2</span><span class="p">]</span>

<span class="nd">@spawn_threads_and_init_comms</span>
<span class="k">def</span> <span class="nf">shard_big_tensor</span><span class="p">(</span><span class="n">world_size</span><span class="p">):</span>

    <span class="n">device</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
    <span class="n">model_device_mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">])</span>

    <span class="n">device_mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">dp</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">tp</span><span class="sh">'</span><span class="p">])</span>

    <span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
            <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
            <span class="n">self</span><span class="p">.</span><span class="n">layer0</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer0</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="nc">Model</span><span class="p">()</span>

    <span class="n">model</span><span class="p">.</span><span class="n">layer0</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">17.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

    <span class="nf">parallelize_module</span><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">model_device_mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">TP</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">parallelize_plan</span><span class="o">=</span><span class="p">{</span>
            <span class="sh">"</span><span class="s">layer0</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RowwiseParallel</span><span class="p">()</span> <span class="c1"># since rowwise parallel will replicate the output layouts, each tp rank will get the same outptu.
</span>        <span class="p">}</span>
    <span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="nf">split_data_list</span><span class="p">(</span><span class="n">data_list</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">dp</span><span class="sh">'</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s"> RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> x= </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s"> </span><span class="se">\n\n</span><span class="sh">'</span><span class="p">)</span>


    <span class="nf">fully_shard</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="n">model_device_mesh</span><span class="p">[</span><span class="sh">'</span><span class="s">FSDP</span><span class="sh">'</span><span class="p">])</span>

    <span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

    <span class="n">optim</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">dist</span><span class="p">.</span><span class="nf">barrier</span><span class="p">()</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">out</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">rank </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> loss </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s"> </span><span class="sh">'</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>


    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s"> RANK </span><span class="si">{</span><span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s"> weights = </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">layer0</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">grad</span><span class="si">}</span><span class="s"> </span><span class="se">\n\n</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">return</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> RANK 2 weights = DTensor(local_tensor=tensor([[412.6720, 412.6720],
        [581.2230, 581.2230]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(Shard(dim=0), Shard(dim=1)))


 RANK 0 weights = DTensor(local_tensor=tensor([[102.3585, 102.3585],
        [260.1725, 260.1725]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(Shard(dim=0), Shard(dim=1)))


 RANK 1 weights = DTensor(local_tensor=tensor([[102.3585, 102.3585],
        [260.1725, 260.1725]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(Shard(dim=0), Shard(dim=1)))


 RANK 3 weights = DTensor(local_tensor=tensor([[412.6720, 412.6720],
        [581.2230, 581.2230]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(Shard(dim=0), Shard(dim=1)))
</code></pre></div></div>

<p>Since we also apply FSDP here, the weights are row-wise sharded across in this output, but if you unshard the FSDP (i.e combine weights across FSDP group), you will see they have the same weights, i.e stack the rank 0,2 across dimension 0.</p>

<p>let’s also see if we get the same process two data on only one machine (no distributed training)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span><span class="o">*</span><span class="mi">2</span><span class="p">]</span>
<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer0</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer0</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">Model</span><span class="p">()</span>


<span class="nf">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="n">layer0</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">17.</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>



<span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

<span class="n">optim</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">out0</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data_list</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">out0</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">out0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>

<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

<span class="n">out1</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data_list</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">original</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">out1</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">out1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>

<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>


<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">weights = </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">layer0</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">grad</span><span class="o">/</span><span class="mi">2</span><span class="si">}</span><span class="s"> </span><span class="se">\n\n</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>weights = tensor([[102.3585, 102.3585, 102.3585, 102.3585],
        [260.1725, 260.1725, 260.1725, 260.1725],
        [412.6720, 412.6720, 412.6720, 412.6720],
        [581.2230, 581.2230, 581.2230, 581.2230]])
</code></pre></div></div>

<p>I’ll be writing more on how all these techniques are used in distributed RL training in the next part.</p>

<p>While using parallelism, we need to be very careful to not break the computation graph, because collectives are not differentiable.</p>

<p>For example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@spawn_threads_and_init_comms</span>
<span class="k">def</span> <span class="nf">shard_big_tensor</span><span class="p">(</span><span class="n">world_size</span><span class="p">):</span>
    <span class="n">logsumexp</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="n">device_mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span>
        <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">DDP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">FSDP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="n">logsumexp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span> <span class="o">*</span> <span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span>
    <span class="n">logsumexp</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>

    <span class="n">logsumexps</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">logsumexp</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">device_mesh</span><span class="p">[</span><span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">].</span><span class="nf">size</span><span class="p">())]</span>
    <span class="n">dist</span><span class="p">.</span><span class="nf">all_gather</span><span class="p">(</span><span class="n">logsumexps</span><span class="p">,</span> <span class="n">logsumexp</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">[</span><span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">].</span><span class="nf">get_group</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">device_mesh</span><span class="p">[</span><span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">].</span><span class="nf">get_local_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">---detached ----</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">logsumexps</span><span class="p">)</span>

    <span class="n">dist</span><span class="p">.</span><span class="nf">barrier</span><span class="p">()</span>
    <span class="n">logsumexps</span><span class="p">[</span><span class="n">device_mesh</span><span class="p">[</span><span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">].</span><span class="nf">get_local_rank</span><span class="p">()]</span> <span class="o">=</span> <span class="n">logsumexp</span>

    <span class="k">if</span> <span class="n">device_mesh</span><span class="p">[</span><span class="sh">"</span><span class="s">TP</span><span class="sh">"</span><span class="p">].</span><span class="nf">get_local_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">----not detached----</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">logsumexps</span><span class="p">)</span>

    <span class="c1"># new_logsumexp = torch.stack(logsumexps, dim=-1).logsumexp(dim=-1)
</span>
    <span class="c1"># print(f'rank {dist.get_rank()} logsumexps {new_logsumexp}')
</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## output
---detached ---- [tensor([[0., 0., 0., 0.], [0., 0., 0., 0.]]), tensor([[1., 1., 1., 1.], [1., 1., 1., 1.]])]
---detached ---- [tensor([[2., 2., 2., 2.], [2., 2., 2., 2.]]), tensor([[3., 3., 3., 3.], [3., 3., 3., 3.]])]

----not detached---- [tensor([[0., 0., 0., 0.], [0., 0., 0., 0.]], requires_grad=True), tensor([[1., 1., 1., 1.], [1., 1., 1., 1.]])]

----not detached---- [tensor([[2., 2., 2., 2.], [2., 2., 2., 2.]], requires_grad=True), tensor([[3., 3., 3., 3.], [3., 3., 3., 3.]])]
</code></pre></div></div>

<p>The main culprit is the dist.all_gather which fills in the detached values of logsumexp into the logsumexps (as you can see no requires_grad in the elements in the logsumexps list), so when we do loss.backward() the gradient stops moving from this detached points and won’t travel to the downstream parameters.</p>

<p>But, using this line <code class="language-plaintext highlighter-rouge">logsumexps[device_mesh["TP"].get_local_rank()] = logsumexp</code> we fill in the local rank’s logsumexp in the logsumexps list, so now gradient’s will propagate through it.</p>

<p>Similar to that we need to use something similar while using all_reduce operation. During the forward pass the values will be the reduced one, and during backward, gradients can flow through the tensor variable.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">differentiable_all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">):</span>
    <span class="n">detached_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">detach</span><span class="p">()</span>
    <span class="n">dist</span><span class="p">.</span><span class="nf">all_reduce</span><span class="p">(</span>
        <span class="n">detached_tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="p">.</span><span class="n">ReduceOp</span><span class="p">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">device_mesh</span><span class="p">.</span><span class="nf">get_group</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">tens</span>                                                                                                                                                                                                                              <span class="ow">or</span> <span class="o">+</span> <span class="n">detached_tensor</span> <span class="o">-</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">detach</span><span class="p">()</span>

</code></pre></div></div>]]></content><author><name>cohlem</name></author><category term="blog" /><category term="RL" /><category term="distributed" /><summary type="html"><![CDATA[I was learning how we can do distributed RL training, saw karpathy posting this and thought why not make a complete blog about what I learned so here it is.]]></summary></entry><entry><title type="html">Python</title><link href="http://localhost:4000/2025/06/09/python.html" rel="alternate" type="text/html" title="Python" /><published>2025-06-09T00:00:00+05:45</published><updated>2025-06-09T00:00:00+05:45</updated><id>http://localhost:4000/2025/06/09/python</id><content type="html" xml:base="http://localhost:4000/2025/06/09/python.html"><![CDATA[<h3 id="unpacking-over-indexing">Unpacking over indexing</h3>

<p>why?</p>

<ul>
  <li>less noisy</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># do this
</span><span class="n">a</span><span class="p">,</span><span class="n">b</span> <span class="o">=</span> <span class="n">somehting</span>

<span class="c1"># over
</span><span class="n">a</span> <span class="o">=</span> <span class="n">something</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">somehting</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div>

<p>another example</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># don't do this
</span><span class="n">snacks</span> <span class="o">=</span> <span class="p">[(</span><span class="sh">'</span><span class="s">bacon</span><span class="sh">'</span><span class="p">,</span> <span class="mi">350</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">donut</span><span class="sh">'</span><span class="p">,</span> <span class="mi">240</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">muffin</span><span class="sh">'</span><span class="p">,</span> <span class="mi">190</span><span class="p">)]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">snacks</span><span class="p">)):</span>

    <span class="n">item</span> <span class="o">=</span> <span class="n">snacks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">calories</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">#</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s"> has </span><span class="si">{</span><span class="n">calories</span><span class="si">}</span><span class="s"> calories</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># do this
</span><span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">calorie</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">snacks</span><span class="p">,</span><span class="mi">1</span><span class="p">):</span>
	<span class="nf">print</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">calorie</span><span class="p">)</span>
</code></pre></div></div>

<p>Unpacking can be applied to any iterables (dict, lists, tuples)</p>

<h3 id="slicing">Slicing</h3>

<p>slicing creates a reference to original list, but changing sliced object won’t reflect in original list</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">c</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">d</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">e</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">f</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">g</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">h</span><span class="sh">'</span><span class="p">]</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>

<span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">=</span> <span class="mi">10</span>
<span class="nf">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="c1"># [10, 'b', 'c']
</span><span class="nf">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="c1"># ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']
</span></code></pre></div></div>

<h3 id="striding">Striding</h3>

<p>Avoid using striding (step) i.e start:stop:step altogether, mostly avoid them while using negative strides.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># outputs ['h', 'g', 'f', 'e', 'd', 'c', 'b', 'a']
</span><span class="n">a</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">::</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="c1"># ['g', 'e', 'c', 'a'] # its confusing, so avoid it
</span></code></pre></div></div>

<h3 id="starred-expression">Starred Expression</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">car_ages</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span>
<span class="n">car_ages_descending</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">car_ages</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">oldest</span><span class="p">,</span> <span class="n">second_oldest</span> <span class="o">=</span> <span class="n">car_ages_descending</span>

<span class="c1"># Instead of doing this
</span><span class="n">oldest</span> <span class="o">=</span> <span class="n">car_ages_descending</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">second_oldest</span> <span class="o">=</span> <span class="n">car_ages_descending</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">others</span> <span class="o">=</span> <span class="n">car_ages_descending</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
<span class="nf">print</span><span class="p">(</span><span class="n">oldest</span><span class="p">,</span> <span class="n">second_oldest</span><span class="p">,</span> <span class="n">others</span><span class="p">)</span>

<span class="c1"># Do this
</span><span class="n">oldest</span><span class="p">,</span> <span class="n">second_oldest</span><span class="p">,</span> <span class="o">*</span><span class="n">others</span> <span class="o">=</span> <span class="n">car_ages_descending</span>
<span class="nf">print</span><span class="p">(</span><span class="n">oldest</span><span class="p">,</span> <span class="n">second_oldest</span><span class="p">,</span> <span class="n">others</span><span class="p">)</span>
</code></pre></div></div>

<p>But it can only be used once in an expression, but it should alway pair with at least one normal unpacking. i.e only doing * others = car_ages_descending will not work.</p>

<h3 id="sorting">Sorting</h3>

<p>.sort() can be called on to any built-in types that has ordering, i.e int, float, str. Defaults to sorting in ascending order, use reverse=True for descending.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">numbers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">93</span><span class="p">,</span> <span class="mi">86</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">68</span><span class="p">,</span> <span class="mi">70</span><span class="p">]</span>
<span class="n">numbers</span><span class="p">.</span><span class="nf">sort</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">numbers</span><span class="p">)</span> <span class="c1"># [11, 68, 70, 86, 93]
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Tool</span><span class="p">:</span>
     <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>

         <span class="n">self</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
         <span class="n">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span>

     <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
         <span class="k">return</span> <span class="sa">f</span><span class="sh">'</span><span class="s">Tool(</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">name</span><span class="si">!r}</span><span class="s">, </span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">weight</span><span class="si">}</span><span class="s">)</span><span class="sh">'</span>
<span class="n">tools</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nc">Tool</span><span class="p">(</span><span class="sh">'</span><span class="s">level</span><span class="sh">'</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">),</span>
    <span class="nc">Tool</span><span class="p">(</span><span class="sh">'</span><span class="s">hammer</span><span class="sh">'</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">),</span>
    <span class="nc">Tool</span><span class="p">(</span><span class="sh">'</span><span class="s">screwdriver</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
    <span class="nc">Tool</span><span class="p">(</span><span class="sh">'</span><span class="s">chisel</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">),</span>

<span class="p">]</span>

<span class="c1">#simply calling tools.sort() will result in error, cause Tool class doesn't implement comparision operator by default.
# sort it using key parameter.
# keys expects a function, and the should return by which we want our list to be ordered i.e by name
</span><span class="n">tools</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="n">name</span><span class="p">)</span>
</code></pre></div></div>

<p>Tuples have built in order defined in them, i.e</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="sh">'</span><span class="s">drill</span><span class="sh">'</span><span class="p">)</span> <span class="o">&lt;</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="sh">'</span><span class="s">sander</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<p>in this case, 4 is compared with 4, and since its true ‘drill’ is compared with ‘sander’, we can take advantage of this tuple within our key function to defined the order i.e</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tools</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">name</span><span class="p">))</span>
</code></pre></div></div>

<p>In this case, it sort by ascending order, but first weight is compared AND if any two items have the same weight their name attribute is compared i.e drill and sander have same weights, so d &lt; s</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1">#output
</span><span class="p">[</span><span class="nc">Tool</span><span class="p">(</span><span class="sh">'</span><span class="s">drill</span><span class="sh">'</span><span class="p">,</span>        <span class="mi">4</span><span class="p">),</span>
 <span class="nc">Tool</span><span class="p">(</span><span class="sh">'</span><span class="s">sander</span><span class="sh">'</span><span class="p">,</span>       <span class="mi">4</span><span class="p">),</span>
 <span class="nc">Tool</span><span class="p">(</span><span class="sh">'</span><span class="s">circular saw</span><span class="sh">'</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
 <span class="nc">Tool</span><span class="p">(</span><span class="sh">'</span><span class="s">jackhammer</span><span class="sh">'</span><span class="p">,</span>   <span class="mi">40</span><span class="p">)]</span>
</code></pre></div></div>

<h3 id="list-multiplication-vs-list-comprehension">list multiplication vs list comprehension</h3>

<p>use multiplication carefully.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]]</span> <span class="o">*</span> <span class="mi">2</span>    <span class="c1"># replicates the *same* inner list reference twice
</span><span class="n">b</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span>  <span class="c1"># makes two distinct inner lists
</span>
</code></pre></div></div>

<h3 id="closures-and-decorators">Closures and decorators</h3>

<p>A simple problem. We need to keep track of every object that is printed. In the code below, how do we access data list inside inner function? doing something like below will have global data, which any other functions can access. We need some list that can keep track of every object printed, but is also only accessible to that specific function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">def</span> <span class="nf">inner</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
    <span class="n">data</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>

</code></pre></div></div>

<p>We use closures.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">print_with_memory</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">def</span> <span class="nf">inner</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
        <span class="n">data</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
        <span class="nf">func</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">inner</span>

<span class="n">print_</span> <span class="o">=</span> <span class="nf">print_with_memory</span><span class="p">(</span><span class="k">print</span><span class="p">)</span>
<span class="nf">print_</span><span class="p">(</span><span class="sh">'</span><span class="s">yoo</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print_</span><span class="p">(</span><span class="sh">'</span><span class="s">whats up</span><span class="sh">'</span><span class="p">)</span>
<span class="n">print_</span><span class="p">.</span><span class="n">__closure__</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">cell_contents</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## outputs
yoo
whats up

['yoo', 'whats up']
</code></pre></div></div>

<p>The drawback with closure function above is that inner func only takes one object, whereas it should be able to take in more than one object as parameters. If so, make use of _ args and _ kwargs.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">print_with_memory</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">def</span> <span class="nf">inner</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">data</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span> <span class="c1"># args is a tuple, and kwargs is a dict
</span>        <span class="nf">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">inner</span>

<span class="n">print_</span> <span class="o">=</span> <span class="nf">print_with_memory</span><span class="p">(</span><span class="k">print</span><span class="p">)</span>
<span class="nf">print_</span><span class="p">(</span><span class="sh">'</span><span class="s">yoo</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">haha</span><span class="sh">'</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="nf">print_</span><span class="p">(</span><span class="sh">'</span><span class="s">whats up</span><span class="sh">'</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="sh">'</span><span class="s">:</span><span class="sh">'</span><span class="p">)</span>
<span class="n">print_</span><span class="p">.</span><span class="n">__closure__</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">cell_contents</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># output
yoo haha 1 2 3
whats up

[(('yoo', 'haha', 1, 2, 3), {}), (('whats up',), {'sep': ':'})]
</code></pre></div></div>

<p>One more thing, our inner function doesn’t return anything, lets add return to it, and it becomes a decorator.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">print_with_memory</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">def</span> <span class="nf">inner</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">data</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span> <span class="c1"># args is a tuple, and kwargs is a dict
</span>        <span class="k">return</span> <span class="nf">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">inner</span>

</code></pre></div></div>

<p>our print_with_memory() function now can take in any function, i.e print, max, min and so on.
We can also do something like this. The whole point of decorators is that we are decorating the function, with additional functionality, i.e adding every parameter passed to that function to data list.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sb">`print = print_with_memory(print)`</span> <span class="c1"># we deocorate with somehting like this
</span></code></pre></div></div>

<h4 id="how-to-use--in-decorator">How to use @ in decorator</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">store_arguments</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">data</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>
        <span class="k">return</span> <span class="nf">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">wrapper</span>
<span class="c1"># or we can do this
</span>
<span class="nd">@store_arguments</span>
<span class="k">def</span> <span class="nf">my_special_function</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">repeat</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">name</span><span class="p">.</span><span class="nf">upper</span><span class="p">()</span> <span class="o">*</span> <span class="n">repeat</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">my_special_function</span><span class="p">(</span><span class="sh">'</span><span class="s">yolo</span><span class="sh">'</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">my_special_function</span><span class="p">(</span><span class="sh">'</span><span class="s">haha</span><span class="sh">'</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">my_special_function</span><span class="p">.</span><span class="n">__closure__</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">cell_contents</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># outputs
YOLOYOLO
HAHAHAHAHAHA
[(('yolo', 2), {}), (('haha', 3), {})]
</code></pre></div></div>]]></content><author><name>cohlem</name></author><summary type="html"><![CDATA[Unpacking over indexing]]></summary></entry><entry><title type="html">Multi Head Latent Attention</title><link href="http://localhost:4000/2025/04/28/Multi-head-latent-attention.html" rel="alternate" type="text/html" title="Multi Head Latent Attention" /><published>2025-04-28T00:00:00+05:45</published><updated>2025-04-28T00:00:00+05:45</updated><id>http://localhost:4000/2025/04/28/Multi-head-latent-attention</id><content type="html" xml:base="http://localhost:4000/2025/04/28/Multi-head-latent-attention.html"><![CDATA[<h2 id="scaled-dot-product-attention">Scaled-dot product Attention</h2>

<h3 id="q1">Q1</h3>

<p>Given the attention equation</p>

\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{(xWq)(xWk)^\top}{\sqrt{d_k}}\right)(xWv)W_O\]

<p>Why don’t we train by combining $WqWk^\top$ and $WvWo$? because mathematically they seem equivalent</p>

\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{x(WqWk^\top)x^\top}{\sqrt{d_k}}\right)x(WvW_O)\]

<p>I initially thought if we could combine those weights, we don’t need to calculate $Q,K,V$ meaning there will be less number of matrix multiplication.</p>

<h4 id="answer">Answer</h4>

<p>We lose the objective of $Q,K,V,O$, they are meant to operate independently. In analogy,
$Q$ represents the notion of “what we have”, $K$ whats available to us and so on. BUT, if we combine them into a giant matrix $WqWk^\top$ during backpropagation weight updates are mixed. We no longer update them separately so we end up loosing their significance.</p>

<h3 id="q2">Q2</h3>

<p>So, if we can’t use them during training, can we still mix those weights during inference now that we aren’t updating weights? since the mathematical equivalence is the same, can we use it to optimize inference performance?</p>

<h4 id="answer-1">Answer</h4>

<p>We decrease the number of matrix multiplication, BUT we end up increasing the actual element wise multiplications inside those matrix multiplication.</p>

<p>We end up decreasing the speed rather than increasing it.</p>

<p>Let’s see this by comparison.</p>

<p><strong>NOTE: Given matrix A with size (2,3) and B with size (3,4), the total number of element wise matrix multiplication between is (2 _ 3 _ 4).</strong></p>

<p>$n$ = number of tokens
$d_{\text{model}}$: embedding dimension
$nh$: number of heads
$hd$: number of head<em>dimension
$d</em>{\text{k}}$: $nh$ x $hd$</p>

<h4 id="case-i-original-attention">CASE I: Original Attention</h4>

<p>Compute $Q = X W_Q$: $\mathcal{O}(n \cdot d_{\text{model}} \cdot d_k)$</p>

<p>Compute $K = X W_K$: $\mathcal{O}(n \cdot d_{\text{model}} \cdot d_k)$</p>

<p>Compute $QK^T$: $\mathcal{O}(n^2 \cdot d_k)$</p>

<h4 id="case-ii-combined">CASE II: Combined</h4>

<p>Compute $X W_{QK}$: $\mathcal{O}(n \cdot d_{\text{model}}^2)$</p>

<p>Compute $(X W_{QK}) X^T$: $\mathcal{O}(n^2 \cdot d_{\text{model}})$</p>

<p>If $d_k \ll d_{\text{model}}$ (e.g., $d_k = 128$, $d_{\text{model}} = 512$):</p>

<p>Original: $\mathcal{O}( n \cdot 512 \cdot 128)+ \mathcal{O}( n \cdot 512 \cdot 128) + \mathcal{O}(n^2 \cdot 128)$</p>

<p>Combined: $\mathcal{O}(n \cdot 512^2) + \mathcal{O}(n^2 \cdot 512)$</p>

<p>As you can see the number of matrix multiplication is 3, but the total elementwise multiplication is very large.</p>

<h2 id="multi-head-latent-attention">Multi-head Latent Attention</h2>

<p>The main reason behind using the variants of <strong>Attention</strong> is that we always to increase our inference speed and we are always bottlenecked by <a href="https://cohlem.github.io/sub-notes/kv-cache-gqa/">KV cache</a> The KV cache needed in original Multi-head attention is $2\cdot nh\cdot hd\cdot l$
for one token, as the tokens get large during inference, the memory needed for storing this case also increases.</p>

<p>Deepseek propose the use of latent dimension to compress the dimension.</p>

<p>As we know $K,V$ both come from the same x i.e $K=xWk$ and $V=xWv$ but the different weights $Wk, Wv$</p>

<p>how about we make an intermediate compressed version of x, from which we can decompress it into K and V, and only store that compressed version of x. This is what they use for multi-head latent attention.</p>

<p>$W_{\text{dkv}}$: compression matrix of size ($d_{\text{model}}$, $d_{\text{c}}$)
$L_{\text{kv}}$= $xW_{\text{dkv}}$ which is the compressed version of x</p>

<p>We decompress $L_{\text{kv}}$ into K, V using $Wuk, Wuv$ i.e</p>

<p>$Q=xWq$</p>

<p>$Kc=L_{\text{kv}} \cdot Wuk$ ($Wuk$ size = ($d_{\text{c}}, nh \cdot hd$))</p>

<p>$Vc=L_{\text{kv}} \cdot Wuv$ ($Wuv$ size = ($d_{\text{c}}, nh \cdot hd$))</p>

<p>Similarly, we can substitute those in our original attention equation</p>

\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{(Q)(Kc)^\top}{\sqrt{d_k}}\right)(Vc)W_O\]

<p>As you can see, we’ve increased the number of matrix multiplication (i.e the computation of $L_{\text{kv}}$= $xW_{\text{dkv}}$), but the total number of elementwise multiplication can be made comparable with the right choice of compression dimension $d_{\text{c}}$</p>

<p>But, our main goal was to reduce the number of KV cache, but if we store only the $L_{\text{kv}}$ only, we still would would need to perform $Kc=L_{\text{kv}} \cdot Wuk$ and $Vc=L_{\text{kv}} \cdot Wuv$ to calculate attention. So whats the point of this compression?</p>

<p>Well there’s a trick to still store only the $L_{\text{kv}}$ and use it without calculating Kc and Vc, we do weight combination like in our Q2 but still end up with less number of elementwise matrix multiplication. The equation above can also we written as</p>

\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{(xWq)(L_{\text{kv}} \cdot Wuk)^\top}{\sqrt{d_k}}\right)(L_{\text{kv}} \cdot Wuv)W_O\]

\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{(x(Wq Wuk^\top) (L_{\text{kv}})^\top}{\sqrt{d_k}}\right)L_{\text{kv}}(WuvW_O)\]

<p>After combining $(Wq Wuk^\top)$ and $(WuvW_O)$ once, we can save $L_{\text{kv}}$ in our cache and then directly multiply with $(Wq Wuk^\top)$ to get the attention, without needing to calculate $Kc$ and $Vc$. Remember the issue we had while combining weights in Q2, this fades away because of the compression dimension because it strictly has to be less than $nh \cdot hd$ i.e ($d_{\text{c}} \ll nh \cdot hd$)</p>

<h3 id="decoupled-rope">Decoupled RoPE</h3>

<p>There are still some parts I feel like I don’t understand completely. But, here’s what I’ve understood till now.</p>

<p>First thing to keep in mind, its the clever weight absorption design and caching only $L_{\text{kv}}$ that helps MLA to retain its fast speed. But, we have yet to apply positional information to our Q and K i.e</p>

\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{R1(xWq)(R2(L_{\text{kv}} \cdot Wuk))^\top}{\sqrt{d_k}}\right)(L_{\text{kv}} \cdot Wuv)W_O\]

<p>In the paper, they say the RoPE matrix gets in the middle but I don’t know what i am missing here cause $({R1(xWq)Wuk^\top)L_{\text{kv}}^\top R2^\top}$ so weights can still be combined? I think there something that I don’t understand here. I’ll correct my understand in future.</p>

<p>lets resume from the point that RoPE matrix gets in the middle. They again design a clever thing here. Add two new matrices $W_{QR}\in \mathbb{R}^{(d,nh \cdot d^R)}$ and $W_{KR} \in \mathbb{R}^{(d,d^R)}$</p>

<p>$Q^{R}=RoPE(xW_{QR})$ and $K^{R}=RoPE(xW_{KR})$</p>

<p>and also cache the $K^R$</p>

<p>add then concatenate the new two matrices to the original Q and K</p>

<p>$Q = [Q, Q^R]$</p>

<p>$K = [K, K^R]$</p>

<p>and then perform our original attention.</p>

\[Q \cdot K^\top = [Q, Q^R] \cdot [K, K^R]^\top\]

\[Q \cdot K^\top = [Q \cdot K + Q^R \cdot K^R]\]

<p>as you can see the original Q and K are still preserved meaning we can still absorb the weights.</p>

<p>Total cached elements will be $K^R$ and $L_{kv}$ so the total saved cache will be $(d^R + dc)l$</p>

<h4 id="question">Question</h4>

<p>Why is second dimension of $W_{KR}$ is $d^R$ and not $(nh \cdot d^R)$
Meaning $d^R$ will be broadcasted across all the heads.</p>

<p>My guess is that they found that keeping only $d^R$ would produce decent result and would also save the cache memory requirement.</p>

<h4 id="references">References</h4>

<p>https://arxiv.org/abs/2405.04434
https://liorsinai.github.io/machine-learning/2025/02/22/mla.html#multi-head-latent-attention</p>]]></content><author><name>cohlem</name></author><category term="architecture" /><summary type="html"><![CDATA[Scaled-dot product Attention]]></summary></entry><entry><title type="html">Lora</title><link href="http://localhost:4000/2025/04/07/LoRA.html" rel="alternate" type="text/html" title="Lora" /><published>2025-04-07T00:00:00+05:45</published><updated>2025-04-07T00:00:00+05:45</updated><id>http://localhost:4000/2025/04/07/LoRA</id><content type="html" xml:base="http://localhost:4000/2025/04/07/LoRA.html"><![CDATA[<h3 id="lora">LoRA</h3>

<p>Main idea is to approximate the change in weights dW by the use of low-rank matrices</p>

<p>Eg: Usually the weight update is done by adding the change in weights dW to the original weight matrix W. dW is obtained through backpropagation, ex if W is 512 x 512 the parameter size of dW is 262,144.</p>

<p>In LoRA, we approximate that dW but by breaking down into two low rank matrices B @ A where B = matrix of size 512 x r and A = matrix of size r x 512,</p>

<p>previously if the forward pass was like this</p>

<p>out = X @ W</p>

<p>we change the forward pass:
out = X @ W + X @ B@A</p>

<p>we freeze all the other parameters (W in this case), and only find gradients for B,A and update only these weights.</p>

<p>First lets implement toy example that approximates sin function, we implement manual backpropagation so that It’ll be easier to understand what gets updated in our LoRA implementation.</p>

<h3 id="paper-findings-only-focus-on-highlighted-sentences">Paper findings (only focus on highlighted sentences)</h3>

<p><img src="lora1.png" alt="lora1" /></p>

<p><img src="lora2.png" alt="lora2" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># import torch
</span><span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Generate synthetic data (sine wave)
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">100</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Convert to PyTorch tensors
</span><span class="n">X_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Define a small MLP
</span><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="bp">True</span><span class="p">),</span>  <span class="c1"># 1 input → 10 hidden
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>    <span class="c1"># 10 hidden → 1 output
</span>        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Initialize model, loss, and optimizer
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">()</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>
<span class="c1"># optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
</span>
<span class="c1"># Training loop
</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

    <span class="c1">#Instead of this forward pass
#     outputs = model(X_tensor)
</span>
    <span class="c1"># Implement this forward to manually do the forward pass
</span>    <span class="n">o1</span> <span class="o">=</span> <span class="n">X_tensor</span> <span class="o">@</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span>
    <span class="n">o2</span> <span class="o">=</span> <span class="n">o1</span> <span class="o">+</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">bias</span>
    <span class="n">o3</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">o2</span><span class="p">)</span> <span class="c1"># relu layer
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="n">o3</span> <span class="o">@</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span>


    <span class="n">diff</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">-</span> <span class="n">y_tensor</span>
    <span class="n">squared_diff</span> <span class="o">=</span> <span class="n">diff</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">o_seven</span> <span class="o">=</span> <span class="n">squared_diff</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">o_seven</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">)</span>

    <span class="c1">#clear all the gradients
</span>    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="n">loss</span><span class="p">,</span> <span class="n">o_seven</span><span class="p">,</span> <span class="n">squared_diff</span><span class="p">,</span> <span class="n">diff</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">o3</span><span class="p">,</span> <span class="n">o2</span><span class="p">,</span> <span class="n">o1</span><span class="p">,</span> <span class="n">X_tensor</span><span class="p">]:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">p</span><span class="p">.</span><span class="nf">retain_grad</span><span class="p">()</span>


    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>


    <span class="c1">### Manual backpropagation
</span>    <span class="n">dL</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">do_seven</span> <span class="o">=</span> <span class="n">dL</span><span class="o">*</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">))</span>
    <span class="n">dsquared_diff</span> <span class="o">=</span> <span class="n">do_seven</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">)</span>
    <span class="n">ddiff</span> <span class="o">=</span> <span class="n">dsquared_diff</span> <span class="o">*</span> <span class="mi">2</span><span class="o">*</span><span class="n">diff</span>
    <span class="n">doutputs</span> <span class="o">=</span> <span class="n">ddiff</span><span class="o">*</span><span class="mi">1</span>

    <span class="n">do3</span> <span class="o">=</span> <span class="n">doutputs</span><span class="o">@</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">weight</span>
    <span class="n">dl2w</span> <span class="o">=</span> <span class="n">o3</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">doutputs</span>
    <span class="n">mo2</span> <span class="o">=</span> <span class="n">o2</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="n">do2</span> <span class="o">=</span> <span class="n">do3</span> <span class="o">*</span> <span class="n">mo2</span>
    <span class="n">do1</span> <span class="o">=</span> <span class="n">do2</span>
    <span class="n">dl0bias</span> <span class="o">=</span> <span class="n">do2</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">dl0w</span> <span class="o">=</span> <span class="n">X_tensor</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">do1</span>


<span class="c1"># #     cmp('dL', dL, loss)
#     cmp('do_seven', do_seven, o_seven)
#     cmp('dsquared_diff', dsquared_diff, squared_diff)
#     cmp('ddiff', ddiff, diff)
#     cmp('doutputs', doutputs, outputs)
#     cmp('do3', do3, o3)
#     cmp('dl2w', dl2w.T, model.layers[2].weight)
#     cmp('do2', do2, o2)
#     cmp('dl0bias', dl0bias, model.layers[0].bias)
#     cmp('do1', do1, o1)
#     cmp('dl0w', dl0w.T, model.layers[0].weight )
</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span>
        <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="n">dl0w</span><span class="p">.</span><span class="n">T</span>
        <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="n">dl0bias</span>
        <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="n">dl2w</span><span class="p">.</span><span class="n">T</span>


<span class="c1">#     optimizer.step()
</span>    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Plot results
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">X_tensor</span><span class="p">).</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">True</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Predicted</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Now, we implement LoRA.
Here we,</p>

<ul>
  <li>construct parameters A, B when B@A the resulting matrix size matches layer 0’s weight matrix</li>
  <li>modify forward pass by adding <code class="language-plaintext highlighter-rouge">lora_w</code></li>
  <li>continue other forward passes as they were previously,</li>
  <li>We don’t find the gradients for weights and don’t update those matrices, which is essentially freezing</li>
  <li>through intermediate gradients find the gradients for B and A and only update those weights.</li>
  <li>that’s it!!!!</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

<span class="c1"># Training loop
</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">100000</span>

<span class="c1"># lets only train only the lora parameters for layer0's weight
</span><span class="n">d</span><span class="p">,</span><span class="n">k</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span>
<span class="n">r</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">d</span><span class="p">,</span><span class="n">r</span><span class="p">)))</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="n">r</span><span class="p">,</span><span class="n">k</span><span class="p">)))</span>
<span class="n">scale</span> <span class="o">=</span> <span class="mi">2</span>


<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>

    <span class="c1"># Implement this forward to manually do the forward pass
</span>    <span class="n">o1</span> <span class="o">=</span> <span class="n">X_tensor</span> <span class="o">@</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span>
    <span class="c1">## add lora part for the layer0's model weight here
</span>
    <span class="n">lora_w</span> <span class="o">=</span> <span class="n">scale</span><span class="o">*</span><span class="n">B</span><span class="nd">@A</span>
    <span class="n">lora_o1</span> <span class="o">=</span> <span class="n">X_tensor</span> <span class="o">@</span> <span class="n">lora_w</span> <span class="c1"># size of B@A should match model.layers[0].weight.T i.e (1,20)
</span>    <span class="n">h</span> <span class="o">=</span> <span class="n">o1</span> <span class="o">+</span> <span class="n">lora_o1</span>

    <span class="n">o2</span> <span class="o">=</span> <span class="n">h</span> <span class="o">+</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">bias</span>
    <span class="n">o3</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">o2</span><span class="p">)</span> <span class="c1"># relu layer
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="n">o3</span> <span class="o">@</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span>


    <span class="n">diff</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">-</span> <span class="n">y_tensor</span>
    <span class="n">squared_diff</span> <span class="o">=</span> <span class="n">diff</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">o_seven</span> <span class="o">=</span> <span class="n">squared_diff</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">o_seven</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">)</span>

    <span class="c1">#Freeze all the model parameters
</span>    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">A</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">B</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="n">loss</span><span class="p">,</span> <span class="n">o_seven</span><span class="p">,</span> <span class="n">squared_diff</span><span class="p">,</span> <span class="n">diff</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">o3</span><span class="p">,</span> <span class="n">o2</span><span class="p">,</span> <span class="n">o1</span><span class="p">,</span> <span class="n">X_tensor</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">lora_o1</span><span class="p">,</span> <span class="n">lora_w</span><span class="p">]:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">p</span><span class="p">.</span><span class="nf">retain_grad</span><span class="p">()</span>


    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>


    <span class="c1">### Manual backpropagation
</span>    <span class="n">dL</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">do_seven</span> <span class="o">=</span> <span class="n">dL</span><span class="o">*</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">))</span>
    <span class="n">dsquared_diff</span> <span class="o">=</span> <span class="n">do_seven</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">)</span>
    <span class="n">ddiff</span> <span class="o">=</span> <span class="n">dsquared_diff</span> <span class="o">*</span> <span class="mi">2</span><span class="o">*</span><span class="n">diff</span>
    <span class="n">doutputs</span> <span class="o">=</span> <span class="n">ddiff</span><span class="o">*</span><span class="mi">1</span>

    <span class="n">do3</span> <span class="o">=</span> <span class="n">doutputs</span><span class="o">@</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">weight</span>
    <span class="c1"># We freeze this weight
#     dl2w = o3.T @ doutputs
</span>
    <span class="n">mo2</span> <span class="o">=</span> <span class="n">o2</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="n">do2</span> <span class="o">=</span> <span class="n">do3</span> <span class="o">*</span> <span class="n">mo2</span>

    <span class="n">dh</span> <span class="o">=</span> <span class="n">do2</span>
    <span class="n">do1</span> <span class="o">=</span> <span class="n">dh</span>
    <span class="n">dlora_o1</span> <span class="o">=</span> <span class="n">dh</span>

    <span class="n">dlora_w</span> <span class="o">=</span> <span class="n">X_tensor</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dlora_o1</span>
    <span class="n">dB</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">dlora_w</span><span class="nd">@A.T</span>
    <span class="n">dA</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">B</span><span class="p">.</span><span class="n">T</span><span class="nd">@dlora_w</span>

    <span class="c1"># And we freeze these weights too
#     dl0bias = do2.sum(0)
#     dl0w = X_tensor.T @ do1
</span>

<span class="c1">#     cmp('dL', dL, loss)
#     cmp('do_seven', do_seven, o_seven)
#     cmp('dsquared_diff', dsquared_diff, squared_diff)
#     cmp('ddiff', ddiff, diff)
#     cmp('doutputs', doutputs, outputs)
#     cmp('do3', do3, o3)
# #     cmp('dl2w', dl2w.T, model.layers[2].weight)
#     cmp('do2', do2, o2)
#     cmp('dh', dh, h)
#     cmp('do1', do1, o1)
#     cmp('dlora_o1', dlora_o1, lora_o1)
#     cmp('dlora_w', dlora_w, lora_w)
#     cmp('dB', dB, B)
#     cmp('dA', dA, A)
</span>



<span class="c1">#     cmp('dl0w', dl0w.T, model.layers[0].weight )
</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span>
        <span class="n">A</span><span class="p">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">dA</span>
        <span class="n">B</span><span class="p">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">dB</span>


<span class="c1">#     optimizer.step()
</span>    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Plot results
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">X_tensor</span><span class="p">).</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">True</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Predicted</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>]]></content><author><name>cohlem</name></author><category term="architecture" /><summary type="html"><![CDATA[LoRA]]></summary></entry><entry><title type="html">Interpretability</title><link href="http://localhost:4000/2025/03/03/interpretability.html" rel="alternate" type="text/html" title="Interpretability" /><published>2025-03-03T00:00:00+05:45</published><updated>2025-03-03T00:00:00+05:45</updated><id>http://localhost:4000/2025/03/03/interpretability</id><content type="html" xml:base="http://localhost:4000/2025/03/03/interpretability.html"><![CDATA[<h3 id="induction-circuits">Induction circuits</h3>

<p><strong>Induction behaviour</strong>
The task of detecting and repeating subsequences in a text by finding some patterns.</p>

<p>For example: If there exist a text containing name “James Bond” and later in the text when the model sees the word “James” it predicts/repeats the word “Bond” because it’s already seen the words “James Bond” and analyzes that “bond” should come after the word “James”. Also called “Strict Induction”</p>

<p><strong>Induction head</strong>
A head which implements the induction behaviour. They allow models to perform in-context learning (ICL).</p>

<h3 id="qk-circuit">QK circuit</h3>

<p>Wq, Wk, Wv be the projection matrices in our attention and Wo be the projection matrix in attention.</p>

<p>Wq,Wk, Wv can be thought of as what we take in(read) from the residual stream and Wo can be though of as what we put into(write) the residual stream. Similar to read/write operation.</p>

<p>QK circuit is represented as combination of
<img src="i1.png" alt="i1" /></p>

<p>and attention scores are calculated as
<img src="i2.png" alt="i2" /></p>

<p>it is similar to Q @ K.T</p>

<p>Conceptually, this matrix tells us <strong>which tokens information is moved to &amp; from</strong> in the residual stream.</p>

<h3 id="ov-circuit">OV circuit</h3>

<p><img src="i3.png" alt="i3" /></p>

<ul>
  <li>Conceptually, this matrix (Vi.Wov) tells us <strong>what information is moved from a token</strong>, if that token is attended to.</li>
</ul>

<p><strong>Some prerequisites</strong></p>

<p><strong>orthogonal subspace</strong>
Important assumption to keep in mind is subspaces are orthogonal i.e their dot product is 0, which means, we can decompose a vector into different subspaces that can have independent meaning because of this assumption.</p>

<p>A simple example is when we add the positional embedding and vector embedding initially,</p>

<p>output = positional embedding + vector embedding</p>

<p>we are simply adding two different informations from different subspaces i.e positional embedding and vector embedding, and if they are orthogonal, we can independently pluck out their embeddings from the output.</p>

<p><strong>K-composition</strong>
In K-composition, the output of one attention at layer 0 is used as a key for another attention calculation in layer1.</p>

<p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1674002595/mirroredImages/TvrfY4c9eaGLeyDkE/dhz5bqfuuoujydh89caa.png" alt="matrix" /></p>

<p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1674002595/mirroredImages/TvrfY4c9eaGLeyDkE/ui0f38ofgdsyr9zzt5ar.png" alt="imag2" /></p>

<p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1674002595/mirroredImages/TvrfY4c9eaGLeyDkE/voncnaecap3tsw8dt0mn.png" alt="image3" /></p>

<p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1674002595/mirroredImages/TvrfY4c9eaGLeyDkE/tuskojx6dtlzjwoqs9zd.png" alt="image3" />
<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1674002595/mirroredImages/TvrfY4c9eaGLeyDkE/eaongteayjgei7no5ycy.png" alt="image5" /></p>

<p>Induction heads are responsible in significant downward loss. It could mainly occur in phase changes, eg. 2B to 4B tokens. Are responsible to ICL but also helps in translation as well as few-shots learning</p>

<h3 id="visualization-of-induction-head">Visualization of induction head</h3>

<p><img src="i4.png" alt="i4" /></p>

<p>the visualization would look like this, for an induction head i.e the destination token attends to token after the previous destination token’s occurance offset by seq_len - 1.</p>

<h3 id="direct-logits-attribution">Direct logits attribution</h3>

<p><img src="i5.png" alt="i5" /></p>

<p>It involves understanding how much each component (e.g., attention heads, layers, or direct paths) contributes to the model’s predictions.</p>

<p>Using attention head pattern is not enough to deduce that the model is actually using that attention information, to finally verify we also have to see their direct logits attribution, i.e how much their RS contribute to the final logits.</p>

<h3 id="ablation">Ablation</h3>

<p>Zeroing activation is not recommended because it completely moves the model’s representation out of distribution. Instead, mean over some batch dimension is recommended.</p>

<h3 id="making-sense-of-induction-circuits-in-terms-of-qk-ov-circuits">Making sense of induction circuits in terms of QK, OV circuits</h3>

<p>source token: the token that is being attended to
destination token: the token that is looking at other tokens</p>

<p>lets say we process a random but repeating sequence.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['&lt;|endoftext|&gt;', 'anguage', ' reign', 'ای', ' aides', ' freight', ' Break', ' compete', ' Ment', '··', ' vaginal', ' burn', '을', 'Connell', ' affiliates', 'Back', 'Trigger', ' blade', 'ų', ' emotion', ' Ash', ' rupture', 'ian', ' pickup', ' Theresa', ' creepy', ' libert', 'uclidean', ' repairs', 'ylvan', 'τή', ' playwright', ' decou', ' deliver', ' Steele', '70', ' employing', 'idopsis', 'hidden', ' Ren', 'UTR', ' morale', ' vacant', ' _(', ' contra', 'ィ', ' vocals', ' reduces', 'Civil', 'ussion', ' deeds', 'anguage', ' reign', 'ای', ' aides', ' freight', ' Break', ' compete', ' Ment', '··', ' vaginal', ' burn', '을', 'Connell', ' affiliates', 'Back', 'Trigger', ' blade', 'ų', ' emotion', ' Ash', ' rupture', 'ian', ' pickup', ' Theresa', ' creepy', ' libert', 'uclidean', ' repairs', 'ylvan', 'τή', ' playwright', ' decou', ' deliver', ' Steele', '70', ' employing', 'idopsis', 'hidden', ' Ren', 'UTR', ' morale', ' vacant', ' _(', ' contra', 'ィ', ' vocals', ' reduces', 'Civil', 'ussion', ' deeds']
</code></pre></div></div>

<p>We pass it through our 2 layer attention-only model</p>

<p>we get attention pattern like this for layer 0 <img src="i6.png" alt="i6" />
here 7 is a <strong>previous token head</strong></p>

<p>and we get this attention pattern for layer 1
<img src="i7.png" alt="i7" /></p>

<p>4 and 7 are induction heads, cause we see the activations for seq_len -1 diagonal</p>

<p>So what we can conclude from this is that OK circuit from attention head no 7 in layer 0 always attends to the previous token, OV circuit from the same head writes the previous token information in the embedding subspace.</p>

<p>In layer 1, attention 4,10, K-composition is used i.e the output of attention head 7 is used as the key in these attentions, the QK circuit looks at the source token whose previous token is the destination token (i.e current token) and OV circuits writes this information in the embedding subspace (i.e info about which tokens come next).</p>

<p><strong>NOTE</strong> decomposition trick only works for things that are linear, it would work if there are no activation functions eg, ReLU, GeLU. or would only work before softmax, rather than after softmax.</p>

<h2 id="a-mathematical-framework-for-transformer-circuits">A Mathematical Framework for Transformer Circuits</h2>

<p><strong>Key takeaways</strong></p>

<ul>
  <li>Residual stream (RS) can be thought of sum of changes added by each layer (embedding, attention, MLP)</li>
  <li>The residual stream has a deeply linear structure.</li>
  <li>dimensions of the residual stream become something like “memory” or “bandwidth”</li>
  <li>Why do we decompose residual stream and study them independently? because they try to communicate in superposition. Meaning, a residual stream only has limited dimensions eg 512 and at MLP it has to interact with a lot more dimensions i.e 4Xmore dimension than it has space on it. So its better to study them independently. So the RS has very high demand, is the reason why some info gets deleted and some added by some layers.</li>
  <li>Attention heads copy information from the residual stream of one token to the residual stream of another. They typically write
to a different subspace than they read from.
-</li>
</ul>

<h3 id="paper">Paper</h3>

<p>implement this formula to find which component/layer are important <img src="i8.png" alt="i8" /></p>

<p>to find how good they are (faithful) use this equation</p>

<p><img src="i9.png" alt="i9" /></p>

<p>IDENTIFYING ANSWER-PROMOTING COMPONENTS
train linear probing for each layer, at each position p, find which which layer at which position promote the correct-answer</p>

<p>DECOMPOSING CIRCUIT MLPS TO INDIVIDUAL NEURONS</p>

<p>Go more deep into the MLP to see which neurons promote arithmetic calculation</p>

<p>do activation patching on neurons.</p>

<p>they find &gt; neurons with the highest effect are different between operators.</p>

<p>only 200 neurons (roughly 1.5%) per layer are needed to achieve high faithfulness and correctly compute arithmetic prompts.</p>

<p>among top 200 they examine neurons, by hypothesizing that 1. their key denotes the activation patterns, and 2. their values encodes the correct output in these dimension.</p>

<p>Approach</p>

<p>Our Hypothesis: Circuits for Math Reasoning only work for similar problems (i.e the patterns/distributions LLMs encoded in their weights during their training) and circuits fail when they receives problems that require calculation which is out-of-distribution.</p>

<p>The hypothesis is to basically prove why LLMs are not robust in Math Reasoning.</p>

<p>Steps to prove:</p>

<ol>
  <li>Find circuits of LMs responsible for Math Reasoning/Calculations for correct output ( i.e we find circuit for problems which LMs can solve and ignore the problems on which they fail) using patching. (calculating indirect effect (IE) as done in this paper https://arxiv.org/pdf/2305.15054)</li>
  <li>We can go further and find specific neurons responsible for calculations by finding the top-k tokens for specific neurons and patching/ablating them to find if they’re really important. However, I believe we can limit experiment to til finding the circuits as mentioned in step 1.</li>
  <li>Generate adversarial math problems (i.e out of distribution problems ). We can simply opt for M2 level problems as done in this paper (https://arxiv.org/pdf/2402.17916)</li>
  <li>Test those adversarial problems on LMs, filter and keep only the problems on which LMs produce incorrect output and study their whole output ( which should also include their step-by-step reasoning)</li>
</ol>

<p>for example:</p>

<p><strong>Prompt</strong>:</p>

<p>Chenny is 64 years old. Alyana is 54 years younger than Chenny. How old is Anne if she is 55 years older than Alyana?</p>

<p>LM output:</p>

<p>To determine Anne’s age, we need to follow these steps:</p>

<ol>
  <li>Calculate Alyana’s age.</li>
  <li>Use Alyana’s age to find Anne’s age.</li>
</ol>

<p><strong>Step 1: Calculate Alyana’s age.</strong> Chenny is 64 years old. Alyana is 54 years younger than Chenny. So, Alyana’s age is: [ 64 - 54 = 100 ]</p>

<p><strong>Step 2: Calculate Anne’s age.</strong> Anne is 55 years older than Alyana. So, Anne’s age is: [ 100 + 55 = 155 ] Thus, Anne is (\boxed{155}) years old.</p>

<ol>
  <li>Analyze the point where there is error in calculation, i.e there was error in <code class="language-plaintext highlighter-rouge">\[ 64 - 54 = 100 \]</code> in the LM’s output above.</li>
  <li>We hypothesize here that the mistake in calculation was because the circuits lacks patterns/ heuristics to solve the problem. The concept about patterns/heuristics can be found in this recent paper (https://arxiv.org/pdf/2410.21272). To elaborate, while training it was able to encode this pattern/ heuristics that 5-4 = 1 in their weights, but it cant perform 65 - 54 because it lacks those patterns/heuristics.</li>
  <li>We can prove our hypothesis to be true here, if we can supply it with the necessary pattern/heuristics. To do so,</li>
  <li>We run the LMs with our prompt new prompt.</li>
</ol>

<p>PROMPT:</p>

<p>Chenny is 64 years old. Alyana is 54 years younger than Chenny. How old is Anne if she is 55 years older than Alyana?</p>

<p>To determine Anne’s age, we need to follow these steps:</p>

<ol>
  <li>Calculate Alyana’s age.</li>
  <li>Use Alyana’s age to find Anne’s age.
<strong>Step 1: Calculate Alyana’s age.</strong> Chenny is 64 years old. Alyana is 54 years younger than Chenny. So, Alyana’s age is: [ 64 - 54 = 10</li>
</ol>

<p>as you can see our new prompt is <strong>previous prompt + the LM’s output til the point where LMs get calculations wrong</strong>
but we manually correct the calculation, example 64 - 55 = 100 was corrected to 64 - 55 = 10,</p>

<p>and we get the activations for this new prompt by only generating the activations for only 1 new sequence (meaning we don’t let it complete the whole sequence, the point is to just get activations till the point where it gets wrong) , and then patch the circuit activations from this new corrected run to the old run where it got it wrong.</p>

<p>Now, if the old run is able to get the answer right after the patching process, it’s very likely that it lacked the necessary pattern/heuristics for solving that problem.</p>

<p>The bottleneck/time consuming thing would be manually identifying the point in the LM’s output where it has error and correcting it.</p>

<p>logit difference</p>

<p>The metric also is a mostly linear function of the residual stream (unlike probability-based metrics) which makes it easy to directly attribute logit difference to individual components (“direct logit attribution”, “logit lens”).</p>

<p>Probabilities: This metric measures the probability of the right answer, or the difference in probabilities of different answers. The main issue with such metrics is Probabilities are non-linear, in the sense that they track the logits ~exponentially. For example, a model component adding +2 to a given logit can create a 1 or 40 percentage point probability increase, just depending on what the baseline was. As an example of the non-linearity consider the orange line in the figure above: A modest increase in logit difference around layer 17 converts to a jump in probability. Probabilities also inherit the problems of the logprob metric, namely saturation and unspecificity. The figure shows the saturation effect for the orange line at layer &gt;18.</p>

<p>TLDR: probability is exponential we can’t attribute model component’s work to prob change completely.</p>

<p>I would like to tell you the update about the work that did in past 2 weeks.</p>

<p>In the first week, I was involved in reading the paper that you sent me. I’ve attached slides for those paper which is attached in this email. Afterwards, I did some experimentation of the basics of Mechanistic Interpretability such as probing, logits lens and ablation using TransformerLens library, then I studied how induction, QK and OV circuits work, and some detail about K composition.</p>

<p>In the second week, I started experimentation by looking into neurons to see if I can find specific neurons that are responsible for math reasoning. I used few samples from GSM8K dataset. I was able to find specific neurons responsible for</p>

<h3 id="direct-effect">Direct effect</h3>

<p>measuring how patching/ablation affects the logits directly</p>

<h3 id="indirect-effect">Indirect effect</h3>

<p>measuring how patching/ablation affects the logits indirectly, for example: patching one attention layer might not contribute directly to logits but may interact with further attention layer, it kind of a chain effect. Hard to measure.</p>

<h3 id="logit-difference">Logit difference</h3>

<p>logit differnce is easier to understand</p>

<p>logit difference == logprob difference</p>

<p><img src="i10.png" alt="i10" /></p>

<h3 id="things-to-check">Things to check</h3>

<ul>
  <li>Compute the likelihood of the input “22 + 78 =” under the model’s probability distribution.</li>
  <li>Compare this likelihood to the likelihood of known in-distribution examples.</li>
  <li>If the likelihood is significantly lower, the input might be OOD.</li>
</ul>

<p>We can also check if the number is deleted by ffn or by the composition.</p>

<ul>
  <li>See all the logit lens of residual network and also the logit lens of ffn and see if it deletes, See ffns are kv memories paper in the end.</li>
</ul>

<h4 id="references">References</h4>

<p>https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_Jzi6YHRHKP1JziwdE02qdYZ
https://www.lesswrong.com/posts/TvrfY4c9eaGLeyDkE/induction-heads-illustrated</p>

<p>best practices for patching https://arxiv.org/pdf/2309.16042</p>]]></content><author><name>cohlem</name></author><category term="interpretability" /><summary type="html"><![CDATA[Induction circuits]]></summary></entry><entry><title type="html">Rlhf</title><link href="http://localhost:4000/2025/02/24/RLHF.html" rel="alternate" type="text/html" title="Rlhf" /><published>2025-02-24T00:00:00+05:45</published><updated>2025-02-24T00:00:00+05:45</updated><id>http://localhost:4000/2025/02/24/RLHF</id><content type="html" xml:base="http://localhost:4000/2025/02/24/RLHF.html"><![CDATA[<p>Before starting, it’s advisable to first complete <a href="https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-">David Silver’s Course on RL</a> and read <a href="https://lilianweng.github.io/posts/2018-02-19-rl-overview/#deadly-triad-issue">Lilian’s notes on RL</a> which explains/provides notes on the David’s course in sequential manner.</p>

<p>In simple problems, we simply start with an arbitrary value function, and then go on updating that value function incrementally, using different algorithms such as Monte Carlo (which collects reward over the whole episoe), Temporal difference, aka TD(0) (which considers bootstrapping, i.e only considering the immediate reward and then approximating other remaining rewards with the help of value function $r + V(s)$ ) and other algorithms. But, over time maintaining a table is not feasible for real world problems, so move towards approximating these value functions with the help of a vector of features or more concretely with Neural Networks but approximating these state and action value functions are infeasible too which is explained in the next section.</p>

<h3 id="why-we-choose-policy-approximation-instead-of-stateaction-value-approximation">Why we choose policy approximation instead of state/action value approximation.</h3>

<p>In real work tasks, the action space is not discrete, i.e up, down, right, left instead it’s continuous for eg. A robot can do infinite tasks. Deep Q Network works by taking the maximum over the action, and minimizing the error over that action. BUT, approximating every action becomes expensive task.</p>

<p>\(Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma \max_{a \in \mathcal{A}} Q(S_{t+1}, a) - Q(S_t, A_t))\)
So we move on to approximating our policy, where the approximated policy will provide the probability over the considerable actions, allowing the agent to <strong>sample actions</strong> rather than search for the best one.</p>

<h2 id="policy-gradient-algorithms">Policy Gradient Algorithms</h2>

<p>Proof of policy gradient theorem.</p>

<p>First we collect trajectory $\tau$, which is a collection of states and actions. $\tau = s_0, a_0, s_1, a_1, \ldots, s_H, a_H, s_{H+1}$</p>

<p>The reward for this trajectory is simply the sum of all the rewards from each timesteps.</p>

\[R(\tau) = \sum_{t=0}^{H-1} R(s_t, a_t)\]

<p>Our overall objective is to construct a policy that gives us the highest reward for our trajectory. We construct a function $J(\theta)$ that is parametrized by this policy, that calculates the total expected reward and we want to maximize this expectation.</p>

\[J(\theta) = E[R(\tau); \theta] \tag{1}\]

\[\max_\theta J(\theta) = \max_\theta E\left[\sum_{t=0}^{H-1} R(s_t, a_t)\right]\]

<p>If we open this objective function it becomes.</p>

\[J(\theta) = \sum_\tau P(\tau) \cdot R(\tau)\]

<p>where $P(\tau)$ is the probability of trajectory $\tau$, and $R(\tau)$ is the total reward for that trajectory. and Lets take the derivative of this $J(\theta)$</p>

\[\nabla_\theta J(\theta) = \sum_\tau \frac{P(\tau)}{P(\tau)} \nabla_\theta P(\tau) \sum_{t=0}^{H-1} R(s_t, a_t)\]

\[\nabla_\theta J(\theta) = \sum_\tau P(\tau) \cdot \nabla_\theta \log P(\tau) \cdot \sum_{t=0}^H R(s_t, a_t) \quad \text{(using } \frac{\partial \log P(\tau)}{\partial \theta} = \frac{1}{P(\tau)} \partial_\theta P(\tau) \text{)}\]

<p>now representing it back into the expectation form we get,</p>

\[\nabla_\theta J(\theta)
= E\left[  \nabla_\theta \log P(\tau) \cdot \sum_{t=0}^{H-1} R(s_t, a_t) \right] \tag{2}\]

<p>Now what is $P(\tau)$, it’s the joint probability for that trajectory, which is given by the chain rule of probability,</p>

\[P(\tau) = P(s_0) \cdot P(a_0|s_0) \cdot P(s_1|a_0, s_0) \cdot P(a_1|s_1) \cdot P(s_2|a_1, s_1) \cdots\]

<p>In the equation above, we assume Markov’s property that next action only depends on the current state.</p>

\[P(a_t|s_t, s_{t-1}) = P(a_t|s_t)\]

\[P(s_{t+1}|a_t, s_t, a_{t-1}, s_{t-1}) = P(s_{t+1}|a_t, s_t)\]

<p>It boils down to this equation below.</p>

\[P(\tau) = P(s_0) \prod_{t=0}^H P(a_t|s_t) \cdot P(s_{t+1}|a_t, s_t)\]

<p>and then to this</p>

\[P(\tau) = P(s_0) \prod_{t=0}^H \pi(a_t|s_t) \cdot P(s_{t+1}|a_t, s_t)\]

<table>
  <tbody>
    <tr>
      <td>because action distribution $\pi(a_t\ s_t)$ is governed by the policy $\theta$ whereas the transition probability $P(s_{t+1}</td>
      <td>a_t, s_t)$ is governed by the environment. Let’s take the log of that $P(\tau)$</td>
    </tr>
  </tbody>
</table>

\[\log P(\tau) = \log(P(s_0)) + \sum_{t=0}^{H-1} \log \pi(a_t|s_t) + \sum_{t=0}^{H-1}\log P(s_{t+1}|a_t, s_t)\]

<p>After taking the derivative of that $\log P(\tau)$, we end up with this equation below, because all the other terms down depend on our policy $\theta$</p>

\[\nabla_\theta \log P(\tau) = \sum_{t=1}^H \nabla_\theta \log \pi(a_t|s_t)\]

<p>Let’s replace the equation above in our equation 2. we get.</p>

\[\nabla_\theta J(\theta) = \frac{1}{M} \sum_{m=1}^{M} \left( \sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m |s_t^m) \cdot \left( \sum_{t=0}^{H-1} R(s_k^m, a_k^m)  \right) \right) \tag{3}\]

<h4 id="reducing-variance">Reducing Variance</h4>

<p>BUT, there’s a problem withe equation (3), it is high variance, because in stochastic environment R can sometimes take high value, and some times low. Since R acts as a magnitude term in the equation (3) it would make the gradient estimation and update unstable because of the high variance, to stabilize it we subtract a baseline from the the return in equation (3).</p>

\[\nabla_\theta J(\theta) = \frac{1}{M} \sum_{m=1}^{M} \left( \sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m |s_t^m)  \cdot \left( \sum_{t=0}^{H-1} R(s_k^m, a_k^m)  - b\right) \right) \tag{3}\]

<p>subtracting b from the trajectory reward doesn’t increase it’s bias and there is a proof of that which I’m not going to explain in this section, it helps us by decreasing the variance. If we’re able to keep the bias stable and variance low, it would be a win win situation and this is what we do here.</p>

<p>What should the b value be?</p>

<p>It would be better if we have that baseline as an average return that we might expect when we are in state t, and that is simply our value function $V^\pi(s)$.</p>

\[\nabla_\theta J(\theta) = \frac{1}{M} \sum_{m=1}^{M} \left( \sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m |s_t^m)  \cdot \left( \sum_{t=0}^{H-1} R(s_k^m, a_k^m)  - V^\pi(s_k^m)\right) \right) \tag{4}\]

<p>Equation (4) tell us we are increasing the logprobs of actions that gives us the return that is better than the average.</p>

<p>There still one way we can reduce further variance from equation (4), we can break down equation (4) into the equation below.</p>

\[\nabla_\theta J(\theta) = \frac{1}{M} \sum_{m=1}^{M} \left( \sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m |s_t^m)  \cdot \left( \sum_{t=0}^{t-1}R(s_k^m, a_k^m) + \sum_{k=t}^{H-1} R(s_k^m, a_k^m)  - V^\pi(s_k^m)\right) \right) \tag{5}\]

\[\nabla_\theta J(\theta) = \frac{1}{M} \sum_{m=1}^{M} \left( \sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m |s_t^m)  \cdot \left(  \sum_{k=t}^{H-1} R(s_k^m, a_k^m)  - V^\pi(s_k^m)\right) \right) \tag{6}\]

<p>we can remove this term $\sum_{t=0}^{t-1}R(s_k^m, a_k^m)$, because why would the rewards at time step 4 affect the logprobs of action at time step 7. We only care about the FUTURE rewards not the past rewards, we are essentially deleting the past rewards from there. For concrete understanding, let’s say we have H = 3, M=1</p>

<p>t=0
$\nabla_\theta \log(a_0|s_0) = 0.5$
$R(s_0,a_0) = 10$</p>

<p>t=1
$\nabla_\theta \log(a_1|s_1) = 0.4$
$R(s_1,a_1) = 4$</p>

<p>t=2
$\nabla_\theta \log(a_2|s_2) = 0.2$
$R(s_2,a_2) = 7$</p>

<p>So,</p>

<table>
  <tbody>
    <tr>
      <td>$\sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m</td>
      <td>s_t^m) = (0.5 + 0.4 + 0.2)$</td>
    </tr>
  </tbody>
</table>

<p>$\sum_{t=0}^{H-1} R(s_k^m, a_k^m) = (10 + 4 + 7)$</p>

<p>Let’s add those values in the equation 4.</p>

<p>$(0.5 + 0.4 + 0.2) \cdot (10 + 4 + 7) = (0.5<em>(10 + 4 + 7) + 0.4</em> (10 + 4 + 7) + 0.2 * (10 + 4 + 7))$</p>

<p>as you can see we are including rewards at past timestep for future actions too (i.e 0.4 is the logprobs for timestep 1 and it is multiplied with (10 + 4 + 7), where 10 is the reward from the past timestep 0, we don’t want that) so, if we use the equation (6), we end up with something like this</p>

<p>$(0.5<em>(10 + 4 + 7) + 0.4</em> (4 + 7) + 0.2 * (7))$</p>

<p>We only consider the future rewards, not the past ones.</p>

<h4 id="how-to-approximate-value-function">How to approximate value function?</h4>

<p>We can approximate value function $V^\pi$ as taught in this <a href="https://www.youtube.com/watch?v=UoPei5o4fps&amp;list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&amp;index=6">lecture</a> we can simply sample from the trajectory and then minimize the squared error i.e</p>

<p>let’s sample $M$ trajectories, $\tau_1, \tau_2, … \tau_M$ and then minimize this squared error</p>

\[\frac{1}{M} \sum_{m=1}^{M} \sum_{t=0}^{H-1} \left( V^\pi(s_t^m) - \sum_{k=t}^{H-1} R(s_k^m, a_k^m) \right)^2\]

<p>There’s one more thing we can do, we can approximate this term $\sum_{k=t}^{H-1} R(s_k^m, a_k^m)$ with our Q value function which would further reduce the variance.</p>

<p>Since $Q^\pi= E[r_0 + \gamma r_1 + \gamma^2 r_2 …]$
adding gamma would reduce the variance further, and approximating this $Q^\pi$ with the help of value function would further reduce the variance.</p>

\[\nabla_\theta J(\theta) = \frac{1}{M} \sum_{m=1}^{M}  \sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m |s_t^m)  \cdot \left( Q^\pi (s_t^m, a_t^m)  - V^\pi(s_t^m)\right) ) \tag{7}\]

\[\nabla_\theta J(\theta) = \frac{1}{M} \sum_{m=1}^{M}  \sum_{t=0}^{H-1} \nabla_\theta \log \pi( a_t^m |s_t^m)  \cdot A^\pi(s_t, a_t) ) \tag{8}\]

<p>We can approximate this using bootstrapping, $A^\pi(s,a)$ in different ways such as.</p>

\[A^\pi(s_t,a_t)= r_0 + V^\pi (s_{t+1}) - V^\pi(s_t))\]

<p>Or more compact form called Generalized Advantaged Estimation (GAE)</p>

\[\delta_t = r_t + \gamma V(s_{t+1} - V(s_t))\]

\[A_t(s_t, a_t) = \sum_{l=0}^\infty \gamma \lambda \delta_{t + l}\]

<h3 id="general-rl-setting">General RL setting</h3>

\[J(\pi) =
  \mathbb{E}\_{\tau \sim \pi} \left[ \sum\_{t=0}^{\infty} \gamma^t r(s\_t,
  a\_t) \right],\qquad{(1)}\]

<p>In RL setting we aim to optimize the objective function $J(\pi)$ by updating the policy $\pi$
given a reward function $r(s,a)$ that takes in a state and the action performed at that state. The next action is determined by $\pi(a|s)$. We find the expected(average) reward over all the trajectories $\tau$. $\gamma$ is the discount factor from 0 to 1 that balances the desirability of of near vs future-rewards.</p>

<p>An example: If our RL setup is confined to finding a maze, a reward function could be $r(s,a)=-\text{distance to goal}$</p>

<h3 id="rlhf">RLHF</h3>

<p>RLHF reduces the equation 1 by removing this discounted factor</p>

\[J(\pi) =
  \mathbb{E}\_{\tau \sim \pi} \left[ \sum\_{t=0}^{\infty} r(s\_t,
  a\_t) \right],\qquad{(2)}\]

<p>We aim to maximize our objective function by optimizing the policy. The reward function is designed such that the actions must align with the human preferences.</p>

<p>The most common reward model predicts the probability that a piece of text was close to a “preferred” piece of text from the training comparisons.</p>

<h3 id="reward-models">Reward Models</h3>

<p>Given two prompts $y1$ and $y2$ we want to have a reward model that gives high score to $y1$ and low score to $y2$ meaning $y1$ is always preferred over $y2$. Their relative preference is given by the Bradly Terry model.</p>

\[P(i &gt; j) =
  \frac{p\_i}{p\_i + p\_j}\qquad{(3)}\]

<p>It gives the probability of i being preferred over j where $p_{i}$ and $p_{j}$ represent “strengths” for those prompts.</p>

<p>We want our model to maximize this probability because later $i$ would represent the text we want (aligned) and $j$ would represent the text we don’t want (not aligned). The training objective can be derived from the equation above. The “strengths” are exponential because we want them to be strictly positive.</p>

\[P(y\_1 &gt;
  y\_2) = \frac{\exp(r(y\_1))}{\exp(r(y\_1)) +
  \exp(r(y\_2))}\qquad{(4)}\]

<p>The loss function becomes</p>

\[\mathcal{L}(\theta) = - \log \left( \sigma
  \left( r\_{\theta}(x, y\_w) - r\_{\theta}(x, y\_l) \right)
  \right)\qquad{(6)}\]

<p>Our existing model can be configured to output just one value by adding a linear head to the model.</p>

<p>In case of language models, we want a model to rate our answer i.e (give a score based on how good or bad it is), it is the most important part because it will guide our training, it is providing supervision for our PPO algorithm.</p>

<h4 id="steps-for-training-a-reward-model">Steps for training a reward model</h4>

<p>Collect pairs of data, it could be either contrasting pairs or some pairs of prompt with priority. Eg. We want our llm to be trained (later using PPO) to be to generate positive response. In this case our priority prompt would be positive prompt and non-priority prompt would be negative prompt.</p>

<p>We take a language model and add a linear head to it. For instance, for each token a LM outputs 512 dimension vector we add a new head that takes in 512 dimension and outputs a one dimensional vector which gives us the reward.</p>

<p>The loss function for the reward model is constructed likewise, which is same the equation 6.</p>

\[L = -log(\sigma(r1 - r2))\]

<p>where $r1$ is reward for priority prompt and $r2$ is reward for non-priority prompt. We want to maximize this difference $r1 - r2$ and minimize the this function. $\sigma$ represents sigmoid function.</p>

<h3 id="notes">NOTES</h3>

<ul>
  <li>we calculate the reward for the ending token which represents a reward given to the whole token.</li>
  <li>reward models overfit fast, so we can consider smaller LM for reward model training.</li>
</ul>

<p>——-<strong>Skipping other reward models for now</strong>——-</p>

<h3 id="regularization">Regularization</h3>

<p>We want aligned reward models but would still “not go off the rails” meaning it stays within the limitation of our reference model. It allows the policy being trained to stay close to the reference policy.</p>

\[r = r\_\theta -
  \lambda r\_{\text{reg.}} \qquad{(1)}\]

\[r = r\_\theta - \lambda\_{\text{KL}} \mathcal{D}\_{\text{KL}} \left(
  \pi^{\text{RL}}(y \mid x) \, \| \, \pi^{\text{Ref.}}(y \mid x) \right)
  \qquad{(2)}\]

<p>KL divergence is calculated as</p>

\[D\_{\text{KL}}(P \,||\, Q) = \mathbb{E}\_{x \sim P} \left[ \log P(x) -
  \log Q(x) \right].
  \qquad{(3)}\]

<h3 id="rejection-sampling">Rejection Sampling</h3>

<p>This process is kind of a filtering process. We first sample outputs from our base language model that we’ll be training next. For example, we generate 5 example outputs for a sample prompt. We then provide a score to each of the generated examples using our reward function. We sort and only take the top-K elements per prompt and fine-tune our base model on these examples. So it’s kind of like a filtering and only keeping the highly rewarded examples.</p>

<h3 id="ppo">PPO</h3>

<h4 id="ppo-algorithm-analogy">PPO algorithm analogy</h4>

<p>Suppose we are taking an exam paper. Our objective is to maximize the change of getting good marks by updating our brain (weights and biases).</p>

<p>$s_t$ is a question that we are looking at and trying to solve. $a_t$ is the our answer for that question. $r_t$ is the immediate reward we get after writing $a_t$, $s_{t+1}$ is the next-question.</p>

<p>$R_t$ is the actual total exam score.</p>

<p>Suppose there is an imaginary machine that gives us the expected exam score that we can get just by looking at our question which is $V(s)$.</p>

<p>$A(s)$ is our advantage i.e how good we are compared to the predicted score.</p>

<p>$A(s) = R_t - V(s)$</p>

<p>this can be modified as as $\delta_t = R_t - V(s)$ and $A_t = \delta_t + \lambda\cdot\gamma\cdot A_{t+1}$. This is just a modified version of advantage to remove the bias and variance.</p>

<p>This can be a little confusing as we might have no idea what our actual total score($R_t$) will be while we are still writing some questions $s$ so approximate this $R_t$ with the help of current reward $r$ and future reward that we might get from next question i.e $V(s+1)$ i.e</p>

<p>this becomes</p>

<p>$\delta_t =  r_t + V(s+1) - V(s)$</p>

<p>so this will still give us our advantage at a point t.</p>

<p>i.e how good/bad we did at point t= immediate reward(marks) after writing answer to t + expected future reward from new question - expected future reward from previous question t.</p>

<p>This will give us our advantage.</p>

<p>PPO is done using two phases.</p>

<ol>
  <li>Rollout phase</li>
  <li>Weight update phase.</li>
</ol>

<h4 id="rollout-phase">Rollout phase</h4>

<p>We write a lot of exam paper in this phase in parallel.
for each exam paper and for each question in the exam paper we calculate $r_t$, $V(s_t)$ and $R_t$ $A_t$ and use it in our equation.</p>

\[L^{PPO}(\theta) = \mathbb{E}\_t \left[
\min \left( r\_t(\theta) \hat{A}\_t, \text{clip}\left(r\_t(\theta), 1 - \epsilon,1+\epsilon\right) \hat{A}\_t \right) - c\_1 \left( V\_\theta(s\_t) -V\_t^\text{target} \right)^2 + c\_2 \mathcal{H}\left[\pi\_\theta\right(s\_t)\right]\]

\[r\_t(\theta) = \frac{\pi\_\theta(a\_t | s\_t)}{\pi\_{\theta\_\text{old}}(a\_t|s\_t)}\]

<h4 id="weight-update-phase">Weight update phase</h4>

<p>We find this loss and try to maximize this clipped loss and entropy loss, but minimize the value function loss.</p>

<p>The PPO clipped surrogate objective is given as:</p>

\[L^{\text{CLIP}}(\theta) = \mathbb{E}\_t \Big[ \min \big( r\_t(\theta) \hat{A}\_t, \; \mathrm{clip}(r\_t(\theta), 1-\varepsilon, 1+\varepsilon) \hat{A}\_t \big) \Big]\]

<p>The gradient ascent update rule is:</p>

\[\theta \gets \theta + \alpha \nabla\_\theta L^{\text{CLIP}}(\theta)\]

<p>The most important part to understand here is</p>

<p>$r_t(\theta) \hat{A}_t$</p>

<p>so when this term gets clipped to either $1 + \varepsilon$ or $1-\varepsilon$ the gradient of this loss $\nabla_{\theta}r_t(\theta) \hat{A}_t$ is 0. So no update to the weights. But when this gradient is $\nabla_{\theta}r_t(\theta) \hat{A}_t$ the update depends on whether $\hat{A}_t$ is &gt;0 or &lt;0,</p>

<p>If $\hat{A}_t &gt; 0$, the gradient update will be in the direction that increases ${\pi_{\theta}}$.
If $\hat{A}_t &gt; 0$, the gradient update will be in the direction that decreases ${\pi_{\theta}}$.</p>

<h3 id="integrating-reward-model-to-ppo">Integrating reward model to PPO</h3>

<p>As you might have noticed, the PPO loss comes from the token level, meaning we need loprobs, value, reward, return and advantage for each token. Buuuuut, this reward function we just trained is trained to output only one reward for last token so how does that work?</p>

<p>Answer: the reward is propagated.</p>

<p>The advantage is calculated reverse recursively i.e advantage at token n is also passed to the n-1 token. This means that we are looking ahead and telling our token n-1 that we already ended up a good state because of the state we are in. Lets look at this from the lens of advantage formula.</p>

<p>$\delta_t = R_t - V(s)$ and $A_t = \delta_t + \lambda\cdot\gamma\cdot A_{t+1}$</p>

<p>Lets consider we are at 100th token which is the ending token of our sequence</p>

<p>\(R\_t = r\_t + V\_{t+1}(s) - V\_t(s)\)
reward model assigns $r_{100}$ 100 and lets ignore both the value function assuming they cancel out as we are already at the end.</p>

\[\begin{gather}
R\_t= 100\\\\A\_{100}=100, considering A\_{101}=0
\end{gather}\]

<p>At 100th token we are at an advantage, now lets calculate $A_{99}$</p>

\[A\_{99} = \delta\_{99} + \lambda\cdot\gamma\cdot A\_{100}\]

<p>as you can see the reward 100 is propagated to the 99th token, considering $\delta_{99}$ is positive here, it tells us that token 99 is still at an advantage because the we can already see the future here i.e 100th token which was at an advantage, so taking action 99 is still an advantage to us.</p>

<h3 id="reward-hacking">Reward Hacking</h3>

<p>our models can find a loophole to maximize their return by generating high reward tokens but no-so-good answer. Example: If we are trying to make our model positive, model may find a way to output tokens such as “thank you” and add it our answer, which will provide a high reward to it, but it is meaningless to us. So we don’t want our new (trained via PPO) model to deviate significantly from the model that we started from (SFT model), so we add KL divergence as a penalty to our each token’s reward.</p>

<p>As described earlier, reward is provided to only the ending token, and the reward for other token becomes this KL penalty.</p>

<p>i.e if we have token_1, token_2, and token_3</p>

<p>r(token_3) = some reward from the reward model
r(token_2) = KL penalty i.e (logprobs_for_token_2_from_model_being_trained - logprobs_for_token_2_from_SFT_model)* -KL_penalty_coefficient.
and so on…</p>

<p>Some key points</p>

<ul>
  <li>Human preferences that are used to train LLMs are multi-dimensional but the reward is a single score. LLMs being complex and “intelligent” will always find a way to exploit these rewards thus the reward hacking, so in large scale RLHF, reward models get saturated very fast, and we might need to train a new reward model.</li>
</ul>

<h3 id="grpo">GRPO</h3>

<p>The per token loss for GRPO is likewise..</p>

<p>Key difference between GRPO and PPO.</p>

<p>GRPO completely removes values function.
as value function is removed the advantage calculate is simplified.</p>

\[A\_i = \frac{r\_i - \text{mean}(\{r\_1, r\_2, \cdots, r\_G\})}{\text{std}(\{r\_1,r\_2, \cdots,r\_G\})}\]

<p>For each question/prompt different G samples are generated, and for each advantage for each question the reward is normalized to form the advantage.</p>

<p>Previously, in PPO we added KL penalty to the rewards themselves, but in GRPO we add it to the loss function directly.</p>

\[J(\theta) =
  \frac{1}{G}\sum\_{i=1}^G  \frac{1}{|a\_i|} \sum\_{t=1}^{|a\_i|} \left(
  \min\left(\frac{\pi\_\theta(a\_{i,t}|s\_{i,t})}{\pi\_{\theta\_{old}}(a\_{i,t}|s\_{i,t})}A\_{i,t},
  \text{clip} \left(
  \frac{\pi\_\theta(a\_{i,t}|s\_{i,t})}{\pi\_{\theta\_{old}}(a\_{i,t}|s\_{i,t})},
  1-\varepsilon, 1+\varepsilon \right) A\_{i,t} \right) - \beta
  D\_{KL}(\pi\_\theta(\cdot|s\_{i,t})||\pi\_{ref}(\cdot|s\_{i,t}))
  \right)\]

<h2 id="observations-from-dapo-paper">Observations from DAPO paper</h2>

<h3 id="decoupled-clipping-parameter">Decoupled Clipping parameter</h3>

<p>The same $\epsilon$ using in clipping doesn’t favor exploration token i.e tokens with less probability ex. 0.02. After some steps, the entropy of the model starts to dampen, i.e model is becoming less explorative, to restrict this entropy collapse, DAPO paper recommends using a high $\epsilon$ for (1 + $\epsilon$) and same $\epsilon=0.2$ for (1-$\epsilon$).</p>

<h3 id="increase-sample-output-variations">Increase sample output variations</h3>

<p>when the accuracy for all the samples in G have same accuracy 1. The Advantage becomes 0, and the gradient becomes 0 as well. Also, the number of samples with accuracy=1 increases as we train further which decreases the effective prompts in the batch which can lead to larger variance in gradients. The direction for the gradient update is not accurate since the effective prompts in batch is small because of which the gradient updates are noisy.</p>

<p>The proposed method is to oversample and filter out prompts with accuracy 0 or 1</p>

<h3 id="per-token-loss">Per token loss</h3>

<p>In GRPO, loss is first averaged over token and then averaged over all samples G. In doing so, there remains no difference between samples with very large/small sequence length i.e each sample will get averaged loss and tokens in long sequence will get less priority which could be a problem for use if that token is an important token in our long sequence.</p>

<p>DAPO solves this by first summing the loss for all tokens for all samples and then averaging it over all tokens.</p>

\[\mathcal{J}\_{\text{DAPO}}(\theta) = \mathbb{E}\_{(q,a) \sim \mathcal{D}, \{o\_i\}\_{i=1}^G \sim \pi\_{\theta\_{old}}(\cdot|q)} \left[
\frac{1}{\sum\_{i=1}^G |o\_i|} \sum\_{i=1}^G \sum\_{t=1}^{|o\_i|}
\min \left(
r\_{i,t}(\theta) \hat{A}\_{i,t}, \;
\text{clip}\left(
r\_{i,t}(\theta), 1 - \epsilon\_{\text{low}}, 1 + \epsilon\_{\text{high}}
\right) \hat{A}\_{i,t}
\right)
\right]\]

<h3 id="overlong-reward-shaping">Overlong Reward Shaping</h3>

<p>Instead of adding penalty for overly long outputs, just mask the truncated samples. This way the training is stable and enhances performances. Why do this ? adding penalty may confuse when the long reasoning is a valid/good reasoning trace.</p>

<h3 id="multi-turn-rl">Multi-turn RL</h3>

<p>llm and user interact over a period of time.</p>

<h4 id="problems">Problems</h4>

<p>(BENCHMARK)
no benchmark to test multi-turn interactions, and is not reasoning intensive(meaning they only focus on narrow domains, require max engineering overhead)</p>

<p>(RL ALGORITHMS)
PPO, DPO they often suffer from high variance when the horizon gets longer, resulting in poor performance.</p>

<h3 id="verl-algorithm">veRL algorithm</h3>

<p>I forget what these variables that we provide in .sh file while training, so I wrote this boilerplate to remember how these variables play out in the veRL implementation.</p>

<p>rollout code: https://github.com/volcengine/verl/blob/332c7d53c131869690578c0ab84f1e016ed4d817/verl/trainer/ppo/ray_trainer.py#L1081
update code: https://github.com/volcengine/verl/blob/332c7d53c131869690578c0ab84f1e016ed4d817/recipe/sppo/dp_actor.py#L60</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Hyperparameters:
# - train_batch_size = Total experiences per PPO iteration (rollout buffer size)
# - ppo_mini_batch_size = Batch size for each gradient step (subdivided from train_batch_size)
# - ppo_micro_batch_size_per_gpu = Per-GPU batch size (for multi-GPU training)
# - ppo_epochs = Number of passes over the rollout data
</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">total_epoch</span><span class="p">:</span>
	<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">complete_train_data</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">train_batch_size</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span> <span class="c1"># each batch size is provided by train_batch_size
</span>		<span class="nf">generate_rollout</span><span class="p">()</span> <span class="c1"># if GRPO use actor.rollout.n variable
</span>		<span class="nf">generate_old_logprobs</span><span class="p">()</span>
		<span class="nf">generate_ref_logprobs</span><span class="p">()</span>
		<span class="nf">calculate_advantages</span><span class="p">()</span>

		<span class="c1"># split batch into mini_batches.
</span>		<span class="n">minibatch_dataloader</span> <span class="o">=</span> <span class="n">batch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">ppo_mini_batch_size</span><span class="p">)</span> <span class="c1"># this is a dataloader with each minibatch of size ppo_mini_batch_size
</span>		<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">ppo_epoch</span><span class="p">:</span>
			<span class="k">for</span> <span class="n">minibatch</span> <span class="ow">in</span> <span class="n">minibatch_dataloader</span><span class="p">:</span>
				<span class="c1">#split minibatch into microbatches if needed to train on different GPUs
</span>				<span class="n">micro_batches</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">ppo_micro_batch_size_per_gpu</span><span class="p">)</span>
				<span class="n">gradient_accumulation</span> <span class="o">=</span> <span class="n">ppo_mini_batch_size</span> <span class="o">//</span> <span class="n">ppo_micro_batch_size_per_gpu</span>
				<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">micro_batches</span><span class="p">:</span>
					<span class="nf">generate_logprobs</span><span class="p">()</span>
					<span class="n">loss</span> <span class="o">=</span> <span class="nf">calculate_ppo_loss</span><span class="p">()</span> <span class="o">/</span> <span class="n">gradient_accumulation</span>
					<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
				<span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>


</code></pre></div></div>

<p>gradient_accumulation step is not used in a sense that we generally do while pretraining, it just maintains the count total number of micro batches that are processed in separate GPU, by dividing loss by gradient_accumulation we obtain loss as if the minibatch was processed directly without using any micro batch splits.</p>

<p>In this code total_training_steps the total number of rollouts that we do, and gradient update steps the number of gradient updates that we do. So keep in mind there two are two different things.</p>

<p>use dynamic_batch size to prevent OOM.
Here’s more complete version</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ---------------------------------
# 0.  CONSTANTS (from config)
# ---------------------------------
</span><span class="n">TOTAL_EPOCHS</span>               <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">trainer</span><span class="p">.</span><span class="n">total_epochs</span>
<span class="n">TRAIN_BATCH_SIZE</span>           <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">train_batch_size</span>          <span class="c1"># raw data loader batch
</span><span class="n">GEN_BATCH_SIZE</span>             <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">gen_batch_size</span><span class="sh">"</span><span class="p">,</span> <span class="n">TRAIN_BATCH_SIZE</span><span class="p">)</span>
<span class="n">ROLLOUT_N</span>                  <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">actor_rollout_ref</span><span class="p">.</span><span class="n">rollout</span><span class="p">.</span><span class="n">n</span>    <span class="c1"># GRPO: responses per prompt
</span><span class="n">PPO_MINI_BATCH_SIZE</span>        <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">actor_rollout_ref</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="n">ppo_mini_batch_size</span>
<span class="n">PPO_MICRO_BATCH_SIZE_PER_GPU</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">actor_rollout_ref</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="n">ppo_micro_batch_size_per_gpu</span>
<span class="n">PPO_EPOCHS</span>                 <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">actor_rollout_ref</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="n">ppo_epochs</span>
<span class="n">GRAD_ACCUM_STEPS</span>           <span class="o">=</span> <span class="n">PPO_MINI_BATCH_SIZE</span> <span class="o">//</span> <span class="n">PPO_MICRO_BATCH_SIZE_PER_GPU</span>  <span class="c1"># only when NOT dynamic-bsz
</span>
<span class="c1"># ---------------------------------
# 1.  OUTER LOOP
# ---------------------------------
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">TOTAL_EPOCHS</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">raw_dict</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span>               <span class="c1"># yields batches of size GEN_BATCH_SIZE
</span>        <span class="n">batch</span> <span class="o">=</span> <span class="n">DataProto</span><span class="p">.</span><span class="nf">from_single_dict</span><span class="p">(</span><span class="n">raw_dict</span><span class="p">)</span>

        <span class="c1"># ---------------------------------
</span>        <span class="c1"># 2.  ROLLOUT PHASE
</span>        <span class="c1"># ---------------------------------
</span>        <span class="n">gen_batch</span> <span class="o">=</span> <span class="n">batch</span><span class="p">.</span><span class="nf">pop_generation_keys</span><span class="p">()</span>     <span class="c1"># strip everything except what the policy needs
</span>        <span class="n">gen_batch</span> <span class="o">=</span> <span class="n">gen_batch</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">ROLLOUT_N</span><span class="p">)</span>     <span class="c1"># GRPO: duplicate prompts ROLLOUT_N times
</span>        <span class="n">rollout_output</span> <span class="o">=</span> <span class="n">actor_rollout_wg</span><span class="p">.</span><span class="nf">generate_sequences</span><span class="p">(</span><span class="n">gen_batch</span><span class="p">)</span>

        <span class="c1"># optional: max-baseline generation for REMAX
</span>        <span class="k">if</span> <span class="n">adv_estimator</span> <span class="o">==</span> <span class="n">REMAX</span><span class="p">:</span>
            <span class="n">baseline_output</span> <span class="o">=</span> <span class="n">actor_rollout_wg</span><span class="p">.</span><span class="nf">generate_sequences</span><span class="p">(</span><span class="n">gen_batch</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="c1"># ---------------------------------
</span>        <span class="c1"># 3.  BUILD FULL PPO BATCH
</span>        <span class="c1"># ---------------------------------
</span>        <span class="n">ppo_batch</span> <span class="o">=</span> <span class="n">batch</span><span class="p">.</span><span class="nf">union</span><span class="p">(</span><span class="n">rollout_output</span><span class="p">)</span>     <span class="c1"># concat prompts + responses
</span>        <span class="n">ppo_batch</span> <span class="o">=</span> <span class="n">ppo_batch</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">ROLLOUT_N</span><span class="p">)</span>     <span class="c1"># align prompt copies with responses
</span>
        <span class="c1"># 3a.  FILL IN EXTRA FIELDS
</span>        <span class="n">ppo_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">response_mask</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="nf">compute_response_mask</span><span class="p">(</span><span class="n">ppo_batch</span><span class="p">)</span>
        <span class="n">ppo_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">old_log_probs</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">actor_rollout_wg</span><span class="p">.</span><span class="nf">compute_log_prob</span><span class="p">(</span><span class="n">ppo_batch</span><span class="p">).</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">old_log_probs</span><span class="sh">"</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">use_reference_policy</span><span class="p">:</span>
            <span class="n">ppo_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">ref_log_prob</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">ref_policy_wg</span><span class="p">.</span><span class="nf">compute_log_prob</span><span class="p">(</span><span class="n">ppo_batch</span><span class="p">).</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">ref_log_prob</span><span class="sh">"</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">use_critic</span><span class="p">:</span>
            <span class="n">ppo_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">values</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">critic_wg</span><span class="p">.</span><span class="nf">compute_values</span><span class="p">(</span><span class="n">ppo_batch</span><span class="p">).</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">values</span><span class="sh">"</span><span class="p">]</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="nf">reward_fn</span><span class="p">(</span><span class="n">ppo_batch</span><span class="p">)</span>               <span class="c1"># rule or RM
</span>        <span class="n">ppo_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">token_level_scores</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">rewards</span>
        <span class="n">ppo_batch</span> <span class="o">=</span> <span class="nf">compute_advantage</span><span class="p">(</span><span class="n">ppo_batch</span><span class="p">)</span>     <span class="c1"># adds "advantages" &amp; "returns"
</span>
        <span class="c1"># ---------------------------------
</span>        <span class="c1"># 4.  MINI-BATCH LOOP
</span>        <span class="c1"># ---------------------------------
</span>        <span class="n">mini_batches</span> <span class="o">=</span> <span class="n">ppo_batch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">PPO_MINI_BATCH_SIZE</span><span class="p">)</span>   <span class="c1"># list of DataProto, each ≤ PPO_MINI_BATCH_SIZE
</span>        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">PPO_EPOCHS</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">mini_batch</span> <span class="ow">in</span> <span class="n">mini_batches</span><span class="p">:</span>
                <span class="c1"># 4a.  MICRO-BATCH LOOP  (gradient accumulation)
</span>                <span class="k">if</span> <span class="n">use_dynamic_bsz</span><span class="p">:</span>
                    <span class="n">micro_batches</span> <span class="o">=</span> <span class="nf">prepare_dynamic_batch</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">,</span>
                                                          <span class="n">max_token_len</span><span class="o">=</span><span class="n">PPO_MAX_TOKEN_LEN_PER_GPU</span> <span class="o">*</span> <span class="n">sp_size</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">micro_batches</span> <span class="o">=</span> <span class="n">mini_batch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">PPO_MICRO_BATCH_SIZE_PER_GPU</span><span class="p">)</span>

                <span class="n">actor_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">micro_batch</span> <span class="ow">in</span> <span class="n">micro_batches</span><span class="p">:</span>
                    <span class="n">log_probs</span><span class="p">,</span> <span class="n">entropy</span> <span class="o">=</span> <span class="n">actor_rollout_wg</span><span class="p">.</span><span class="nf">forward_micro_batch</span><span class="p">(</span><span class="n">micro_batch</span><span class="p">)</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="nf">compute_ppo_loss</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span>
                                            <span class="n">micro_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">old_log_probs</span><span class="sh">"</span><span class="p">],</span>
                                            <span class="n">micro_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">advantages</span><span class="sh">"</span><span class="p">],</span>
                                            <span class="n">micro_batch</span><span class="p">.</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">response_mask</span><span class="sh">"</span><span class="p">])</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">GRAD_ACCUM_STEPS</span>         <span class="c1"># scale for accumulation
</span>                    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

                <span class="n">actor_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>                    <span class="c1"># update happens once per mini-batch
</span></code></pre></div></div>]]></content><author><name>cohlem</name></author><category term="blog" /><category term="RL" /><summary type="html"><![CDATA[Before starting, it’s advisable to first complete David Silver’s Course on RL and read Lilian’s notes on RL which explains/provides notes on the David’s course in sequential manner.]]></summary></entry><entry><title type="html">Flops Calculation</title><link href="http://localhost:4000/2025/02/11/flops-calculation.html" rel="alternate" type="text/html" title="Flops Calculation" /><published>2025-02-11T00:00:00+05:45</published><updated>2025-02-11T00:00:00+05:45</updated><id>http://localhost:4000/2025/02/11/flops-calculation</id><content type="html" xml:base="http://localhost:4000/2025/02/11/flops-calculation.html"><![CDATA[<h4 id="calculation-of-flops">Calculation of FLOPs</h4>

<ul>
  <li>multiply accumulate cost: 2FLOPS i.e 1 for multiplication and 1 for accumulation (addition)</li>
  <li>if we multiply two matrices with sizes (a x b) and (b x c), the flops involved is b Multiply-add operation per the output size (a x c) i.e 2 x b x (a x c)</li>
</ul>

<h5 id="embedding-lookup">Embedding lookup</h5>

<p>we initially have tokens with (seq_len,vocab_size) one-hot representation and embedding lookup matrix is (vocab_size, d_model), it will take</p>

<p>FLOPs = 2 x ( vocab_size x (seq_len x d_model))</p>

<h5 id="attention">Attention</h5>

<p><strong>Q,K,V projections</strong>
X @ (Wq or Wk or Wv)
i.e 2 x (seq_len x d_model x key_size x num_heads)</p>

<p><strong>attention matrix</strong>
Q @ K.T
i.e 2* (seq_len x seq_len x key_size x num_heads)</p>

<p><strong>softmax</strong></p>

<ul>
  <li>1 for exponential calculation (e^x).</li>
  <li>seq_len - 1 sum for each row. so if we divide it per row, its basically 1 FLOPs per elements.</li>
  <li>1 for division
so it becomes, 2 x num_heads x seq_len x seq_len</li>
</ul>

<p><strong>Softmax @ query reductions</strong>
2 × seq_len × seq_len × (key_size × num_heads)</p>

<p><strong>Final Linear</strong>
2 × seq_len × (key_size × num_heads) × d_model</p>

<p><strong>Dense Block</strong> (per layer)
2×seq_len×(d_model×ffw_size+d_model×ffw_size) (ignoring FLOPs for actions here,)</p>

<p><strong>Final Logits</strong>
2×seq_len×d_model×vocab_size</p>

<p>so total FLOPs: embeddings+num_layers×(total_attention+dense_block) + logits</p>

<p>For backward, it takes 2 times the flops taken in backward.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calculate_transformer_flops</span><span class="p">(</span>
    <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">key_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">ffw_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Calculate FLOPs for each component of a transformer model including forward and backward passes.

    Args:
        seq_len: Sequence length
        vocab_size: Vocabulary size
        d_model: Model dimension
        key_size: Key dimension
        num_heads: Number of attention heads
        ffw_size: Feed-forward layer size
        num_layers: Number of transformer layers

    Returns:
        Dictionary containing FLOPs for each component and total forward/backward passes
    </span><span class="sh">"""</span>

    <span class="c1"># Embeddings
</span>    <span class="n">embedding_flops</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">vocab_size</span> <span class="o">*</span> <span class="n">d_model</span>

    <span class="c1"># Single Attention Layer
</span>    <span class="n">key_query_value_proj</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">*</span> <span class="p">(</span><span class="n">key_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="n">key_query_logits</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="p">(</span><span class="n">key_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="n">softmax_ops</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">seq_len</span>
    <span class="n">softmax_query_reduction</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="p">(</span><span class="n">key_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="n">final_linear</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="p">(</span><span class="n">key_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">)</span> <span class="o">*</span> <span class="n">d_model</span>

    <span class="n">total_attention_flops</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">key_query_value_proj</span>
        <span class="o">+</span> <span class="n">key_query_logits</span>
        <span class="o">+</span> <span class="n">softmax_ops</span>
        <span class="o">+</span> <span class="n">softmax_query_reduction</span>
        <span class="o">+</span> <span class="n">final_linear</span>
    <span class="p">)</span>

    <span class="c1"># Single Dense Block
</span>    <span class="n">dense_block_flops</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_model</span> <span class="o">*</span> <span class="n">ffw_size</span> <span class="o">+</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">ffw_size</span><span class="p">)</span>

    <span class="c1"># Final Logits
</span>    <span class="n">final_logits_flops</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">vocab_size</span>

    <span class="c1"># Total forward pass
</span>    <span class="n">total_forward_pass</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">embedding_flops</span>
        <span class="o">+</span> <span class="n">num_layers</span> <span class="o">*</span> <span class="p">(</span><span class="n">total_attention_flops</span> <span class="o">+</span> <span class="n">dense_block_flops</span><span class="p">)</span>
        <span class="o">+</span> <span class="n">final_logits_flops</span>
    <span class="p">)</span>

    <span class="c1"># Backward pass is approximately 2x forward pass
</span>    <span class="n">total_backward_pass</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">total_forward_pass</span>

    <span class="c1"># Total forward + backward
</span>    <span class="n">total_flops</span> <span class="o">=</span> <span class="n">total_forward_pass</span> <span class="o">+</span> <span class="n">total_backward_pass</span>

    <span class="k">return</span> <span class="n">total_flops</span>


<span class="c1"># Example usage
</span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">seq_len</span><span class="sh">"</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">vocab_size</span><span class="sh">"</span><span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">d_model</span><span class="sh">"</span><span class="p">:</span> <span class="mi">640</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">key_size</span><span class="sh">"</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">num_heads</span><span class="sh">"</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">ffw_size</span><span class="sh">"</span><span class="p">:</span> <span class="mi">2560</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">num_layers</span><span class="sh">"</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
<span class="p">}</span>


<span class="n">flops</span> <span class="o">=</span> <span class="nf">calculate_transformer_flops</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>


<span class="nf">print</span><span class="p">(</span><span class="n">flops</span><span class="p">)</span>
</code></pre></div></div>

<p>So this is flops required for our model per step with one batch.</p>]]></content><author><name>cohlem</name></author><summary type="html"><![CDATA[Calculation of FLOPs]]></summary></entry><entry><title type="html">Post Training Strategies</title><link href="http://localhost:4000/2025/02/06/post-training-strategies.html" rel="alternate" type="text/html" title="Post Training Strategies" /><published>2025-02-06T00:00:00+05:45</published><updated>2025-02-06T00:00:00+05:45</updated><id>http://localhost:4000/2025/02/06/post-training-strategies</id><content type="html" xml:base="http://localhost:4000/2025/02/06/post-training-strategies.html"><![CDATA[<p>After training, we generally perform alignment i.e teaching the model how to behave/act in desired manner. Post training mainly consists 1) Supervised Fine-tuning 2) RLHF</p>

<blockquote>
  <p>the current consensus within the research community seems to be that the optimal approach to alignment is to <em>i)</em> perform SFT over a moderately-sized dataset of examples with very high quality and <em>ii)</em> invest remaining efforts into curating human preference data for fine-tuning via RLHF.</p>
</blockquote>

<h3 id="supervised-fine-tuning">Supervised Fine-tuning</h3>

<p>Similar to pretraining, we perform next token prediction, but on different high-quality dataset.</p>

<p>Why doesn’t pre-training work out of the box ?
because the training objective is different, In pretraining we force the model to just predict the next token using data sampled from the internet, in one iteration the model could be learning about “how to make pizza” and in another iteration it could be learning “how half the species in Australia became extinct after humans arrived”. The data is sampled randomly. However, In SFT do the next token predict on highly curated instruction following dataset, so now we are making it to follow instructions again and again. As you can see the instruction following objective allows the model to learn instruction following with very small data.</p>

<p><strong>NOTE: in SFT we don’t consider the loss for the input tokens but only the output tokens</strong></p>

<p>for instance, we only consider the loss for the tokens from “assistant” role, and not the ‘user’ role, which can be found here in llama2 paper: https://arxiv.org/pdf/2307.09288</p>

<blockquote>
  <p>We utilize an autoregressive objective and zero-out the loss on tokens from the user prompt, so as a result, we backpropagate only on answer tokens. Finally, we fine-tune the model for 2 epochs.</p>
</blockquote>

<p><img src="pts1.png" alt="pts1" />
figure: <a href="https://arxiv.org/pdf/2203.02155">InstructGPT paper</a></p>

<blockquote>
  <p>Supervised fine-tuning (SFT). We fine-tune GPT-3 on our labeler demonstrations using supervised learning. We trained for 16 epochs, using a cosine learning rate decay, and residual dropout of 0.2. We do our final SFT model selection based on the RM score on the validation set. Similarly to Wu et al. (2021), we find that our SFT models overfit on validation loss after 1 epoch; however, we find that training for more epochs helps both the RM score and human preference ratings, despite this overfitting - <a href="https://arxiv.org/pdf/2203.02155">InstructGPT paper</a></p>
</blockquote>

<h3 id="datasets">Datasets</h3>

<p>For basic conversations:
https://huggingface.co/datasets/HuggingFaceTB/everyday-conversations-llama3.1-2k</p>

<p>from smolTalk</p>

<p>keep all of Everyday-Conversations
filter math, coding dataset
filter to 512 tokens</p>

<p>magic-pie ultra - he dataset contains challenging instructions and responses for a wide variety of tasks, such as Coding &amp; debugging, Math, Data analysis, Creative Writing, advice seeking, or Brainstorming.</p>

<p>Take magic-pie-ultra remove math and coding and debugging, limit upto two-turn conversation, and keep only rows with &lt; 512 tokens</p>

<p>take smoltalk, do the same, select Smol-Rewrite, smol-constraints, smol-summarization</p>

<p>remove the system prompt, by combining it to user prompt</p>

<p>select 5k examples from each</p>

<p>combine it with filtered magpie</p>

<p>mix it with all of everyday conversations</p>

<p>see here
https://colab.research.google.com/drive/1QkIpkhaZVNvZwBoD69N5O-md5FjRSW_W?usp=sharing</p>

<p>colab train link: https://www.linkedin.com/in/mihirsinh-chauhan-5bb437239/</p>

<p>remove columns with larger seq len than 512</p>

<p>conversational open data:
https://huggingface.co/datasets/OpenAssistant/oasst1/viewer/default/train?row=0</p>

<p>synthetic data
https://github.com/thunlp/UltraChat?tab=readme-ov-file</p>

<p>To teach models to say certain things we either train it on sft datasets, or we put it in system message i.e put it in it’s context windows (usually hidden from users)</p>

<p>olmo hard coded sft mixture data
https://huggingface.co/datasets/allenai/olmo-2-hard-coded?row=2</p>

<p>First, I remember that in supervised fine-tuning (SFT) for language models, especially in conversational settings, the standard practice is to train the model only on the assistant’s responses. This is because the model’s role is to generate appropriate responses given the user’s input, not to learn to predict the user’s messages.</p>]]></content><author><name>cohlem</name></author><summary type="html"><![CDATA[After training, we generally perform alignment i.e teaching the model how to behave/act in desired manner. Post training mainly consists 1) Supervised Fine-tuning 2) RLHF]]></summary></entry><entry><title type="html">Building Lillm</title><link href="http://localhost:4000/2025/01/29/building-lillm.html" rel="alternate" type="text/html" title="Building Lillm" /><published>2025-01-29T00:00:00+05:45</published><updated>2025-01-29T00:00:00+05:45</updated><id>http://localhost:4000/2025/01/29/building-lillm</id><content type="html" xml:base="http://localhost:4000/2025/01/29/building-lillm.html"><![CDATA[<h2 id="pre-training">Pre-training</h2>

<h3 id="document-packing">Document packing</h3>

<p>while pretraining, different documents could be packed inside a sequence. For instance, a model with context_length 1024 can have 256 tokens from one doc and rest from the other. Demilited by EOS token.</p>

<p>The samples may contaminate the attention, for which cross sample attention masking is used.
But, it isn’t used by DeepSeek v3, lets not use it.</p>

<p>while packing documents. we simply pack them as they appear in order and then add EOS token (used by GPT-2,3). But DeekSeek also uses FIM (Fill in middle) strategy using this Prefix-Suffix-Middle (PSM) framework.</p>

<p><code class="language-plaintext highlighter-rouge">&lt;|fim_begin|&gt; 𝑓pre &lt;|fim_hole|&gt; 𝑓suf &lt;|fim_end|&gt; 𝑓middle &lt;|eos_token|&gt;.</code></p>

<p>adopted for 0.1% of data, generally used for overfitting or limiting the model from using the same general method.</p>

<ul>
  <li>Do vibe check once in a while</li>
</ul>

<p>commands</p>

<p>change num_proc in process.py</p>

<p>python process.py –tokenizer_path /model/tokenizer</p>

<p>training run
torchrun –standalone –nproc_per_node=2 pretrain.py</p>]]></content><author><name>cohlem</name></author><summary type="html"><![CDATA[Pre-training]]></summary></entry></feed>