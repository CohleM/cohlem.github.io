[{"content":"Backpropagation Backpropagation on scalars from scratch Manual Backpropagation on tensor Loss function Maximum likelihood estimate as loss function Why we add regularization to loss functioñ Goto video for intuitively understanding Entropy/Cross-Entropy/KL-divergence Optimization Optimization Algorithms (SGD with momentum, RMSProp, Adam) Optimizing loss with weight initialization BatchNormalization RMSNorm Diagnostic tool to look out for while training NN Skip Connections Training Misc Matrix Visualization SwiGLU activation- not mine, but offers best explanation Architecture Implementation GPT implementation MoE RoPE KV Cache and Grouped Query Attention LoRA Multi-head Latent Attention GPU Basic intro to GPU architecture Interpretability Mechanistic Interpretability\n","permalink":"https://cohlem.github.io/notes/deep-learning-notes/","summary":"Backpropagation Backpropagation on scalars from scratch Manual Backpropagation on tensor Loss function Maximum likelihood estimate as loss function Why we add regularization to loss functioñ Goto video for intuitively understanding Entropy/Cross-Entropy/KL-divergence Optimization Optimization Algorithms (SGD with momentum, RMSProp, Adam) Optimizing loss with weight initialization BatchNormalization RMSNorm Diagnostic tool to look out for while training NN Skip Connections Training Misc Matrix Visualization SwiGLU activation- not mine, but offers best explanation Architecture Implementation GPT implementation MoE RoPE KV Cache and Grouped Query Attention LoRA Multi-head Latent Attention GPU Basic intro to GPU architecture Interpretability Mechanistic Interpretability","title":"Deep Learning Notes"},{"content":"Image Source: https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts\nBasic MoE structure Experts are FFNN themselves, instead of passing input representation to only one dense FFNN we now have option to route them to more FFNNs. Since most LLMs have several decoder blocks, a given text will pass through multiple experts before the text is generated.\nDown the line it could use multiple experts but at different blocks i.e (layers)\nA routing layer is set to choose experts depending on how many experts are selected MoE are categorized into two i.e dense MoE in which almost all the experts are selected and sparse MoE only some experts are selected.\nNot only will there be an uneven distribution of experts chosen, but some experts will hardly be trained at all. This results in issues during both training and inference.\nInstead, we want equal importance among experts during training and inference, which we call load balancing. In a way, it’s to prevent overfitting on the same experts.\nLoad Balancing To balance the importance of experts, we will need to look at the router as it is the main component to decide which experts to choose at a given time.\nKeepTopK By introducing trainable (gaussian) noise, we can prevent the same experts from always being picked. It\u0026rsquo;ll help router to distribute experts and not restrict to some specific experts.\nCapacity Factor Distributing experts is not enough because distribution of expert happens close to no.of.steps times but there are a lot of batch of tokens that are processed in a single step. An expert could be assigned more than the others but it can also be assigned less tokens as compared to others. The solution is to equally divide the number of tokens to all the expert using capacity factor given by this formula. Implementation Now that we know what MoE is, let\u0026rsquo;s implement it from scratch.\nImplementation of Adaptive Mixture of Local Experts The MoE was defined as a set of independent experts (feed-forward networks) alongside a gating network (also a feed-forward network, ). All the experts and the gating network receive the same input . The gating network outputs the distribution of each expert relevance/importance for the given input and is defined as by Softmax(x@Wg) in its simplest form, where Wg is a (optional) learnable transformation. Finally, the output of the system is the sum of the outputs of all experts weighted by the output of the gating network.\nfrom dataclasses import dataclass torch.manual_seed(42) @dataclass class Config(): n_embd:int = 10 block_size:int = 5 expert_size:int = 2 vocab_size:int = 65 class Router(nn.Module): def __init__(self,config): super().__init__() self.config = config self.router = nn.Sequential(nn.Linear(self.config.n_embd,self.config.expert_size)) def forward(self,x): return self.router(x) class FFN(nn.Module): def __init__(self,config): super().__init__() self.config = config self.ffn = nn.Sequential(nn.Linear(self.config.n_embd, self.config.vocab_size)) def forward(self,x): return self.ffn(x) class MoE(nn.Module): def __init__(self, config): super().__init__() self.config = config self.router = Router(config) self.experts = nn.ModuleList([FFN(config) for _ in range(config.expert_size)]) def forward(self, x): # we = self.we(x) # batch\u0026#39;s embeddings (B,T,C) ep = self.router(x).softmax(dim=-1) # expert probability (B,T,E) ep = ep.unsqueeze(-1) # adding one dim to each of our experts, (B,T,E,1) exp_out = torch.stack([out(x) for out in self.experts], dim=-2) # (B,T,E,C) out = exp_out * ep # (B,T,E,C) x (B,T,E,1) out = out.sum(-2) # (B,T,C) return out Implementation of OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER Using all the experts for inputs will be computationally expensive. A way to reduce that is to implement noise_gating + topK method specified in the paper OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER\nlet\u0026rsquo;s understand these equation with reference to the code\nclean_logits = self.router(x) # corresponds to (x.W) in the equation 4 This is simply using the gating function to calculate the probability of experts for each input tokens. (this self.router(x) is an object of a class Router defined above)\nnoise = torch.randn_like(clean_logits)*F.softplus(self.wnoise(x)) here torch.randn_like(clean_logits) resembles StandardNormal(), where we choose a random gaussian noise (mean = 0, std=1) to be added as a noise. Adding this will introduce some noise which encourages model to choose other experts.\nself.wnoise is a noise but it\u0026rsquo;s input dependent learnable parameter, because we don\u0026rsquo;t want to choose experts completely randomly, instead it has to be input dependent, and it is learned during backprop.\nand by adding F.softplus we are capping the output of noise to be greater than 0. it\u0026rsquo;s approximately similar to relu.\nh = clean_logits + noise we add the clean logits. and the noise, so we now encourage models to explore other experts too.\nNow, let\u0026rsquo;s make our MoE compute efficient i.e by choosing only the topK models for specific tokens.\ntopk_val, topk_ind = torch.topk(h,self.config.top_k, dim = -1) it will choose the topK experts for each token. topK is taken along the last dimension i.e -1 because experts probability is along the last axis. i.e h shape is (B,T,expert_size)\nout = torch.full_like(h, fill_value=float(\u0026#39;-inf\u0026#39;)) out = out.scatter(dim=-1,index=topk_ind, src=topk_val) out = torch.softmax(out, dim=-1) we are now creating a new tensor with all the values with negative infinity and setting the topK values with their original values and for other there will be negative infinity. Now normalizing using softmax we get normalized probabilities and 0 in place of negative infinity.\nnow that we have our expert\u0026rsquo;s probability for each input. Let\u0026rsquo;s now pass the input through expert and take the weighted sum because we have topk probability assigned to a token input.\ncalculating weighted sum for the input tokens can be a difficult in terms of implementation.\nThe general idea is to iterate over all the experts to first create a mask from our router\u0026rsquo;s output probabilities for each expert. (i.e creating a mask of True if this input probability in within topk for that specific expert) and flatten that mask, and pluck out the inputs from the flattened input using that mask (mask will help us pluck out input tokens with specific index where mask value is true) and pass that plucked out input to the expert layer and then multiply the expert layer\u0026rsquo;s output with the router\u0026rsquo;s probability for that specific expert and then keep adding these values for all the experts because we are doing the weighted sum.\nThe code to do that is given below (it can take some time to understand, but it\u0026rsquo;s relative easy if you understand this explanation)\ndef forward(self, x): # we = self.we(x) # batch\u0026#39;s embeddings (B,T,C) out,topk_val,topk_ind = self.router(x) # out is size (B,T,experts) experts (B,T,top_k) flat_x = x.view(-1, x.shape[-1]) #(B*T,C) final_output = torch.zeros_like(flat_x) # for i,expert in enumerate(self.experts): expert_mask = (out[:,:,i] != 0) # (B,T) expert_mask = expert_mask.view(-1) # (B*T) if expert_mask.any(): # pass through expert layer only if flattened expert has any one true value select_x = flat_x[expert_mask] expert_x = expert(select_x) * (out[:,:,i]).view(-1)[expert_mask].unsqueeze(-1) #unsqueeze for broadcasting across columns final_output[expert_mask] += expert_x # add because our inp is combination of experts, i.e one token can take can top2 prob final_output = final_output.view(x.shape) return final_output Full code til here class Router(nn.Module): def __init__(self,config): super().__init__() self.config = config self.router = nn.Sequential(nn.Linear(self.config.n_embd,self.config.expert_size)) self.wnoise = nn.Linear(self.config.n_embd, self.config.expert_size) def forward(self,x): clean_logits = self.router(x) noise = torch.randn_like(clean_logits)*nn.Softplus(self.wnoise(x)) h = clean_logits + noise topk_val, topk_ind = torch.topk(h,self.config.top_k, dim = -1) out = torch.full_like(h, fill_value=float(\u0026#39;-inf\u0026#39;)) out = out.scatter(dim=-1,index=topk_ind, src=topk_val) out = torch.softmax(out, dim=-1) return out,topk_val, topk_ind class MoE(nn.Module): def __init__(self, config): super().__init__() self.config = config self.router = Router(config) self.experts = nn.ModuleList([FFN(config) for _ in range(config.expert_size)]) def forward(self, x): # we = self.we(x) # batch\u0026#39;s embeddings (B,T,C) out,topk_val,topk_ind = self.router(x) # out is size (B,T,experts) experts (B,T,top_k) flat_x = x.view(-1, x.shape[-1]) #(B*T,C) final_output = torch.zeros_like(flat_x) for i,expert in enumerate(self.experts): expert_mask = (out[:,:,i] != 0) # (B,T) expert_mask = expert_mask.view(-1) # (B*T) print(em_new == expert_mask) if expert_mask.any(): select_x = flat_x[expert_mask] expert_x = expert(select_x) * (out[:,:,i]).view(-1)[expert_mask].unsqueeze(-1) #unsqueeze for broadcasting across columns final_output[expert_mask] += expert_x # add because our inp is combination of experts, i.e one token can take can top2 prob final_output = final_output.view(x.shape) return final_output This implementation assigns expert for each no.of.steps in our training step, here even if the experts are assigned equally, the total number of tokens assigned to those expert could be different, for that reason we would want to distribute input tokens equally to all the experts. We can do that using expert capacity explained above in the Load Balancing section above.\nExpert Capacity implementation we define expert capacity as total number of tokens in a batch divided by number of experts times the capacity factor (which controls the scale).\nexp_cap = int(B*T*self.config.top_k / self.config.expert_size * self.config.capacity_factor) select the indices of input where this specific expert is applied and limit the inputs to be processed to be within the expert capacity if it is greater than expert capacity and truncate other tokens.\nselected_indices = torch.nonzero(expert_mask).squeeze(-1) limited_indices = selected_indices[:exp_cap] if selected_indices.numel() \u0026gt; exp_cap else selected_indices Full implementation class Router(nn.Module): def __init__(self,config): super().__init__() self.config = config self.router = nn.Sequential(nn.Linear(self.config.n_embd,self.config.expert_size)) self.wnoise = nn.Linear(self.config.n_embd, self.config.expert_size) def forward(self,x): clean_logits = self.router(x) noise = torch.randn_like(clean_logits)*F.softplus(self.wnoise(x)) h = clean_logits + noise topk_val, topk_ind = torch.topk(h,self.config.top_k, dim = -1) out = torch.full_like(h, fill_value=float(\u0026#39;-inf\u0026#39;)) out = out.scatter(dim=-1,index=topk_ind, src=topk_val) out = torch.softmax(out, dim=-1) return out,topk_val, topk_ind class MoE(nn.Module): def __init__(self, config): super().__init__() self.config = config self.router = Router(config) self.experts = nn.ModuleList([FFN(config) for _ in range(config.expert_size)]) self.wimportance = 1.0 def forward(self, x): # we = self.we(x) # batch\u0026#39;s embeddings (B,T,C) B,T,C = x.shape out,topk_val,topk_ind = self.router(x) # out is size (B,T,experts) experts (B,T,top_k) flat_x = x.view(-1, x.shape[-1]) #(B*T,C) final_output = torch.zeros_like(flat_x) exp_cap = int(B*T*self.config.top_k / self.config.expert_size * self.config.capacity_factor) for i,expert in enumerate(self.experts): expert_mask = (out[:,:,i] != 0) # (B,T) expert_mask = expert_mask.view(-1) # (B*T) selected_indices = torch.nonzero(expert_mask).squeeze(-1) limited_indices = selected_indices[:exp_cap] if selected_indices.numel() \u0026gt; exp_cap else selected_indices if expert_mask.any(): select_x = flat_x[limited_indices] expert_x = expert(select_x) * (out[:,:,i]).view(-1)[limited_indices].unsqueeze(-1) #unsqueeze for broadcasting across columns final_output[limited_indices] += expert_x # add because our inp is combination of experts, i.e one token can take can top2 prob final_output = final_output.view(x.shape) # Importance scores for experts i.e batchwise sum of the router\u0026#39;s output importance = out.sum(dim=0) # loss that needs to be added to encourage model to choose diverse experts imp_std,imp_mean = importance.std(), importance.mean() loss_moe = ((imp_std/imp_mean)**2)*self.wimportance return final_output,loss_moe Auxiliary Loss To encourage models to make expert\u0026rsquo;s probability uniform (choosing all the experts and not restricting to some experts) We add to our main loss another loss term that we get from our MoE layer. It is calculated by first calculating the importance which is simply calculating the batch sum over the inputs for the router\u0026rsquo;s output and calculating the square of the coefficient of variation from the importance and then multiplying with a hand tuned scaling factor called Wimportance.\nThe implementation from this auxiliary loss is already implemented in the code above.\nVisualizations what\u0026rsquo;s the point of adding the MoE losses and noises if we can\u0026rsquo;t visualize it\u0026rsquo;s effect. I\u0026rsquo;ve trained my GPT + MoE architecture with variations to visualize what they actually do.\nNo Noise and No MoE loss I did not add the noise term i.e in the equation 4 in the picture above and did not add MoE loss to our original loss function.\nx axis = no of steps y axis = no of tokens assigned to each expert Even though the plot seems to fluctuate too much, we can see that there is inappropriate distribution of tokens among the experts i.e expert 0 is assigned less tokens and and expert 1 and 3 are assigned more tokens and the curve relatively stays the same because we did not add the loss function too.\nNoise but no MoE loss There is relatively small gap between number of tokens assigned to the experts because this time we added gaussian noise which reduces the gap, but the number of tokens assigned to them remains constantly fluctuating.\nNo Noise but MoELoss included As you can see there was inappropriate distribution of tokens in the beginning but the model seems to have learned to distribute the tokens among the experts after training for some time.\nIncluding Noise and MoE Loss it looks similar to the previous one, i.e big variation in the beginning but learns to distribute afterwards, its different from the previous one in that initially the range of tokens assigned to experts are between (150, 350) but in the previous plot it was (100,400). This less variation in this plot can be attributed to the addition of gaussian noise.\nIn conclusion, addition of loss seems to be more important than including the Noise because it seems to become stable in the later iterations.\nWhole code with GPT architecture and this MoE implementation can be found here: https://github.com/CohleM/deep_learning/blob/master/MoE/moe.ipynb\nImprovements Replace for loop with matrix multiplication while calculating the weighted sum of experts. Improve Block class\u0026rsquo; implementation, it doesn\u0026rsquo;t look neat, we return the modified x and the loss from the moe Switch Transformers (paper) Key points model improves as the parameters are increase. Following the same pattern they increase the parameter count but with same the FLOP used for a token in previous implementations. Instead of routing tokens to topK \u0026gt; 1 experts they route tokens to k=1 experts (this preserves model quality, reduces routing computation and performs better) since gate function is a softmax layer it is perfectly differentiable. (out of this paper: discrete operations are not differentiable i.e choosing max from a list, cause derivative of constant is 0 so gradient propagation stops)\nChoosing right capacity factor is important as shown in figure below. as you can see the when CF is 1 only one token is truncated (not evaluated) but is passed through to next layers through residual connection.\nwhen CF is 1.5 3 memory spaces are wasted. So there\u0026rsquo;s a tradeoff. They find ensuring lower rates of dropped tokens are important for the scaling of sparse expert-models. The auxiliary loss is given by this equation Since we seek uniform routing of the batch of tokens across the N experts, we desire both vectors to have values of 1/N\n1/N is a percentage term i.e if there are 4 experts then each experts should be assigned 25% i.e 1/4 tokens.\nsimilarly for Pi it should be 1/n because each router should assign equal probability i.e 25% to promote uniform distribution.\nThe implementation of loss is likewise.\nF vector consider this sample is our router probability for all 5 tokens and 4 experts (5x4) matrix\nsample = torch.randn(5,4) sample tensor([[ 0.0384, 0.3811, -0.9004, 0.0853], [ 0.2770, 0.1141, -0.6625, 0.4889], [ 0.7854, 0.7123, -0.3660, -1.2273], [ 0.9355, 1.9071, 0.7386, -0.3621], [ 0.8633, -0.5028, -1.0617, -1.2414]]) val,ind = torch.topk(sample,k=1,dim=-1) val,ind (tensor([[0.3811], [0.4889], [0.7854], [1.9071], [0.8633]]), tensor([[1], [3], [0], [1], [0]])) f_vector = torch.zeros_like(sample) ones = torch.ones_like(sample) f_vector tensor([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) f_vector.scatter_(dim=-1,index=ind, src=ones) tensor([[0., 1., 0., 0.], [0., 0., 0., 1.], [1., 0., 0., 0.], [0., 1., 0., 0.], [1., 0., 0., 0.]]) f_vector.sum(dim=0) tensor([2., 2., 0., 1.]) f_vector = f_vector.sum(dim=0)/f_matrix.shape[0] f_vector tensor([0.4000, 0.4000, 0.0000, 0.2000]) see the imbalance? each expert should get 1/4= 0.25 tokens, we need to minimize this inappropriate distribution by adding this in loss function.\nP vector sample tensor([[ 0.0384, 0.3811, -0.9004, 0.0853], [ 0.2770, 0.1141, -0.6625, 0.4889], [ 0.7854, 0.7123, -0.3660, -1.2273], [ 0.9355, 1.9071, 0.7386, -0.3621], [ 0.8633, -0.5028, -1.0617, -1.2414]]) sample = sample.softmax(dim=-1) sample tensor([[0.2599, 0.3661, 0.1016, 0.2724], [0.2877, 0.2444, 0.1124, 0.3555], [0.4203, 0.3907, 0.1329, 0.0562], [0.2111, 0.5578, 0.1734, 0.0577], [0.6567, 0.1675, 0.0958, 0.0800]]) p_vector = sample.sum(dim=0)/sample.shape[0] p_vector tensor([0.3671, 0.3453, 0.1232, 0.1644]) as you can see the experts have imbalanced distribution of probabilities, the objective of including P vector is to make this p vector\u0026rsquo;s distribution uniform\nloss function N = 4 alpha = 1.0 loss = alpha * N * (p_vector*f_vector).sum() loss tensor(1.2714) DeepSeekMoE (paper) One of the recent MoE paper.\nThere were these limitations.\nKnowledge Hybridity: models utilize small experts (N), information is shared among these experts, and experts can\u0026rsquo;t be specialized. Knowledge Redundancy: They may train same tokens resulting in experts that learn same concepts, ultimately there is redundancy of knowledge. In this paper, they propose two changes.\nFie-grained Expert Segmentation: i.e divide the hidden dimension of current MoE layer to 1/m and create separate mxN number of experts (total parameters remains the same). Doing so will result in greater possibility of choosing experts for a token and experts can be specialized. Shared Expert Isolation: There must be expert that should process some general knowledge task. for that reason they separate out some experts for this knowledge sharing. By sharing knowledge, the fine grained experts don\u0026rsquo;t need to acquire extra knowledge, enabling them to specialize in specific tasks. The output representation for a batch, will look like this.\nThe loss is similar to what we read in switch transformers but incorporating the changes that we made i.e dividing expert into m expert and assigning a common expert. The also include device level balance loss, which is for balancing load across devices. I haven\u0026rsquo;t read more into how load is balanced in devices, which I leave it for future studies.\nReferences A Visual Guide to Mixture of Experts (MoE) Adaptive Mixture of Local Experts OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models) ","permalink":"https://cohlem.github.io/sub-notes/mixture-of-experts/","summary":"Image Source: https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts\nBasic MoE structure Experts are FFNN themselves, instead of passing input representation to only one dense FFNN we now have option to route them to more FFNNs. Since most LLMs have several decoder blocks, a given text will pass through multiple experts before the text is generated.\nDown the line it could use multiple experts but at different blocks i.e (layers)\nA routing layer is set to choose experts depending on how many experts are selected MoE are categorized into two i.","title":"Mixture of Experts"},{"content":"Scaled-dot product Attention Q1 Given the attention equation $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{(xWq)(xWk)^\\top}{\\sqrt{d_k}}\\right)(xWv)W_O $$ Why don\u0026rsquo;t we train by combining $WqWk^\\top$ and $WvWo$? because mathematically they seem equivalent $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{x(WqWk^\\top)x^\\top}{\\sqrt{d_k}}\\right)x(WvW_O) $$ I initially thought if we could combine those weights, we don\u0026rsquo;t need to calculate $Q,K,V$ meaning there will be less number of matrix multiplication.\nAnswer We lose the objective of $Q,K,V,O$, they are meant to operate independently. In analogy, $Q$ represents the notion of \u0026ldquo;what we have\u0026rdquo;, $K$ whats available to us and so on. BUT, if we combine them into a giant matrix $WqWk^\\top$ during backpropagation weight updates are mixed. We no longer update them separately so we end up loosing their significance.\nQ2 So, if we can\u0026rsquo;t use them during training, can we still mix those weights during inference now that we aren\u0026rsquo;t updating weights? since the mathematical equivalence is the same, can we use it to optimize inference performance?\nAnswer We decrease the number of matrix multiplication, BUT we end up increasing the actual element wise multiplications inside those matrix multiplication.\nWe end up decreasing the speed rather than increasing it.\nLet\u0026rsquo;s see this by comparison.\nNOTE: Given matrix A with size (2,3) and B with size (3,4), the total number of element wise matrix multiplication between is (2 * 3 * 4).\n$n$ = number of tokens $d_{\\text{model}}$: embedding dimension $nh$: number of heads $hd$: number of head_dimension $d_{\\text{k}}$: $nh$ x $hd$\nCASE I: Original Attention Compute $Q = X W_Q$: $\\mathcal{O}(n \\cdot d_{\\text{model}} \\cdot d_k)$\nCompute $K = X W_K$: $\\mathcal{O}(n \\cdot d_{\\text{model}} \\cdot d_k)$\nCompute $QK^T$: $\\mathcal{O}(n^2 \\cdot d_k)$\nCASE II: Combined Compute $X W_{QK}$: $\\mathcal{O}(n \\cdot d_{\\text{model}}^2)$\nCompute $(X W_{QK}) X^T$: $\\mathcal{O}(n^2 \\cdot d_{\\text{model}})$\nIf $d_k \\ll d_{\\text{model}}$ (e.g., $d_k = 128$, $d_{\\text{model}} = 512$):\nOriginal: $\\mathcal{O}( n \\cdot 512 \\cdot 128)+ \\mathcal{O}( n \\cdot 512 \\cdot 128) + \\mathcal{O}(n^2 \\cdot 128)$\nCombined: $\\mathcal{O}(n \\cdot 512^2) + \\mathcal{O}(n^2 \\cdot 512)$\nAs you can see the number of matrix multiplication is 3, but the total elementwise multiplication is very large.\nMulti-head Latent Attention The main reason behind using the variants of Attention is that we always to increase our inference speed and we are always bottlenecked by KV cache The KV cache needed in original Multi-head attention is $2\\cdot nh\\cdot hd\\cdot l$ for one token, as the tokens get large during inference, the memory needed for storing this case also increases.\nDeepseek propose the use of latent dimension to compress the dimension.\nAs we know $K,V$ both come from the same x i.e $K=xWk$ and $V=xWv$ but the different weights $Wk, Wv$\nhow about we make an intermediate compressed version of x, from which we can decompress it into K and V, and only store that compressed version of x. This is what they use for multi-head latent attention.\n$W_{\\text{dkv}}$: compression matrix of size ($d_{\\text{model}}$, $d_{\\text{c}}$) $L_{\\text{kv}}$= $xW_{\\text{dkv}}$ which is the compressed version of x\nWe decompress $L_{\\text{kv}}$ into K, V using $Wuk, Wuv$ i.e\n$Q=xWq$\n$Kc=L_{\\text{kv}} \\cdot Wuk$ ($Wuk$ size = ($d_{\\text{c}}, nh \\cdot hd$))\n$Vc=L_{\\text{kv}} \\cdot Wuv$ ($Wuv$ size = ($d_{\\text{c}}, nh \\cdot hd$))\nSimilarly, we can substitute those in our original attention equation $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{(Q)(Kc)^\\top}{\\sqrt{d_k}}\\right)(Vc)W_O $$ As you can see, we\u0026rsquo;ve increased the number of matrix multiplication (i.e the computation of $L_{\\text{kv}}$= $xW_{\\text{dkv}}$), but the total number of elementwise multiplication can be made comparable with the right choice of compression dimension $d_{\\text{c}}$\nBut, our main goal was to reduce the number of KV cache, but if we store only the $L_{\\text{kv}}$ only, we still would would need to perform $Kc=L_{\\text{kv}} \\cdot Wuk$ and $Vc=L_{\\text{kv}} \\cdot Wuv$ to calculate attention. So whats the point of this compression?\nWell there\u0026rsquo;s a trick to still store only the $L_{\\text{kv}}$ and use it without calculating Kc and Vc, we do weight combination like in our Q2 but still end up with less number of elementwise matrix multiplication. The equation above can also we written as\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{(xWq)(L_{\\text{kv}} \\cdot Wuk)^\\top}{\\sqrt{d_k}}\\right)(L_{\\text{kv}} \\cdot Wuv)W_O $$\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{(x(Wq Wuk^\\top) (L_{\\text{kv}})^\\top}{\\sqrt{d_k}}\\right)L_{\\text{kv}}(WuvW_O) $$\nAfter combining $(Wq Wuk^\\top)$ and $(WuvW_O)$ once, we can save $L_{\\text{kv}}$ in our cache and then directly multiply with $(Wq Wuk^\\top)$ to get the attention, without needing to calculate $Kc$ and $Vc$. Remember the issue we had while combining weights in Q2, this fades away because of the compression dimension because it strictly has to be less than $nh \\cdot hd$ i.e ($d_{\\text{c}} \\ll nh \\cdot hd$)\nDecoupled RoPE There are still some parts I feel like I don\u0026rsquo;t understand completely. But, here\u0026rsquo;s what I\u0026rsquo;ve understood till now.\nFirst thing to keep in mind, its the clever weight absorption design and caching only $L_{\\text{kv}}$ that helps MLA to retain its fast speed. But, we have yet to apply positional information to our Q and K i.e\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{R1(xWq)(R2(L_{\\text{kv}} \\cdot Wuk))^\\top}{\\sqrt{d_k}}\\right)(L_{\\text{kv}} \\cdot Wuv)W_O $$\nIn the paper, they say the RoPE matrix gets in the middle but I don\u0026rsquo;t know what i am missing here cause $({R1(xWq)Wuk^\\top)L_{\\text{kv}}^\\top R2^\\top}$ so weights can still be combined? I think there something that I don\u0026rsquo;t understand here. I\u0026rsquo;ll correct my understand in future.\nlets resume from the point that RoPE matrix gets in the middle. They again design a clever thing here. Add two new matrices $W_{QR}\\in \\mathbb{R}^{(d,nh \\cdot d^R)}$ and $W_{KR} \\in \\mathbb{R}^{(d,d^R)}$\n$Q^{R}=RoPE(xW_{QR})$ and $K^{R}=RoPE(xW_{KR})$\nand also cache the $K^R$\nadd then concatenate the new two matrices to the original Q and K\n$Q = [Q, Q^R]$\n$K = [K, K^R]$\nand then perform our original attention. $$ Q \\cdot K^\\top = [Q, Q^R] \\cdot [K, K^R]^\\top $$\n$$ Q \\cdot K^\\top = [Q \\cdot K + Q^R \\cdot K^R] $$\nas you can see the original Q and K are still preserved meaning we can still absorb the weights.\nTotal cached elements will be $K^R$ and $L_{kv}$ so the total saved cache will be $(d^R + dc)l$\nQuestion Why is second dimension of $W_{KR}$ is $d^R$ and not $(nh \\cdot d^R)$ Meaning $d^R$ will be broadcasted across all the heads.\nMy guess is that they found that keeping only $d^R$ would produce decent result and would also save the cache memory requirement.\nReferences https://arxiv.org/abs/2405.04434 https://liorsinai.github.io/machine-learning/2025/02/22/mla.html#multi-head-latent-attention\n","permalink":"https://cohlem.github.io/sub-notes/multi-head-latent-attention/","summary":"Scaled-dot product Attention Q1 Given the attention equation $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{(xWq)(xWk)^\\top}{\\sqrt{d_k}}\\right)(xWv)W_O $$ Why don\u0026rsquo;t we train by combining $WqWk^\\top$ and $WvWo$? because mathematically they seem equivalent $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{x(WqWk^\\top)x^\\top}{\\sqrt{d_k}}\\right)x(WvW_O) $$ I initially thought if we could combine those weights, we don\u0026rsquo;t need to calculate $Q,K,V$ meaning there will be less number of matrix multiplication.\nAnswer We lose the objective of $Q,K,V,O$, they are meant to operate independently.","title":"Multi-head latent attention"},{"content":"LoRA Main idea is to approximate the change in weights dW by the use of low-rank matrices\nEg: Usually the weight update is done by adding the change in weights dW to the original weight matrix W. dW is obtained through backpropagation, ex if W is 512 x 512 the parameter size of dW is 262,144.\nIn LoRA, we approximate that dW but by breaking down into two low rank matrices B @ A where B = matrix of size 512 x r and A = matrix of size r x 512,\npreviously if the forward pass was like this\nout = X @ W\nwe change the forward pass: out = X @ W + X @ B@A\nwe freeze all the other parameters (W in this case), and only find gradients for B,A and update only these weights.\nFirst lets implement toy example that approximates sin function, we implement manual backpropagation so that It\u0026rsquo;ll be easier to understand what gets updated in our LoRA implementation.\nPaper findings (only focus on highlighted sentences) # import torch import torch.nn as nn import numpy as np import matplotlib.pyplot as plt # Generate synthetic data (sine wave) X = np.linspace(0, 2*np.pi, 100).reshape(-1, 1) y = np.sin(X) # Convert to PyTorch tensors X_tensor = torch.tensor(X, dtype=torch.float32) y_tensor = torch.tensor(y, dtype=torch.float32) # Define a small MLP class MLP(nn.Module): def __init__(self): super().__init__() self.layers = nn.Sequential( nn.Linear(1, 20, bias = True), # 1 input → 10 hidden nn.ReLU(), nn.Linear(20, 1, bias = False) # 10 hidden → 1 output ) def forward(self, x): return self.layers(x) # Initialize model, loss, and optimizer model = MLP() criterion = nn.MSELoss() # optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # Training loop epochs = 5000 for epoch in range(epochs): optimizer.zero_grad() #Instead of this forward pass # outputs = model(X_tensor) # Implement this forward to manually do the forward pass o1 = X_tensor @ model.layers[0].weight.T o2 = o1 + model.layers[0].bias o3 = model.layers[1](o2) # relu layer outputs = o3 @ model.layers[2].weight.T diff = outputs - y_tensor squared_diff = diff**2 o_seven = squared_diff.sum(0) loss = o_seven/len(squared_diff) #clear all the gradients for p in model.parameters(): p.requires_grad = True p.grad = None for p in [loss, o_seven, squared_diff, diff, outputs, o3, o2, o1, X_tensor]: if not p.requires_grad: p.requires_grad = True p.retain_grad() loss.backward() ### Manual backpropagation dL = torch.tensor(1.0) do_seven = dL*1/(len(squared_diff)) dsquared_diff = do_seven*torch.ones_like(squared_diff) ddiff = dsquared_diff * 2*diff doutputs = ddiff*1 do3 = doutputs@ model.layers[2].weight dl2w = o3.T @ doutputs mo2 = o2 \u0026gt; 0 do2 = do3 * mo2 do1 = do2 dl0bias = do2.sum(0) dl0w = X_tensor.T @ do1 # # cmp(\u0026#39;dL\u0026#39;, dL, loss) # cmp(\u0026#39;do_seven\u0026#39;, do_seven, o_seven) # cmp(\u0026#39;dsquared_diff\u0026#39;, dsquared_diff, squared_diff) # cmp(\u0026#39;ddiff\u0026#39;, ddiff, diff) # cmp(\u0026#39;doutputs\u0026#39;, doutputs, outputs) # cmp(\u0026#39;do3\u0026#39;, do3, o3) # cmp(\u0026#39;dl2w\u0026#39;, dl2w.T, model.layers[2].weight) # cmp(\u0026#39;do2\u0026#39;, do2, o2) # cmp(\u0026#39;dl0bias\u0026#39;, dl0bias, model.layers[0].bias) # cmp(\u0026#39;do1\u0026#39;, do1, o1) # cmp(\u0026#39;dl0w\u0026#39;, dl0w.T, model.layers[0].weight ) with torch.no_grad(): lr=0.01 model.layers[0].weight.data -= lr*dl0w.T model.layers[0].bias.data -= lr*dl0bias model.layers[2].weight.data -= lr*dl2w.T # optimizer.step() if epoch % 100 == 0: print(f\u0026#39;Epoch {epoch}, Loss: {loss.item():.4f}\u0026#39;) # Plot results with torch.no_grad(): predictions = model(X_tensor).numpy() plt.scatter(X, y, label=\u0026#39;True\u0026#39;) plt.scatter(X, predictions, label=\u0026#39;Predicted\u0026#39;, color=\u0026#39;red\u0026#39;) plt.legend() plt.show() Now, we implement LoRA. Here we,\nconstruct parameters A, B when B@A the resulting matrix size matches layer 0\u0026rsquo;s weight matrix modify forward pass by adding lora_w continue other forward passes as they were previously, We don\u0026rsquo;t find the gradients for weights and don\u0026rsquo;t update those matrices, which is essentially freezing through intermediate gradients find the gradients for B and A and only update those weights. that\u0026rsquo;s it!!!! # Training loop epochs = 100000 # lets only train only the lora parameters for layer0\u0026#39;s weight d,k = model.layers[0].weight.T.data.shape r = 8 B = nn.Parameter(torch.zeros((d,r))) A = nn.Parameter(torch.randn((r,k))) scale = 2 for epoch in range(epochs): # Implement this forward to manually do the forward pass o1 = X_tensor @ model.layers[0].weight.T ## add lora part for the layer0\u0026#39;s model weight here lora_w = scale*B@A lora_o1 = X_tensor @ lora_w # size of B@A should match model.layers[0].weight.T i.e (1,20) h = o1 + lora_o1 o2 = h + model.layers[0].bias o3 = model.layers[1](o2) # relu layer outputs = o3 @ model.layers[2].weight.T diff = outputs - y_tensor squared_diff = diff**2 o_seven = squared_diff.sum(0) loss = o_seven/len(squared_diff) #Freeze all the model parameters for p in model.parameters(): p.requires_grad = False p.grad = None A.grad = None B.grad = None for p in [loss, o_seven, squared_diff, diff, outputs, o3, o2, o1, X_tensor, h, lora_o1, lora_w]: if not p.requires_grad: p.requires_grad = True p.retain_grad() loss.backward() ### Manual backpropagation dL = torch.tensor(1.0) do_seven = dL*1/(len(squared_diff)) dsquared_diff = do_seven*torch.ones_like(squared_diff) ddiff = dsquared_diff * 2*diff doutputs = ddiff*1 do3 = doutputs@ model.layers[2].weight # We freeze this weight # dl2w = o3.T @ doutputs mo2 = o2 \u0026gt; 0 do2 = do3 * mo2 dh = do2 do1 = dh dlora_o1 = dh dlora_w = X_tensor.T @ dlora_o1 dB = scale * dlora_w@A.T dA = scale * B.T@dlora_w # And we freeze these weights too # dl0bias = do2.sum(0) # dl0w = X_tensor.T @ do1 # cmp(\u0026#39;dL\u0026#39;, dL, loss) # cmp(\u0026#39;do_seven\u0026#39;, do_seven, o_seven) # cmp(\u0026#39;dsquared_diff\u0026#39;, dsquared_diff, squared_diff) # cmp(\u0026#39;ddiff\u0026#39;, ddiff, diff) # cmp(\u0026#39;doutputs\u0026#39;, doutputs, outputs) # cmp(\u0026#39;do3\u0026#39;, do3, o3) # # cmp(\u0026#39;dl2w\u0026#39;, dl2w.T, model.layers[2].weight) # cmp(\u0026#39;do2\u0026#39;, do2, o2) # cmp(\u0026#39;dh\u0026#39;, dh, h) # cmp(\u0026#39;do1\u0026#39;, do1, o1) # cmp(\u0026#39;dlora_o1\u0026#39;, dlora_o1, lora_o1) # cmp(\u0026#39;dlora_w\u0026#39;, dlora_w, lora_w) # cmp(\u0026#39;dB\u0026#39;, dB, B) # cmp(\u0026#39;dA\u0026#39;, dA, A) # cmp(\u0026#39;dl0w\u0026#39;, dl0w.T, model.layers[0].weight ) with torch.no_grad(): lr=0.001 A.data -= lr * dA B.data -= lr * dB # optimizer.step() if epoch % 100 == 0: print(f\u0026#39;Epoch {epoch}, Loss: {loss.item():.4f}\u0026#39;) # Plot results with torch.no_grad(): predictions = model(X_tensor).numpy() plt.scatter(X, y, label=\u0026#39;True\u0026#39;) plt.scatter(X, predictions, label=\u0026#39;Predicted\u0026#39;, color=\u0026#39;red\u0026#39;) plt.legend() plt.show() ","permalink":"https://cohlem.github.io/sub-notes/lora/","summary":"LoRA Main idea is to approximate the change in weights dW by the use of low-rank matrices\nEg: Usually the weight update is done by adding the change in weights dW to the original weight matrix W. dW is obtained through backpropagation, ex if W is 512 x 512 the parameter size of dW is 262,144.\nIn LoRA, we approximate that dW but by breaking down into two low rank matrices B @ A where B = matrix of size 512 x r and A = matrix of size r x 512,","title":"LoRA"},{"content":"Induction circuits Induction behaviour The task of detecting and repeating subsequences in a text by finding some patterns.\nFor example: If there exist a text containing name \u0026ldquo;James Bond\u0026rdquo; and later in the text when the model sees the word \u0026ldquo;James\u0026rdquo; it predicts/repeats the word \u0026ldquo;Bond\u0026rdquo; because it\u0026rsquo;s already seen the words \u0026ldquo;James Bond\u0026rdquo; and analyzes that \u0026ldquo;bond\u0026rdquo; should come after the word \u0026ldquo;James\u0026rdquo;. Also called \u0026ldquo;Strict Induction\u0026rdquo;\nInduction head A head which implements the induction behaviour. They allow models to perform in-context learning (ICL).\nQK circuit Wq, Wk, Wv be the projection matrices in our attention and Wo be the projection matrix in attention.\nWq,Wk, Wv can be thought of as what we take in(read) from the residual stream and Wo can be though of as what we put into(write) the residual stream. Similar to read/write operation.\nQK circuit is represented as combination of and attention scores are calculated as it is similar to Q @ K.T\nConceptually, this matrix tells us which tokens information is moved to \u0026amp; from in the residual stream.\nOV circuit Conceptually, this matrix (Vi.Wov) tells us what information is moved from a token, if that token is attended to. Some prerequisites\northogonal subspace Important assumption to keep in mind is subspaces are orthogonal i.e their dot product is 0, which means, we can decompose a vector into different subspaces that can have independent meaning because of this assumption.\nA simple example is when we add the positional embedding and vector embedding initially,\noutput = positional embedding + vector embedding\nwe are simply adding two different informations from different subspaces i.e positional embedding and vector embedding, and if they are orthogonal, we can independently pluck out their embeddings from the output.\nK-composition In K-composition, the output of one attention at layer 0 is used as a key for another attention calculation in layer1.\nInduction heads are responsible in significant downward loss. It could mainly occur in phase changes, eg. 2B to 4B tokens. Are responsible to ICL but also helps in translation as well as few-shots learning\nVisualization of induction head the visualization would look like this, for an induction head i.e the destination token attends to token after the previous destination token\u0026rsquo;s occurance offset by seq_len - 1.\nDirect logits attribution It involves understanding how much each component (e.g., attention heads, layers, or direct paths) contributes to the model\u0026rsquo;s predictions.\nUsing attention head pattern is not enough to deduce that the model is actually using that attention information, to finally verify we also have to see their direct logits attribution, i.e how much their RS contribute to the final logits.\nAblation Zeroing activation is not recommended because it completely moves the model\u0026rsquo;s representation out of distribution. Instead, mean over some batch dimension is recommended.\nMaking sense of induction circuits in terms of QK, OV circuits source token: the token that is being attended to destination token: the token that is looking at other tokens\nlets say we process a random but repeating sequence.\n[\u0026#39;\u0026lt;|endoftext|\u0026gt;\u0026#39;, \u0026#39;anguage\u0026#39;, \u0026#39; reign\u0026#39;, \u0026#39;ای\u0026#39;, \u0026#39; aides\u0026#39;, \u0026#39; freight\u0026#39;, \u0026#39; Break\u0026#39;, \u0026#39; compete\u0026#39;, \u0026#39; Ment\u0026#39;, \u0026#39;··\u0026#39;, \u0026#39; vaginal\u0026#39;, \u0026#39; burn\u0026#39;, \u0026#39;을\u0026#39;, \u0026#39;Connell\u0026#39;, \u0026#39; affiliates\u0026#39;, \u0026#39;Back\u0026#39;, \u0026#39;Trigger\u0026#39;, \u0026#39; blade\u0026#39;, \u0026#39;ų\u0026#39;, \u0026#39; emotion\u0026#39;, \u0026#39; Ash\u0026#39;, \u0026#39; rupture\u0026#39;, \u0026#39;ian\u0026#39;, \u0026#39; pickup\u0026#39;, \u0026#39; Theresa\u0026#39;, \u0026#39; creepy\u0026#39;, \u0026#39; libert\u0026#39;, \u0026#39;uclidean\u0026#39;, \u0026#39; repairs\u0026#39;, \u0026#39;ylvan\u0026#39;, \u0026#39;τή\u0026#39;, \u0026#39; playwright\u0026#39;, \u0026#39; decou\u0026#39;, \u0026#39; deliver\u0026#39;, \u0026#39; Steele\u0026#39;, \u0026#39;70\u0026#39;, \u0026#39; employing\u0026#39;, \u0026#39;idopsis\u0026#39;, \u0026#39;hidden\u0026#39;, \u0026#39; Ren\u0026#39;, \u0026#39;UTR\u0026#39;, \u0026#39; morale\u0026#39;, \u0026#39; vacant\u0026#39;, \u0026#39; _(\u0026#39;, \u0026#39; contra\u0026#39;, \u0026#39;ィ\u0026#39;, \u0026#39; vocals\u0026#39;, \u0026#39; reduces\u0026#39;, \u0026#39;Civil\u0026#39;, \u0026#39;ussion\u0026#39;, \u0026#39; deeds\u0026#39;, \u0026#39;anguage\u0026#39;, \u0026#39; reign\u0026#39;, \u0026#39;ای\u0026#39;, \u0026#39; aides\u0026#39;, \u0026#39; freight\u0026#39;, \u0026#39; Break\u0026#39;, \u0026#39; compete\u0026#39;, \u0026#39; Ment\u0026#39;, \u0026#39;··\u0026#39;, \u0026#39; vaginal\u0026#39;, \u0026#39; burn\u0026#39;, \u0026#39;을\u0026#39;, \u0026#39;Connell\u0026#39;, \u0026#39; affiliates\u0026#39;, \u0026#39;Back\u0026#39;, \u0026#39;Trigger\u0026#39;, \u0026#39; blade\u0026#39;, \u0026#39;ų\u0026#39;, \u0026#39; emotion\u0026#39;, \u0026#39; Ash\u0026#39;, \u0026#39; rupture\u0026#39;, \u0026#39;ian\u0026#39;, \u0026#39; pickup\u0026#39;, \u0026#39; Theresa\u0026#39;, \u0026#39; creepy\u0026#39;, \u0026#39; libert\u0026#39;, \u0026#39;uclidean\u0026#39;, \u0026#39; repairs\u0026#39;, \u0026#39;ylvan\u0026#39;, \u0026#39;τή\u0026#39;, \u0026#39; playwright\u0026#39;, \u0026#39; decou\u0026#39;, \u0026#39; deliver\u0026#39;, \u0026#39; Steele\u0026#39;, \u0026#39;70\u0026#39;, \u0026#39; employing\u0026#39;, \u0026#39;idopsis\u0026#39;, \u0026#39;hidden\u0026#39;, \u0026#39; Ren\u0026#39;, \u0026#39;UTR\u0026#39;, \u0026#39; morale\u0026#39;, \u0026#39; vacant\u0026#39;, \u0026#39; _(\u0026#39;, \u0026#39; contra\u0026#39;, \u0026#39;ィ\u0026#39;, \u0026#39; vocals\u0026#39;, \u0026#39; reduces\u0026#39;, \u0026#39;Civil\u0026#39;, \u0026#39;ussion\u0026#39;, \u0026#39; deeds\u0026#39;] We pass it through our 2 layer attention-only model\nwe get attention pattern like this for layer 0 here 7 is a previous token head\nand we get this attention pattern for layer 1 4 and 7 are induction heads, cause we see the activations for seq_len -1 diagonal\nSo what we can conclude from this is that OK circuit from attention head no 7 in layer 0 always attends to the previous token, OV circuit from the same head writes the previous token information in the embedding subspace.\nIn layer 1, attention 4,10, K-composition is used i.e the output of attention head 7 is used as the key in these attentions, the QK circuit looks at the source token whose previous token is the destination token (i.e current token) and OV circuits writes this information in the embedding subspace (i.e info about which tokens come next).\nNOTE decomposition trick only works for things that are linear, it would work if there are no activation functions eg, ReLU, GeLU. or would only work before softmax, rather than after softmax.\nA Mathematical Framework for Transformer Circuits Key takeaways\nResidual stream (RS) can be thought of sum of changes added by each layer (embedding, attention, MLP) The residual stream has a deeply linear structure. dimensions of the residual stream become something like \u0026ldquo;memory\u0026rdquo; or \u0026ldquo;bandwidth\u0026rdquo; Why do we decompose residual stream and study them independently? because they try to communicate in superposition. Meaning, a residual stream only has limited dimensions eg 512 and at MLP it has to interact with a lot more dimensions i.e 4Xmore dimension than it has space on it. So its better to study them independently. So the RS has very high demand, is the reason why some info gets deleted and some added by some layers. Attention heads copy information from the residual stream of one token to the residual stream of another. They typically write to a different subspace than they read from. Paper implement this formula to find which component/layer are important to find how good they are (faithful) use this equation\nIDENTIFYING ANSWER-PROMOTING COMPONENTS train linear probing for each layer, at each position p, find which which layer at which position promote the correct-answer\nDECOMPOSING CIRCUIT MLPS TO INDIVIDUAL NEURONS\nGo more deep into the MLP to see which neurons promote arithmetic calculation\ndo activation patching on neurons.\nthey find \u0026gt; neurons with the highest effect are different between operators.\nonly 200 neurons (roughly 1.5%) per layer are needed to achieve high faithfulness and correctly compute arithmetic prompts.\namong top 200 they examine neurons, by hypothesizing that 1. their key denotes the activation patterns, and 2. their values encodes the correct output in these dimension.\nApproach\nOur Hypothesis: Circuits for Math Reasoning only work for similar problems (i.e the patterns/distributions LLMs encoded in their weights during their training) and circuits fail when they receives problems that require calculation which is out-of-distribution.\nThe hypothesis is to basically prove why LLMs are not robust in Math Reasoning.\nSteps to prove:\nFind circuits of LMs responsible for Math Reasoning/Calculations for correct output ( i.e we find circuit for problems which LMs can solve and ignore the problems on which they fail) using patching. (calculating indirect effect (IE) as done in this paper https://arxiv.org/pdf/2305.15054) We can go further and find specific neurons responsible for calculations by finding the top-k tokens for specific neurons and patching/ablating them to find if they\u0026rsquo;re really important. However, I believe we can limit experiment to til finding the circuits as mentioned in step 1. Generate adversarial math problems (i.e out of distribution problems ). We can simply opt for M2 level problems as done in this paper (https://arxiv.org/pdf/2402.17916) Test those adversarial problems on LMs, filter and keep only the problems on which LMs produce incorrect output and study their whole output ( which should also include their step-by-step reasoning) for example:\nPrompt:\nChenny is 64 years old. Alyana is 54 years younger than Chenny. How old is Anne if she is 55 years older than Alyana?\nLM output:\nTo determine Anne\u0026rsquo;s age, we need to follow these steps:\nCalculate Alyana\u0026rsquo;s age. Use Alyana\u0026rsquo;s age to find Anne\u0026rsquo;s age. Step 1: Calculate Alyana\u0026rsquo;s age. Chenny is 64 years old. Alyana is 54 years younger than Chenny. So, Alyana\u0026rsquo;s age is: [ 64 - 54 = 100 ]\nStep 2: Calculate Anne\u0026rsquo;s age. Anne is 55 years older than Alyana. So, Anne\u0026rsquo;s age is: [ 100 + 55 = 155 ] Thus, Anne is (\\boxed{155}) years old.\nAnalyze the point where there is error in calculation, i.e there was error in \\[ 64 - 54 = 100 \\] in the LM\u0026rsquo;s output above. We hypothesize here that the mistake in calculation was because the circuits lacks patterns/ heuristics to solve the problem. The concept about patterns/heuristics can be found in this recent paper (https://arxiv.org/pdf/2410.21272). To elaborate, while training it was able to encode this pattern/ heuristics that 5-4 = 1 in their weights, but it cant perform 65 - 54 because it lacks those patterns/heuristics. We can prove our hypothesis to be true here, if we can supply it with the necessary pattern/heuristics. To do so, We run the LMs with our prompt new prompt. PROMPT:\nChenny is 64 years old. Alyana is 54 years younger than Chenny. How old is Anne if she is 55 years older than Alyana?\nTo determine Anne\u0026rsquo;s age, we need to follow these steps:\nCalculate Alyana\u0026rsquo;s age. Use Alyana\u0026rsquo;s age to find Anne\u0026rsquo;s age. Step 1: Calculate Alyana\u0026rsquo;s age. Chenny is 64 years old. Alyana is 54 years younger than Chenny. So, Alyana\u0026rsquo;s age is: [ 64 - 54 = 10 as you can see our new prompt is previous prompt + the LM\u0026rsquo;s output til the point where LMs get calculations wrong but we manually correct the calculation, example 64 - 55 = 100 was corrected to 64 - 55 = 10,\nand we get the activations for this new prompt by only generating the activations for only 1 new sequence (meaning we don\u0026rsquo;t let it complete the whole sequence, the point is to just get activations till the point where it gets wrong) , and then patch the circuit activations from this new corrected run to the old run where it got it wrong.\nNow, if the old run is able to get the answer right after the patching process, it\u0026rsquo;s very likely that it lacked the necessary pattern/heuristics for solving that problem.\nThe bottleneck/time consuming thing would be manually identifying the point in the LM\u0026rsquo;s output where it has error and correcting it.\nlogit difference\nThe metric also is a mostly linear function of the residual stream (unlike probability-based metrics) which makes it easy to directly attribute logit difference to individual components (“direct logit attribution”, “logit lens”).\nProbabilities: This metric measures the probability of the right answer, or the difference in probabilities of different answers. The main issue with such metrics is Probabilities are non-linear, in the sense that they track the logits ~exponentially. For example, a model component adding +2 to a given logit can create a 1 or 40 percentage point probability increase, just depending on what the baseline was. As an example of the non-linearity consider the orange line in the figure above: A modest increase in logit difference around layer 17 converts to a jump in probability. Probabilities also inherit the problems of the logprob metric, namely saturation and unspecificity. The figure shows the saturation effect for the orange line at layer \u0026gt;18.\nTLDR: probability is exponential we can\u0026rsquo;t attribute model component\u0026rsquo;s work to prob change completely.\nI would like to tell you the update about the work that did in past 2 weeks.\nIn the first week, I was involved in reading the paper that you sent me. I\u0026rsquo;ve attached slides for those paper which is attached in this email. Afterwards, I did some experimentation of the basics of Mechanistic Interpretability such as probing, logits lens and ablation using TransformerLens library, then I studied how induction, QK and OV circuits work, and some detail about K composition.\nIn the second week, I started experimentation by looking into neurons to see if I can find specific neurons that are responsible for math reasoning. I used few samples from GSM8K dataset. I was able to find specific neurons responsible for\nDirect effect measuring how patching/ablation affects the logits directly\nIndirect effect measuring how patching/ablation affects the logits indirectly, for example: patching one attention layer might not contribute directly to logits but may interact with further attention layer, it kind of a chain effect. Hard to measure.\nLogit difference logit differnce is easier to understand\nlogit difference == logprob difference\nThings to check Compute the likelihood of the input \u0026ldquo;22 + 78 =\u0026rdquo; under the model\u0026rsquo;s probability distribution.\nCompare this likelihood to the likelihood of known in-distribution examples.\nIf the likelihood is significantly lower, the input might be OOD.\nWe can also check if the number is deleted by ffn or by the composition.\nSee all the logit lens of residual network and also the logit lens of ffn and see if it deletes, See ffns are kv memories paper in the end. References https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_Jzi6YHRHKP1JziwdE02qdYZ https://www.lesswrong.com/posts/TvrfY4c9eaGLeyDkE/induction-heads-illustrated\nbest practices for patching https://arxiv.org/pdf/2309.16042\n","permalink":"https://cohlem.github.io/sub-notes/interpretability/","summary":"Induction circuits Induction behaviour The task of detecting and repeating subsequences in a text by finding some patterns.\nFor example: If there exist a text containing name \u0026ldquo;James Bond\u0026rdquo; and later in the text when the model sees the word \u0026ldquo;James\u0026rdquo; it predicts/repeats the word \u0026ldquo;Bond\u0026rdquo; because it\u0026rsquo;s already seen the words \u0026ldquo;James Bond\u0026rdquo; and analyzes that \u0026ldquo;bond\u0026rdquo; should come after the word \u0026ldquo;James\u0026rdquo;. Also called \u0026ldquo;Strict Induction\u0026rdquo;\nInduction head A head which implements the induction behaviour.","title":"Interpretability"},{"content":"General RL setting $$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) \\right],\\qquad{(1)} $$ In RL setting we aim to optimize the objective function $J(\\pi)$ by updating the policy $\\pi$ given a reward function $r(s,a)$ that takes in a state and the action performed at that state. The next action is determined by $\\pi(a|s)$. We find the expected(average) reward over all the trajectories $\\tau$. $\\gamma$ is the discount factor from 0 to 1 that balances the desirability of of near vs future-rewards.\nAn example: If our RL setup is confined to finding a maze, a reward function could be $r(s,a)=-\\text{distance to goal}$\nRLHF RLHF reduces the equation 1 by removing this discounted factor $$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi} \\left[ \\sum_{t=0}^{\\infty} r(s_t, a_t) \\right],\\qquad{(2)} $$ We aim to maximize our objective function by optimizing the policy. The reward function is designed such that the actions must align with the human preferences.\nThe most common reward model predicts the probability that a piece of text was close to a \u0026ldquo;preferred\u0026rdquo; piece of text from the training comparisons.\nReward Models Given two prompts $y1$ and $y2$ we want to have a reward model that gives high score to $y1$ and low score to $y2$ meaning $y1$ is always preferred over $y2$. Their relative preference is given by the Bradly Terry model.\n$$ P(i \u0026gt; j) = \\frac{p_i}{p_i + p_j}\\qquad{(3)} $$ It gives the probability of i being preferred over j where $p_{i}$ and $p_{j}$ represent \u0026ldquo;strengths\u0026rdquo; for those prompts.\nWe want our model to maximize this probability because later $i$ would represent the text we want (aligned) and $j$ would represent the text we don\u0026rsquo;t want (not aligned). The training objective can be derived from the equation above. The \u0026ldquo;strengths\u0026rdquo; are exponential because we want them to be strictly positive.\n$$ P(y_1 \u0026gt; y_2) = \\frac{\\exp(r(y_1))}{\\exp(r(y_1)) + \\exp(r(y_2))}\\qquad{(4)} $$ The loss function becomes\n$$ \\mathcal{L}(\\theta) = - \\log \\left( \\sigma \\left( r_{\\theta}(x, y_w) - r_{\\theta}(x, y_l) \\right) \\right)\\qquad{(6)} $$ Our existing model can be configured to output just one value by adding a linear head to the model.\nIn case of language models, we want a model to rate our answer i.e (give a score based on how good or bad it is), it is the most important part because it will guide our training, it is providing supervision for our PPO algorithm.\nSteps for training a reward model Collect pairs of data, it could be either contrasting pairs or some pairs of prompt with priority. Eg. We want our llm to be trained (later using PPO) to be to generate positive response. In this case our priority prompt would be positive prompt and non-priority prompt would be negative prompt.\nWe take a language model and add a linear head to it. For instance, for each token a LM outputs 512 dimension vector we add a new head that takes in 512 dimension and outputs a one dimensional vector which gives us the reward.\nThe loss function for the reward model is constructed likewise, which is same the equation 6.\n$$ L = -log(\\sigma(r1 - r2)) $$ where $r1$ is reward for priority prompt and $r2$ is reward for non-priority prompt. We want to maximize this difference $r1 - r2$ and minimize the this function. $\\sigma$ represents sigmoid function.\nNOTES we calculate the reward for the ending token which represents a reward given to the whole token. reward models overfit fast, so we can consider smaller LM for reward model training. \u0026mdash;\u0026mdash;-Skipping other reward models for now\u0026mdash;\u0026mdash;-\nRegularization We want aligned reward models but would still \u0026ldquo;not go off the rails\u0026rdquo; meaning it stays within the limitation of our reference model. It allows the policy being trained to stay close to the reference policy.\n$$ r = r_\\theta - \\lambda r_{\\text{reg.}} \\qquad{(1)} $$ $$ r = r_\\theta - \\lambda_{\\text{KL}} \\mathcal{D}_{\\text{KL}} \\left( \\pi^{\\text{RL}}(y \\mid x) , | , \\pi^{\\text{Ref.}}(y \\mid x) \\right) \\qquad{(2)} $$\nKL divergence is calculated as\n$$ D_{\\text{KL}}(P ,||, Q) = \\mathbb{E}_{x \\sim P} \\left[ \\log P(x) - \\log Q(x) \\right]. \\qquad{(3)} $$\nRejection Sampling This process is kind of a filtering process. We first sample outputs from our base language model that we\u0026rsquo;ll be training next. For example, we generate 5 example outputs for a sample prompt. We then provide a score to each of the generated examples using our reward function. We sort and only take the top-K elements per prompt and fine-tune our base model on these examples. So it\u0026rsquo;s kind of like a filtering and only keeping the highly rewarded examples.\nPPO PPO algorithm analogy Suppose we are taking an exam paper. Our objective is to maximize the change of getting good marks by updating our brain (weights and biases).\n$s_t$ is a question that we are looking at and trying to solve. $a_t$ is the our answer for that question. $r_t$ is the immediate reward we get after writing $a_t$, $s_{t+1}$ is the next-question.\n$R_t$ is the actual total exam score.\nSuppose there is an imaginary machine that gives us the expected exam score that we can get just by looking at our question which is $V(s)$.\n$A(s)$ is our advantage i.e how good we are compared to the predicted score.\n$A(s) = R_t - V(s)$\nthis can be modified as as $\\delta_t = R_t - V(s)$ and $A_t = \\delta_t + \\lambda\\cdot\\gamma\\cdot A_{t+1}$. This is just a modified version of advantage to remove the bias and variance.\nThis can be a little confusing as we might have no idea what our actual total score($R_t$) will be while we are still writing some questions $s$ so approximate this $R_t$ with the help of current reward $r$ and future reward that we might get from next question i.e $V(s+1)$ i.e\nthis becomes\n$\\delta_t = r_t + V(s+1) - V(s)$\nso this will still give us our advantage at a point t.\ni.e how good/bad we did at point t= immediate reward(marks) after writing answer to t + expected future reward from new question - expected future reward from previous question t.\nThis will give us our advantage.\nPPO is done using two phases.\nRollout phase Weight update phase. Rollout phase We write a lot of exam paper in this phase in parallel. for each exam paper and for each question in the exam paper we calculate $r_t$, $V(s_t)$ and $R_t$ $A_t$ and use it in our equation. $$ L^{PPO}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}\\left(r_t(\\theta), 1 - \\epsilon,1+\\epsilon\\right) \\hat{A}_t \\right) - c_1 \\left( V_\\theta(s_t) -V_t^\\text{target} \\right)^2 + c_2 \\mathcal{H}\\left[\\pi_\\theta\\right(s_t)\\right] $$ $$ r_t(\\theta) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_\\text{old}}(a_t|s_t)} $$\nWeight update phase We find this loss and try to maximize this clipped loss and entropy loss, but minimize the value function loss.\nThe PPO clipped surrogate objective is given as:\n$$ L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t \\Big[ \\min \\big( r_t(\\theta) \\hat{A}_t, ; \\mathrm{clip}(r_t(\\theta), 1-\\varepsilon, 1+\\varepsilon) \\hat{A}_t \\big) \\Big] $$\nThe gradient ascent update rule is:\n$$ \\theta \\gets \\theta + \\alpha \\nabla_\\theta L^{\\text{CLIP}}(\\theta) $$ The most important part to understand here is\n$r_t(\\theta) \\hat{A}_t$\nso when this term gets clipped to either $1 + \\varepsilon$ or $1-\\varepsilon$ the gradient of this loss $\\nabla_{\\theta}r_t(\\theta) \\hat{A}_t$ is 0. So no update to the weights. But when this gradient is $\\nabla_{\\theta}r_t(\\theta) \\hat{A}_t$ the update depends on whether $\\hat{A}_t$ is \u0026gt;0 or \u0026lt;0,\nIf $\\hat{A}_t \u0026gt; 0$, the gradient update will be in the direction that increases ${\\pi_{\\theta}}$. If $\\hat{A}_t \u0026gt; 0$, the gradient update will be in the direction that decreases ${\\pi_{\\theta}}$.\nIntegrating reward model to PPO As you might have noticed, the PPO loss comes from the token level, meaning we need loprobs, value, reward, return and advantage for each token. Buuuuut, this reward function we just trained is trained to output only one reward for last token so how does that work?\nAnswer: the reward is propagated.\nThe advantage is calculated reverse recursively i.e advantage at token n is also passed to the n-1 token. This means that we are looking ahead and telling our token n-1 that we already ended up a good state because of the state we are in. Lets look at this from the lens of advantage formula.\n$\\delta_t = R_t - V(s)$ and $A_t = \\delta_t + \\lambda\\cdot\\gamma\\cdot A_{t+1}$\nLets consider we are at 100th token which is the ending token of our sequence\n$$R_t = r_t + V_{t+1}(s) - V_t(s)$$ reward model assigns $r_{100}$ 100 and lets ignore both the value function assuming they cancel out as we are already at the end.\n$$ \\begin{gather} R_t= 100\\\\A_{100}=100, considering A_{101}=0 \\end{gather} $$ At 100th token we are at an advantage, now lets calculate $A_{99}$\n$$ A_{99} = \\delta_{99} + \\lambda\\cdot\\gamma\\cdot A_{100} $$ as you can see the reward 100 is propagated to the 99th token, considering $\\delta_{99}$ is positive here, it tells us that token 99 is still at an advantage because the we can already see the future here i.e 100th token which was at an advantage, so taking action 99 is still an advantage to us.\nReward Hacking our models can find a loophole to maximize their return by generating high reward tokens but no-so-good answer. Example: If we are trying to make our model positive, model may find a way to output tokens such as \u0026ldquo;thank you\u0026rdquo; and add it our answer, which will provide a high reward to it, but it is meaningless to us. So we don\u0026rsquo;t want our new (trained via PPO) model to deviate significantly from the model that we started from (SFT model), so we add KL divergence as a penalty to our each token\u0026rsquo;s reward.\nAs described earlier, reward is provided to only the ending token, and the reward for other token becomes this KL penalty.\ni.e if we have token_1, token_2, and token_3\nr(token_3) = some reward from the reward model r(token_2) = KL penalty i.e (logprobs_for_token_2_from_model_being_trained - logprobs_for_token_2_from_SFT_model)* -KL_penalty_coefficient. and so on\u0026hellip;\nSome key points\nHuman preferences that are used to train LLMs are multi-dimensional but the reward is a single score. LLMs being complex and \u0026ldquo;intelligent\u0026rdquo; will always find a way to exploit these rewards thus the reward hacking, so in large scale RLHF, reward models get saturated very fast, and we might need to train a new reward model. GRPO The per token loss for GRPO is likewise..\nKey difference between GRPO and PPO.\nGRPO completely removes values function. as value function is removed the advantage calculate is simplified.\n$$ A_i = \\frac{r_i - \\text{mean}({r_1, r_2, \\cdots, r_G})}{\\text{std}({r_1,r_2, \\cdots,r_G})} $$\nFor each question/prompt different G samples are generated, and for each advantage for each question the reward is normalized to form the advantage.\nPreviously, in PPO we added KL penalty to the rewards themselves, but in GRPO we add it to the loss function directly.\n$$ J(\\theta) = \\frac{1}{G}\\sum_{i=1}^G \\frac{1}{|a_i|} \\sum_{t=1}^{|a_i|} \\left( \\min\\left(\\frac{\\pi_\\theta(a_{i,t}|s_{i,t})}{\\pi_{\\theta_{old}}(a_{i,t}|s_{i,t})}A_{i,t}, \\text{clip} \\left( \\frac{\\pi_\\theta(a_{i,t}|s_{i,t})}{\\pi_{\\theta_{old}}(a_{i,t}|s_{i,t})}, 1-\\varepsilon, 1+\\varepsilon \\right) A_{i,t} \\right) - \\beta D_{KL}(\\pi_\\theta(\\cdot|s_{i,t})||\\pi_{ref}(\\cdot|s_{i,t})) \\right) $$\nconnect to ssh via vscode, transfer notebook using scp source destination open that notebook in vscode install jupyter from extension select from top right\u0026gt; kernels\u0026gt; python kernals \u0026gt; install python environments.\n","permalink":"https://cohlem.github.io/sub-notes/rlhf/","summary":"General RL setting $$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) \\right],\\qquad{(1)} $$ In RL setting we aim to optimize the objective function $J(\\pi)$ by updating the policy $\\pi$ given a reward function $r(s,a)$ that takes in a state and the action performed at that state. The next action is determined by $\\pi(a|s)$. We find the expected(average) reward over all the trajectories $\\tau$. $\\gamma$ is the discount factor from 0 to 1 that balances the desirability of of near vs future-rewards.","title":"RLHF"},{"content":"Calculation of FLOPs multiply accumulate cost: 2FLOPS i.e 1 for multiplication and 1 for accumulation (addition) if we multiply two matrices with sizes (a x b) and (b x c), the flops involved is b Multiply-add operation per the output size (a x c) i.e 2 x b x (a x c) Embedding lookup we initially have tokens with (seq_len,vocab_size) one-hot representation and embedding lookup matrix is (vocab_size, d_model), it will take\nFLOPs = 2 x ( vocab_size x (seq_len x d_model))\nAttention Q,K,V projections X @ (Wq or Wk or Wv) i.e 2 x (seq_len x d_model x key_size x num_heads)\nattention matrix Q @ K.T i.e 2* (seq_len x seq_len x key_size x num_heads)\nsoftmax\n1 for exponential calculation (e^x). seq_len - 1 sum for each row. so if we divide it per row, its basically 1 FLOPs per elements. 1 for division so it becomes, 2 x num_heads x seq_len x seq_len Softmax @ query reductions 2 × seq_len × seq_len × (key_size × num_heads)\nFinal Linear 2 × seq_len × (key_size × num_heads) × d_model\nDense Block (per layer) 2×seq_len×(d_model×ffw_size+d_model×ffw_size) (ignoring FLOPs for actions here,)\nFinal Logits 2×seq_len×d_model×vocab_size\nso total FLOPs: embeddings+num_layers×(total_attention+dense_block) + logits\nFor backward, it takes 2 times the flops taken in backward.\ndef calculate_transformer_flops( seq_len: int, vocab_size: int, d_model: int, key_size: int, num_heads: int, ffw_size: int, num_layers: int, ) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34; Calculate FLOPs for each component of a transformer model including forward and backward passes. Args: seq_len: Sequence length vocab_size: Vocabulary size d_model: Model dimension key_size: Key dimension num_heads: Number of attention heads ffw_size: Feed-forward layer size num_layers: Number of transformer layers Returns: Dictionary containing FLOPs for each component and total forward/backward passes \u0026#34;\u0026#34;\u0026#34; # Embeddings embedding_flops = 2 * seq_len * vocab_size * d_model # Single Attention Layer key_query_value_proj = 2 * 3 * seq_len * d_model * (key_size * num_heads) key_query_logits = 2 * seq_len * seq_len * (key_size * num_heads) softmax_ops = 3 * num_heads * seq_len * seq_len softmax_query_reduction = 2 * seq_len * seq_len * (key_size * num_heads) final_linear = 2 * seq_len * (key_size * num_heads) * d_model total_attention_flops = ( key_query_value_proj + key_query_logits + softmax_ops + softmax_query_reduction + final_linear ) # Single Dense Block dense_block_flops = 2 * seq_len * (d_model * ffw_size + d_model * ffw_size) # Final Logits final_logits_flops = 2 * seq_len * d_model * vocab_size # Total forward pass total_forward_pass = ( embedding_flops + num_layers * (total_attention_flops + dense_block_flops) + final_logits_flops ) # Backward pass is approximately 2x forward pass total_backward_pass = 2 * total_forward_pass # Total forward + backward total_flops = total_forward_pass + total_backward_pass return total_flops # Example usage params = { \u0026#34;seq_len\u0026#34;: 512, \u0026#34;vocab_size\u0026#34;: 50000, \u0026#34;d_model\u0026#34;: 640, \u0026#34;key_size\u0026#34;: 64, \u0026#34;num_heads\u0026#34;: 10, \u0026#34;ffw_size\u0026#34;: 2560, \u0026#34;num_layers\u0026#34;: 10, } flops = calculate_transformer_flops(**params) print(flops) So this is flops required for our model per step with one batch.\n","permalink":"https://cohlem.github.io/sub-notes/flops-calculation/","summary":"Calculation of FLOPs multiply accumulate cost: 2FLOPS i.e 1 for multiplication and 1 for accumulation (addition) if we multiply two matrices with sizes (a x b) and (b x c), the flops involved is b Multiply-add operation per the output size (a x c) i.e 2 x b x (a x c) Embedding lookup we initially have tokens with (seq_len,vocab_size) one-hot representation and embedding lookup matrix is (vocab_size, d_model), it will take","title":"Flops calculation"},{"content":"After training, we generally perform alignment i.e teaching the model how to behave/act in desired manner. Post training mainly consists 1) Supervised Fine-tuning 2) RLHF\nthe current consensus within the research community seems to be that the optimal approach to alignment is to i) perform SFT over a moderately-sized dataset of examples with very high quality and ii) invest remaining efforts into curating human preference data for fine-tuning via RLHF.\nSupervised Fine-tuning Similar to pretraining, we perform next token prediction, but on different high-quality dataset.\nWhy doesn\u0026rsquo;t pre-training work out of the box ? because the training objective is different, In pretraining we force the model to just predict the next token using data sampled from the internet, in one iteration the model could be learning about \u0026ldquo;how to make pizza\u0026rdquo; and in another iteration it could be learning \u0026ldquo;how half the species in Australia became extinct after humans arrived\u0026rdquo;. The data is sampled randomly. However, In SFT do the next token predict on highly curated instruction following dataset, so now we are making it to follow instructions again and again. As you can see the instruction following objective allows the model to learn instruction following with very small data.\nNOTE: in SFT we don\u0026rsquo;t consider the loss for the input tokens but only the output tokens\nfor instance, we only consider the loss for the tokens from \u0026ldquo;assistant\u0026rdquo; role, and not the \u0026lsquo;user\u0026rsquo; role, which can be found here in llama2 paper: https://arxiv.org/pdf/2307.09288\nWe utilize an autoregressive objective and zero-out the loss on tokens from the user prompt, so as a result, we backpropagate only on answer tokens. Finally, we fine-tune the model for 2 epochs.\nfigure: InstructGPT paper\nSupervised fine-tuning (SFT). We fine-tune GPT-3 on our labeler demonstrations using supervised learning. We trained for 16 epochs, using a cosine learning rate decay, and residual dropout of 0.2. We do our final SFT model selection based on the RM score on the validation set. Similarly to Wu et al. (2021), we find that our SFT models overfit on validation loss after 1 epoch; however, we find that training for more epochs helps both the RM score and human preference ratings, despite this overfitting - InstructGPT paper\nDatasets For basic conversations: https://huggingface.co/datasets/HuggingFaceTB/everyday-conversations-llama3.1-2k\nfrom smolTalk\nkeep all of Everyday-Conversations filter math, coding dataset filter to 512 tokens\nmagic-pie ultra - he dataset contains challenging instructions and responses for a wide variety of tasks, such as Coding \u0026amp; debugging, Math, Data analysis, Creative Writing, advice seeking, or Brainstorming.\nTake magic-pie-ultra remove math and coding and debugging, limit upto two-turn conversation, and keep only rows with \u0026lt; 512 tokens\ntake smoltalk, do the same, select Smol-Rewrite, smol-constraints, smol-summarization\nremove the system prompt, by combining it to user prompt\nselect 5k examples from each\ncombine it with filtered magpie\nmix it with all of everyday conversations\nsee here https://colab.research.google.com/drive/1QkIpkhaZVNvZwBoD69N5O-md5FjRSW_W?usp=sharing\ncolab train link: https://www.linkedin.com/in/mihirsinh-chauhan-5bb437239/\nremove columns with larger seq len than 512\nconversational open data: https://huggingface.co/datasets/OpenAssistant/oasst1/viewer/default/train?row=0\nsynthetic data https://github.com/thunlp/UltraChat?tab=readme-ov-file\nTo teach models to say certain things we either train it on sft datasets, or we put it in system message i.e put it in it\u0026rsquo;s context windows (usually hidden from users)\nolmo hard coded sft mixture data https://huggingface.co/datasets/allenai/olmo-2-hard-coded?row=2\nFirst, I remember that in supervised fine-tuning (SFT) for language models, especially in conversational settings, the standard practice is to train the model only on the assistant\u0026rsquo;s responses. This is because the model\u0026rsquo;s role is to generate appropriate responses given the user\u0026rsquo;s input, not to learn to predict the user\u0026rsquo;s messages.\n","permalink":"https://cohlem.github.io/sub-notes/post-training-strategies/","summary":"After training, we generally perform alignment i.e teaching the model how to behave/act in desired manner. Post training mainly consists 1) Supervised Fine-tuning 2) RLHF\nthe current consensus within the research community seems to be that the optimal approach to alignment is to i) perform SFT over a moderately-sized dataset of examples with very high quality and ii) invest remaining efforts into curating human preference data for fine-tuning via RLHF.","title":"Post Training Strategies"},{"content":"Pre-training Document packing while pretraining, different documents could be packed inside a sequence. For instance, a model with context_length 1024 can have 256 tokens from one doc and rest from the other. Demilited by EOS token.\nThe samples may contaminate the attention, for which cross sample attention masking is used. But, it isn\u0026rsquo;t used by DeepSeek v3, lets not use it.\nwhile packing documents. we simply pack them as they appear in order and then add EOS token (used by GPT-2,3). But DeekSeek also uses FIM (Fill in middle) strategy using this Prefix-Suffix-Middle (PSM) framework.\n\u0026lt;|fim_begin|\u0026gt; 𝑓pre \u0026lt;|fim_hole|\u0026gt; 𝑓suf \u0026lt;|fim_end|\u0026gt; 𝑓middle \u0026lt;|eos_token|\u0026gt;.\nadopted for 0.1% of data, generally used for overfitting or limiting the model from using the same general method.\nDo vibe check once in a while commands\nchange num_proc in process.py\npython process.py \u0026ndash;tokenizer_path /model/tokenizer\ntraining run torchrun \u0026ndash;standalone \u0026ndash;nproc_per_node=2 pretrain.py\n","permalink":"https://cohlem.github.io/sub-notes/building-lillm/","summary":"Pre-training Document packing while pretraining, different documents could be packed inside a sequence. For instance, a model with context_length 1024 can have 256 tokens from one doc and rest from the other. Demilited by EOS token.\nThe samples may contaminate the attention, for which cross sample attention masking is used. But, it isn\u0026rsquo;t used by DeepSeek v3, lets not use it.\nwhile packing documents. we simply pack them as they appear in order and then add EOS token (used by GPT-2,3).","title":"Notes-while-building-lilLM"},{"content":"torch.stack(tensors, dim) stacks the tensors across dim\n#usage # data has to be tensor torch.stack([data[i:i+some_number] for i in range(10)]) torch.from_numpy(numpy_array) shares the memory with the numpy_array but is tensor type\na = np.array([1,2,3]) b = torch.tensor(a) # creates copy c = torch.from_numpy(a) # shares memory a[0] = 11 c # outputs: tensor([11, 2, 3]) torch.flatten(input, start,end=-1) flattens the input from dim start to end (-1 by default)\nt = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]) torch.flatten(t) torch.flatten(t, start_dim=1) # (2,2,2) --\u0026gt; (2,2*2) [5, 6, 7, 8]]) torch.stack and torch.cat((tensors), dim) torch.stack stacks tensors along new dim, whereas torch.cat concatenates along that specific dim.\nexample:\na = torch.randn(2,5,8,32) b = torch.randn(2,1,8,32) torch.cat((a,b), dim=1).shape #outputs : torch.Size([2, 6, 8, 32]) a = torch.randn(3,5,8,32) b = torch.randn(3,5,8,32) torch.stack((a,b), dim=1).shape #outputs: torch.Size([3, 2, 5, 8, 32]) For the past 2 years I\u0026rsquo;ve been involved in training and experimenting machine learning systems, mostly using third party packages such as sklearn, huggingface and so on. Sometimes the experiments become too specific and the abstraction provided by these packages become a bottleneck for the performance optimization. My research goal is to understand these bottlenecks in deep and write my own optimized code for hardware-specific optimization which enables resource efficient training or inference.\n","permalink":"https://cohlem.github.io/sub-notes/pytorch/","summary":"torch.stack(tensors, dim) stacks the tensors across dim\n#usage # data has to be tensor torch.stack([data[i:i+some_number] for i in range(10)]) torch.from_numpy(numpy_array) shares the memory with the numpy_array but is tensor type\na = np.array([1,2,3]) b = torch.tensor(a) # creates copy c = torch.from_numpy(a) # shares memory a[0] = 11 c # outputs: tensor([11, 2, 3]) torch.flatten(input, start,end=-1) flattens the input from dim start to end (-1 by default)\nt = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]) torch.","title":"Pytorch Commands I forget time to time/ commands that are essential"},{"content":"Unicode Character encoding standard aims to incorporate all the available digital characters Each character in Unicode has a unique 4 to 6-digit hexadecimal number. For Example, the letter \u0026lsquo;A\u0026rsquo; has the code 0041, represented as U+0041. compatible with ASCII first 128 characters in Unicode directly correspond to the characters represented in the 7-bit ASCII table Unicode Transformation Format (UTF-8) uses 1-4 bytes to represent each character can encode all the unicode code points backward compatible with ASCII Example: (1 byte) The character \u0026#39;A\u0026#39; (U+0041) is encoded as `01000001` (0x41 in hexadecimal). (2 byte) The character \u0026#39;¢\u0026#39; (U+00A2) is encoded as `11000010 10100010` (0xC2 0xA2 in hexadecimal). (3 byte) The character \u0026#39;€\u0026#39; (U+20AC) is encoded as `11100010 10000010 10101100` (0xE2 0x82 0xAC in hexadecimal). (4 byte) The character \u0026#39;𠜎\u0026#39; (U+2070E) is encoded as `11110000 10100000 10011100 10001110`(0xF0 0xA0 0x9C 0x8E in hexadecimal). let\u0026rsquo;s understand some difference between unicode and utf-8 character \u0026lsquo;€\u0026rsquo; has\nunicode code point (hex): U+20AC unicode code point (decimal): 8364\nSo there is a single number (decimal) that represents characters in unicode\nbut, the same character in utf-8 is represented as\nBinary: 11100010 10000010 10101100\nHexadecimal: 0xE2 0x82 0xAC\nDecimal: 226, 130, 172\nWhy ? utf-8 is a standard that stores characters in 1-4 bytes as described above.\nsimilarly in python, we can get it\u0026rsquo;s hex values by '€'.encode('utf-8') and converting it to list gives us it\u0026rsquo;s list of decimal values and doing ord(\u0026lsquo;€\u0026rsquo;) gives us it\u0026rsquo;s unicode code point in decimal`\nBuild vocabulary We build our vocabulary by gathering chunks of bytes that appear together most of the times,\nSuppose the data to be encoded is\naaabdaaabac\nthe byte pair \u0026ldquo;aa\u0026rdquo; occurs most often, so it will be replaced by a byte that is not used in the data, such as \u0026ldquo;Z\u0026rdquo;. Now there is the following data and replacement table:\nZabdZabac Z=aa Then the process is repeated with byte pair \u0026ldquo;ab\u0026rdquo;, replacing it with \u0026ldquo;Y\u0026rdquo;:\nZYdZYac Y=ab Z=aa again, the pair ZY occurs twice so our data and replacement table becomes.\nXdXac X=ZY Y=ab Z=aa We now write code to implement this same functionality.\nThe function below constructs a dictionary that keeps track of frequency of bytes that come together.\ndef get_stats(text): freq = dict() for t1,t2 in zip(text, text[1:]): freq[(t1,t2)] = freq.get((t1,t2),0) + 1 return freq This merge function is now used to merge the two bytes into one. similar to Z=aa in the example above.\ndef merge(pair, ids, idx): n_ids = [] i=0 while i \u0026lt; len(ids): if i\u0026lt;len(ids) -1 and ids[i] == pair[0] and ids[i+1] == pair[1]: n_ids.append(idx) i+=2 else: n_ids.append(ids[i]) i+=1 return n_ids We do this iteratively for total_merges times,\nmax_vocab = 356 total_merges = max_vocab - 256 merge_dict = {} for i in range(total_merges): stats = get_stats(ids) pair = max(stats, key=stats.get) idx = 256+i ids = merge(pair, text_utf8, idx) merge_dict[pair] = idx Encode lets encode our text into our tokens using our merge_dict which keeps track of all the possible combination of characters.\ndef encode(text): # given a string, return list of integers (the tokens) tokens = list(text.encode(\u0026#34;utf-8\u0026#34;)) while len(tokens) \u0026gt;= 2: stats = get_stats(tokens) pair = min(stats, key=lambda p: merge_dict.get(p, float(\u0026#34;inf\u0026#34;))) print(pair) if pair not in merges: break # nothing else can be merged idx = merge_dict[pair] tokens = merge(tokens, pair, idx) return tokens here we need to be careful about how we encode the tokens, i.e for instance the new 256 index in our merge_dict should be encoded first because later index for ex: 352 could be combination of 256 and 108. So we have to maintain this order.\nto do that we first get all the token combination available to us in our dataset using stats = get_stats(tokens), and this line of code pair = min(stats, key=lambda p: merges.get(p, float(\u0026quot;inf\u0026quot;))) finds the pair with the lowest key (lets say 101,32 -\u0026gt; 256) and then merges those 101,32 tokens to be 256 and the process is continued until there are no pairs that can be combined using our merge_dict mapping.\nDecode vocab = {idx : bytes([idx]) for idx in range(256)} for (p1,p2),idx in merge_dict.items(): vocab[idx] = vocab[p1] + vocab[p2] The code above maps index to its corresponding byte in utf-8, and the loop combines the byte information corresponding to their indexs, for instance lets say 256 is combo of 101,32, the bytes of 101 and 32 will be combined. lets say 352 is now a combo of 256 and 32, their byte information will be combined, which will be easier to decode the information in the code below.\ndef decode(ids): tokens = b\u0026#34;\u0026#34;.join(vocab[idx] for idx in ids) return tokens.decode(\u0026#39;utf-8\u0026#39;, errors = \u0026#34;replace\u0026#34;) the first line in the function above maps their index to byte information, and then those utf-8 bytes will be decoded to their corresponding characters in utf-8.\nThis is the building block of tokenizer, everything that comes next is a more complex and efficient version of the tokenizer.\nOpenai\u0026rsquo;s BPE tokenizer Let\u0026rsquo;s first understand the problem our current implementation of tokenizer has.\nIt will tokenize the whole sequence. i.e lets say we have a sentence\nI\u0026rsquo;ve chosen a bit funky sentence here, just for the purpose of explanation.\nyou are 52 years old, u are good too, you've achieved so much\nif you look closely, this sequence of characters u are appear twice and will have their own mapping in the vocabulary.\nso for token u are it will have one index in our vocabulary, we can see it combines characters from two separate words, which we would like to minimize, and we would like to separate out these types of tokens 've because they generally go along with other words as well. For this purpose, we process our initial text through this regex.\nre.compile(r\u0026#34;\u0026#34;\u0026#34;\u0026#39;s|\u0026#39;t|\u0026#39;re|\u0026#39;ve|\u0026#39;m|\u0026#39;ll|\u0026#39;d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\u0026#34;\u0026#34;\u0026#34;) as you can see these tokens s|'t|'re|'ve|'m|'ll|'d| are separated from text, and we separate out words and numbers as well\nimport regex as re samplere = re.compile(r\u0026#34;\u0026#34;\u0026#34;\u0026#39;s|\u0026#39;t|\u0026#39;re|\u0026#39;ve|\u0026#39;m|\u0026#39;ll|\u0026#39;d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\u0026#34;\u0026#34;\u0026#34;) samplere.findall(\u0026#34; you\u0026#39;ve done soo much 1245234 \u0026#34;) The sentence is processed in this way.\n[\u0026#39; you\u0026#39;, \u0026#34;\u0026#39;ve\u0026#34;, \u0026#39; done\u0026#39;, \u0026#39; soo\u0026#39;, \u0026#39; much\u0026#39;, \u0026#39; 1245234\u0026#39;, \u0026#39; \u0026#39;] and then we train our tokenizer on each elements that we get from this.\nComplete code for tokenizer class Tokenizer: def __init__(self): self.pattern = None self.merges = dict() self.vocab = None def get_stats(self,ids, freq=None): freq = dict() if freq is None else freq for t1,t2 in zip(ids, ids[1:]): freq[(t1,t2)] = freq.get((t1,t2),0) + 1 return freq def merge(self, ids, pair, idx): n_ids = [] i=0 while i \u0026lt; len(ids): if i\u0026lt;len(ids) -1 and ids[i] == pair[0] and ids[i+1] == pair[1]: n_ids.append(idx) i+=2 else: n_ids.append(ids[i]) i+=1 return n_ids GPT4Pattern = r\u0026#34;\u0026#34;\u0026#34;\u0026#39;(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\u0026#34;\u0026#34;\u0026#34; class SampleTokenizer(Tokenizer): def __init__(self,max_vocab, pattern=None): super().__init__() self.pattern = re.compile(GPT4Pattern if pattern is None else pattern) self.max_vocab = max_vocab self.total_merges = max_vocab - 256 def train(self, x): \u0026#34;\u0026#34;\u0026#34;This objective of this function is to build the merges dictionary mapping and build vocab\u0026#34;\u0026#34;\u0026#34; chunks = self.pattern.findall(x) ids = [list(ch.encode(\u0026#39;utf-8\u0026#39;)) for ch in chunks] for p in range(self.total_merges): freq = dict() idx = 256 + p for item in ids: self.get_stats(item, freq) pair = max(freq, key=freq.get) ids = [self.merge(i,pair, idx) for i in ids] self.merges[pair] = idx self._build_vocab() def _build_vocab(self): self.vocab = {idx : bytes([idx]) for idx in range(256)} for (p1,p2),idx in self.merges.items(): self.vocab[idx] = self.vocab[p1] + self.vocab[p2] def encode(self, x): # given a string, return list of integers (the tokens) tokens = list(x.encode(\u0026#34;utf-8\u0026#34;)) while len(tokens) \u0026gt;= 2: stats = self.get_stats(tokens) pair = min(stats, key=lambda p: self.merges.get(p, float(\u0026#34;inf\u0026#34;))) if pair not in self.merges: break # nothing else can be merged idx = self.merges[pair] tokens = self.merge(tokens, pair, idx) return tokens def decode(self,ids): tokens = b\u0026#34;\u0026#34;.join(self.vocab[idx] for idx in ids) return tokens.decode(\u0026#39;utf-8\u0026#39;, errors = \u0026#34;replace\u0026#34;) st = SampleTokenizer(356) st.train(text) a = st.encode(\u0026#34;hey how are you doing 124\u0026#34;) st.decode(a) # outputs: \u0026#39;hey how are you doing 124\u0026#39; ","permalink":"https://cohlem.github.io/sub-notes/tokenization/","summary":"Unicode Character encoding standard aims to incorporate all the available digital characters Each character in Unicode has a unique 4 to 6-digit hexadecimal number. For Example, the letter \u0026lsquo;A\u0026rsquo; has the code 0041, represented as U+0041. compatible with ASCII first 128 characters in Unicode directly correspond to the characters represented in the 7-bit ASCII table Unicode Transformation Format (UTF-8) uses 1-4 bytes to represent each character can encode all the unicode code points backward compatible with ASCII Example: (1 byte) The character \u0026#39;A\u0026#39; (U+0041) is encoded as `01000001` (0x41 in hexadecimal).","title":"Tokenization"},{"content":"Papers that I\u0026rsquo;ve read with their respective notes.\nLLaMA: Open and Efficient Foundation Language Models Trained on 1.4T tokens. Wikipedia and Books domain trained for 2 epochs (maybe because its cleaner, smaller, offers coherent long sequences) use manual backprop for training efficiency i.e save checkpoints of activations that take longer to compute (linear layers) and use them during backprop and generate others such as (ReLu) on the fly. SmolLM2 including specific data eg. math doesn\u0026rsquo;t only do well in math, but also seems to improve reasoning.\nrather than training on one specific dataset, training on mixture of datasets yields better results, for instance, 60-40 mixture of FineWeb-Edu and DCLM yielded almost similar performance to only training on FineWeb-Edu\ndecontamination of curated dataset is generally done, using some bi-gram matching using the eval dataset.\nthey do a multi-stage training approach rather than fixed-data mixture.\nLR decay\nWarmup Phase (Steps 0–2,000):\nLearning rate increases linearly from near 0 to 5.0×10−45.0×10−4. Stable Phase (Steps 2,000–N):\nLearning rate remains constant at 5.0×10−45.0×10−4. Decay Phase (Last 10% of Steps):\nLearning rate decreases linearly from 5.0×10−45.0×10−4 to 0. had loss spikes during stage 3, which remained persistent even after rewinding the trianing, and changing the data that caused the spike. The cause of spike remains undetermined, however the eval metrics recovered in the end.\nThey include high quality math data in the end, and decay the to 0\nThey expand the context length from 2k to 8k before the final 75 billion tokens of training and the mixture was adjusted to include 40% long-context documents\nthey curate their own instruction dataset named SmolTalk, because of low performance after training on previously available dataset i.e MagPie-Pro and OpenHermes2.5.\nFilter high conversational dataset and deduplicate using gte-large embedding models.\nin short they do a lot of decontamination (using bi-gram overlaps), deduplication, filtering,\nFor smaller models during sft, they filter smoltalk dataset (e.g., function calling) and hard examples from MagPie-Ultra to better align with the models’ capacity and do DPO on UltraFeedback dataset.\nTraining Compute-Optimal Large Language Models Given a fixed FLOPs budget, how should one trade-off model size and the number of training tokens? https://lifearchitect.ai/chinchilla/#deepmind\n","permalink":"https://cohlem.github.io/sub-notes/paper-summaries/","summary":"Papers that I\u0026rsquo;ve read with their respective notes.\nLLaMA: Open and Efficient Foundation Language Models Trained on 1.4T tokens. Wikipedia and Books domain trained for 2 epochs (maybe because its cleaner, smaller, offers coherent long sequences) use manual backprop for training efficiency i.e save checkpoints of activations that take longer to compute (linear layers) and use them during backprop and generate others such as (ReLu) on the fly. SmolLM2 including specific data eg.","title":"Papers Summaries"},{"content":"KV Cache KV cache visual operation In the note blow, I first describe how inferencing is done if we simply do operation without KV cache and then describe how KV cache helps removing redundant operations.\nWe don\u0026rsquo;t make use of KV cache while training because we already have data filled for each sequence length, we don\u0026rsquo;t need to calculate loss one by one, instead we do it in batches, whereas while inferencing we do it generally for 1 batch with some sequences and then we keep on appending next-predicted token to that sequence one by one. To understand better look at the notes below.\nMemory needed for storing KV cache let\u0026rsquo;s calculate total memory needed for storing KV cache\nbatch_size = 1 (for inferencing) d_model = 4096 num_of_kv_heads = 32 head_dim = d_model/num_of_kv_heads = 128 seq_len = 10000 precision (fp16) = 2 bytes 2 (for k and v separately) x precision x head_dim x num_of_kv_heads x d_model x seq_len x batch_size = 5,24,28,80,000 bytes close to 5GB lets say we have 7B parameter model, 7x10^9 x2 (bytes) = 14x10^9 bytes = 14GB\nwe need almost 1/3 total memory for inferencing.\nLet\u0026rsquo;s explore the code for using KV cache in Llama models.\nPlease note I\u0026rsquo;ve modified some part of the original Llama code below to just explain the case of KV cache here.\n@dataclass class ModelArgs: dim: int = 4096 n_layers: int = 32 n_heads: int = 32 n_kv_heads: int = 16 # modified to explain GQA vocab_size: int = -1 # defined later by tokenizer multiple_of: int = 256 # make SwiGLU hidden layer size multiple of large power of 2 ffn_dim_multiplier: Optional[float] = None norm_eps: float = 1e-5 max_batch_size: int = 32 max_seq_len: int = 2048 Attention Architecture Code you might be familiar with the code below if you\u0026rsquo;ve implemented attention mechanism on your own (more explanation below)\nclass Attention(nn.Module): \u0026#34;\u0026#34;\u0026#34;Multi-head attention module.\u0026#34;\u0026#34;\u0026#34; def __init__(self, args: ModelArgs): \u0026#34;\u0026#34;\u0026#34; Initialize the Attention module. Args: args (ModelArgs): Model configuration parameters. Attributes: n_kv_heads (int): Number of key and value heads. n_local_heads (int): Number of local query heads. n_local_kv_heads (int): Number of local key and value heads. n_rep (int): Number of repetitions for local heads. head_dim (int): Dimension size of each attention head. wq (ColumnParallelLinear): Linear transformation for queries. wk (ColumnParallelLinear): Linear transformation for keys. wv (ColumnParallelLinear): Linear transformation for values. wo (RowParallelLinear): Linear transformation for output. cache_k (torch.Tensor): Cached keys for attention. cache_v (torch.Tensor): Cached values for attention. \u0026#34;\u0026#34;\u0026#34; super().__init__() self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads # model_parallel_size = fs_init.get_model_parallel_world_size() self.n_local_heads = args.n_heads self.n_local_kv_heads = self.n_kv_heads self.n_rep = self.n_local_heads // self.n_local_kv_heads self.head_dim = args.dim // args.n_heads self.wq = nn.Linear( args.dim, args.n_heads * self.head_dim, bias=False, ) self.wk = nn.Linear( args.dim, self.n_kv_heads * self.head_dim, bias=False, ) self.wv = nn.Linear( args.dim, self.n_kv_heads * self.head_dim, bias=False, ) self.wo = nn.Linear( args.n_heads * self.head_dim, args.dim, bias=False, ) self.cache_k = torch.zeros( ( args.max_batch_size, args.max_seq_len, self.n_local_kv_heads, self.head_dim, ) ) self.cache_v = torch.zeros( ( args.max_batch_size, args.max_seq_len, self.n_local_kv_heads, self.head_dim, ) ) def forward( self, x: torch.Tensor, start_pos: int, # freqs_cis: torch.Tensor, mask: Optional[torch.Tensor], ): \u0026#34;\u0026#34;\u0026#34; Forward pass of the attention module. Args: x (torch.Tensor): Input tensor. start_pos (int): Starting position for caching. freqs_cis (torch.Tensor): Precomputed frequency tensor. mask (torch.Tensor, optional): Attention mask tensor. Returns: torch.Tensor: Output tensor after attention. \u0026#34;\u0026#34;\u0026#34; bsz, seqlen, _ = x.shape xq, xk, xv = self.wq(x), self.wk(x), self.wv(x) xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim) xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim) xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim) # xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis) self.cache_k = self.cache_k.to(xq) self.cache_v = self.cache_v.to(xq) self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv keys = self.cache_k[:bsz, : start_pos + seqlen] values = self.cache_v[:bsz, : start_pos + seqlen] # Grouped Query Attention # for production use repeat_kv function, it\u0026#39;s memory efficient ## Repeat the key, values heads, repeats values at dim =2 self.n_rep times, now keys and values size match query size. keys = torch.repeat_interleave(keys, dim=2, repeats=self.n_rep) # (bs, cache_len + seqlen, n_local_heads, head_dim) values= torch.repeat_interleave(values, dim=2, repeats=self.n_rep) # (bs, cache_len + seqlen, n_local_heads, head_dim) xq = xq.transpose(1, 2) # (bs, n_local_heads, seqlen, head_dim) keys = keys.transpose(1, 2) # (bs, n_local_heads, cache_len + seqlen, head_dim) values = values.transpose(1, 2) # (bs, n_local_heads, cache_len + seqlen, head_dim) print(xq.shape) print(keys.transpose(2,3).shape) scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim) if mask is not None: scores = scores + mask # (bs, n_local_heads, seqlen, cache_len + seqlen) scores = F.softmax(scores.float(), dim=-1).type_as(xq) output = torch.matmul(scores, values) # (bs, n_local_heads, seqlen, head_dim) output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1) return self.wo(output) Inferencing code This code generates embedding for next tokens by passing existing sequence of tokens to our attention layer (not FFNN) and append it to our existing sequence of tokens and pass it again to the attention layer and do it to generate it til 10 tokens.\nimport torch import torch.nn as nn import math # Define a simple ModelArgs class for testing # Initialize the Attention module args = ModelArgs() attn = Attention(args) # Input sentence and tokenization sentence = \u0026#39;this is awesome\u0026#39; tokens = sentence.split(\u0026#39; \u0026#39;) # Hyperparameters B = 1 # Batch size T = len(tokens) # Sequence length C = args.dim # Feature dimension # Initialize input tensor with random values (for demonstration) x = torch.randn(B, args.max_seq_len, C) # Shape: (batch_size, max_seq_len, feature_dim) # Inference loop start_pos = 0 for cur_pos in range(T, 10): # Generate tokens up to max_seq_len # Forward pass through the attention module out = attn(x[:, start_pos:cur_pos], start_pos, None) # Shape: (batch_size, cur_pos - start_pos, feature_dim) # Update the input tensor with the output for the next position x[:, cur_pos] = out[:, -1, :] # Take the last token\u0026#39;s output and append it to the sequence # Update start_pos for the next iteration start_pos = cur_pos # Final output print(\u0026#34;Final output tensor:\u0026#34;) print(x) let\u0026rsquo;s break down the whole code one by one\n# Initialize the Attention module args = ModelArgs() attn = Attention(args) # Input sentence and tokenization sentence = \u0026#39;this is a\u0026#39; tokens = sentence.split(\u0026#39; \u0026#39;) # Hyperparameters B = 1 # Batch size T = len(tokens) # Sequence length C = args.dim # Feature dimension # Initialize input tensor with random values (for demonstration) x = torch.randn(B, args.max_seq_len, C) # Shape: (batch_size, max_seq_len, feature_dim) this is straightforward, we have initial sentence that we pass it to the model i.e \u0026ldquo;this is awesome\u0026rdquo;, construct a random input matrix \u0026ldquo;x\u0026rdquo; of (batch_size, max_seq_len, feature_dim). In real, these random input matrix is the matrix full of input embeddings.\nstart_pos = 0 the is the starting point for caching, since we haven\u0026rsquo;t cached anything yet, we start from the initial position i.e 0 for token \u0026ldquo;this\u0026rdquo;\nfor cur_pos in range(T, 10): # Generate tokens up to max_seq_len # Forward pass through the attention module out = attn(x[:, start_pos:cur_pos], start_pos, None) # Shape: (batch_size, cur_pos - start_pos, feature_dim) # Update the input tensor with the output for the next position x[:, cur_pos] = out[:, -1, :] # Take the last token\u0026#39;s output and append it to the sequence # Update start_pos for the next iteration start_pos = cur_pos we iterate from T,10 because we already have tokens til T and we want to generate 10-T tokens.\nwe pass the sequence from start_pos:cur_pos to our attn architecture.\nfirst this will be x[:, 0:3] (which is the initial tokens \u0026ldquo;this is a\u0026rdquo;, because we first need to calculate the attention for these initial tokens and cache them first.\nlet\u0026rsquo;s directly come down to this part of code, because all other are usual code for attention without caching.\nself.cache_k[:bsz, start_pos : start_pos + seqlen] = xk self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv keys = self.cache_k[:bsz, : start_pos + seqlen] values = self.cache_v[:bsz, : start_pos + seqlen] as you can see we first cache xk to the positions [:bsz, start_pos : start_pos + seqlen]\nwhich is basically caching this index of our input [:1, 0:3] which is basically the initially tokens (\u0026rsquo;this is a')\nand then we pluck out the same tokens from our cached keys and cached values\nkeys = self.cache_k[:bsz, : start_pos + seqlen]\nthis plucking out will make sense in the next run.\nnow rest of the code in the attn class is executed and we get output same as the size of query i.e which is prediction for these positions [:bsz, start_pos : start_pos + seqlen]\nand then we pluck out the last one, because its want we need and add it in the end of our input x[:, cur_pos] = out[:, -1, :].\nnow lets say the predicted token was \u0026ldquo;good\u0026rdquo;, we now have sequence \u0026ldquo;this is a good\u0026rdquo; and start_pos=3, and now in the next iteration cur_pos=4, we pass this input i.e (x[:, 3:4], 3, None) to our attention\nas you can see this is simply the prediction from earlier iteration and this is what we pass, because we only need the embedding for this token \u0026ldquo;good\u0026rdquo;.\nand then we only this new token in the code\nself.cache_k[:bsz, start_pos : start_pos + seqlen] = xk self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv self.cache_k[:bsz, 3:3 + 1], as you can see we are just appending this new token to the preivous cache to be used in the later iteration.\nnow we pluck out this cache\nkeys = self.cache_k[:bsz, : start_pos + seqlen] values = self.cache_v[:bsz, : start_pos + seqlen] i.e keys = self.cache_k[:bsz, :3+1] which is the key and values cache til that token and simply calculate the attention scores and apply them, and this process goes on until the required sequence is generated (in our case 10)\nThis is all we need to know about KV cache.\nGrouped Query Attention Explanation As we know the main bottleneck while training and inferencing is not the amount of operations that our GPU can perform but rather the amount of data our GPU can move between tensor cores and the GPU memory. This is what GQA tries to solve, it tries to achieve balance between the accuracy of Multi-Head Attention (it performs better than these attention variants) and speed of attention calculation. The picture below accurately explains Multi-Head Attention (MHA), Multi-Query Attention (MQA) and Grouped Query Attention (GQA)\nThe main difference between them is:\nMHA : Q,K,V are divided into equal number of heads MQA: Only Q is divided into different heads, whereas K,V remain the same. However, the resulting number of attention heads are the same as MHA. (K,V are same, we don\u0026rsquo;t have to move data back and forth, this is where it helps in achieving performance gains.) GQA: Query is divided into total number of heads but mainly in groups, and K, V have different number of heads mainly referred to as kv_heads. As shown in the figure, similar group of query interact with their respective heads. Code for GQA keys = torch.repeat_interleave(keys, dim=2, repeats=self.n_rep) # (bs, cache_len + seqlen, n_local_heads, head_dim) values= torch.repeat_interleave(values, dim=2, repeats=self.n_rep) # The way this code works is just by duplicating the keys and values across the dim=2, self.n_rep number of times. This self.n_rep is obtained by dividing self.n_local_heads by self.n_local_kv_heads.\nFor instance, lets say our keys were (2,2,2,4) ( # (bs, cache_len + seqlen, n_local_kv_heads, head_dim))\nThe dimension across n_local_kv_heads will be repeated self.n_rep times,\na = torch.randn(2,2,2,4) # B,T,n_kv_head, head_dim a tensor([[[[ 0.6406, -1.2496, 0.9831, -0.3773], [ 1.0520, 0.5683, 0.6138, 0.0082]], [[-0.6792, 1.0518, 0.6339, 0.9386], [-0.0693, 0.8445, 1.8666, 1.6446]]], [[[-0.5852, -1.5809, -0.3186, 1.2536], [-0.9714, 0.4342, -1.0229, 0.1140]], [[-0.4645, 0.6589, -0.6345, 0.9500], [ 0.3443, -0.7342, -0.0163, 0.3242]]]]) torch.repeat_interleave(a, dim=2, repeats=2) tensor([[[[ 0.6406, -1.2496, 0.9831, -0.3773], [ 0.6406, -1.2496, 0.9831, -0.3773], [ 1.0520, 0.5683, 0.6138, 0.0082], [ 1.0520, 0.5683, 0.6138, 0.0082]], [[-0.6792, 1.0518, 0.6339, 0.9386], [-0.6792, 1.0518, 0.6339, 0.9386], [-0.0693, 0.8445, 1.8666, 1.6446], [-0.0693, 0.8445, 1.8666, 1.6446]]], [[[-0.5852, -1.5809, -0.3186, 1.2536], [-0.5852, -1.5809, -0.3186, 1.2536], [-0.9714, 0.4342, -1.0229, 0.1140], [-0.9714, 0.4342, -1.0229, 0.1140]], [[-0.4645, 0.6589, -0.6345, 0.9500], [-0.4645, 0.6589, -0.6345, 0.9500], [ 0.3443, -0.7342, -0.0163, 0.3242], [ 0.3443, -0.7342, -0.0163, 0.3242]]]]) you can see how the values are copied one by one 2 times for this scenario.\nBUT, this simply copies the numbers twice, there\u0026rsquo;s another way that\u0026rsquo;s used in Llama code\ndef repeat_kv(x: torch.Tensor, n_rep: int) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34;torch.repeat_interleave(x, dim=2, repeats=n_rep)\u0026#34;\u0026#34;\u0026#34; bs, slen, n_kv_heads, head_dim = x.shape if n_rep == 1: return x return ( x[:, :, :, None, :] .expand(bs, slen, n_kv_heads, n_rep, head_dim) .reshape(bs, slen, n_kv_heads * n_rep, head_dim) ) It performs the same operation as torch.repeat_interleave, but the in a more memory efficient way.\nx[:, :, :, None, :] adding None will add one extra dimension to our vector. its shape will be (2,2,2,1,4)\n.expand(bs, slen, n_kv_heads, n_rep, head_dim) will expand(repeat) the the singleton dimension i.e our dimension 3 to n_rep, it will repeat n_rep times but not by copying the same elements but by creating a new view for that dimension which points to the same old memory location, and then we reshape it to (bs, slen, n_kv_heads * n_rep, head_dim).\nBy not copying and simply creating a new view, it saves memory.\n","permalink":"https://cohlem.github.io/sub-notes/kv-cache-gqa/","summary":"KV Cache KV cache visual operation In the note blow, I first describe how inferencing is done if we simply do operation without KV cache and then describe how KV cache helps removing redundant operations.\nWe don\u0026rsquo;t make use of KV cache while training because we already have data filled for each sequence length, we don\u0026rsquo;t need to calculate loss one by one, instead we do it in batches, whereas while inferencing we do it generally for 1 batch with some sequences and then we keep on appending next-predicted token to that sequence one by one.","title":"KV cache and Grouped Query Attention"},{"content":"Recap of LayerNorm let\u0026rsquo;s first recap by understanding why LayerNorm was used:\nWe needed to balance the distribution of inputs (internal covariance shift) i.e we want inputs to be roughly gaussian (mean 0, std 1), it not maintained it would result in zeroing out the gradients. output of some blocks (transformer block) may produce large values or very small values that would result in either exploding or vanishing gradient problem, in order to have stable training, we needed to have stable range for those outputs. In this paper, the authors raise concern about LayerNorm.\nit\u0026rsquo;s computationally expensive and they claim that re-centering (calculating the x - mean) has little impact for stabilization. they\ncompletely get rid of mean statistic now there is less overhead because we don\u0026rsquo;t have to calculate one extra statistic. they also provide that RMSNorm is invariant (does not change) to inputs or weights matrices.\nwhich indicates that the change in scale of input of weights doesn\u0026rsquo;t affect the RMSNorm.\n","permalink":"https://cohlem.github.io/sub-notes/rmsnorm/","summary":"Recap of LayerNorm let\u0026rsquo;s first recap by understanding why LayerNorm was used:\nWe needed to balance the distribution of inputs (internal covariance shift) i.e we want inputs to be roughly gaussian (mean 0, std 1), it not maintained it would result in zeroing out the gradients. output of some blocks (transformer block) may produce large values or very small values that would result in either exploding or vanishing gradient problem, in order to have stable training, we needed to have stable range for those outputs.","title":"RMSNorm"},{"content":"Recap of Absolute PE We previously used absolute positional embedding in our GPT-2 model.\nDisadvantages No notion of relative information between tokens doesn\u0026rsquo;t work for sequences larger than context length the model is trained with, because we run out of token embeddings for tokens that come at sequence larger than the context length. RoPE pre-requisites This is how we rotate a point by an angel theta in a two dimensional space and this is all we need in RoPE.\nIn RoPE, the notion of providing a position to a token is by rotating it by some angle. For instance, if we have a token \u0026ldquo;this\u0026rdquo; with an embedding of 2 i.e [0.2782, 1.5109] and lets say it occurs in our sequence at first\n[this,is,awesome] The idea is to rotate token\u0026rsquo;s embedding according to the position it occurs in our sequence.\nFor instance, if this occurs in 2nd position it\u0026rsquo;s original embedding ([0.2782, 1.5109]) will be rotated by some angle m x theta, where m is the position and theta is some angle. You can see how we\u0026rsquo;re incorporating information about absolute position of tokens (m) while rotating it.\nbut the embedding is usually bigger than what we\u0026rsquo;ve assumed here, it\u0026rsquo;s basically 256,512,768 depending on what we choose. Let\u0026rsquo;s say we have embedding dimension of 4 for the sake of simplicity. The idea is to first divide embedding dimensions into pairs and apply rotation to each pairs.\nd_model = 4 sentence = \u0026#39;this is awesome\u0026#39; x = [i for i in range(len(sentence.split(\u0026#39; \u0026#39;)))] Q = torch.randn(len(sequence), d_model) Q output\n[[ 0.2782, 1.5109, 0.1739, -0.7098], [ 0.3792, -0.1098, 0.3707, -0.4049], [ 0.1652, 0.5787, 0.4085, -0.7005]] so embedding for our token \u0026ldquo;this\u0026rdquo; corresponds to\n[ 0.2782, 1.5109, 0.1739, -0.7098] it is divided into two i.e i values i.e i = 0 will correspond to [ 0.2782, 1.5109] and i=1 will correspond to[0.1739, -0.7098] so, the i values ranges from 0 to d_model/2 - 1, where d_model is the embedding dimension and and we can apply rotation to each of these separately and combine them together into one single embedding.\nthe angle by which we rotate a pair depends on m and theta, as m is simply the position and theta is generally this\n10000 ** (-2 * i / d_model) #where i corresponds to each pair as described above. let\u0026rsquo;s see how embedding of our token \u0026ldquo;this\u0026rdquo; is rotated if it occurs at different positions (i.e different m) in this case i\u0026rsquo;m assuming i=0 (only considering the first pair)\nWe construct our rotation matrix like this\ndef rotation_matrix(m, i, d_model): \u0026#34;\u0026#34;\u0026#34; Compute the 2x2 rotation matrix for RoPE. Args: m (int or torch.Tensor): Position (token index). i (int or torch.Tensor): Dimension index. d_model (int): Embedding dimension. Returns: torch.Tensor: 2x2 rotation matrix. \u0026#34;\u0026#34;\u0026#34; # Compute theta_i thetai = 10000 ** (-2 * i / d_model) print(thetai) # Ensure m is a tensor m = torch.tensor(m, dtype=torch.float32) # Compute cos(m * theta_i) and sin(m * theta_i) angle = m * thetai cos = torch.cos(angle) sin = torch.sin(angle) # Construct the 2x2 rotation matrix rotation_matrix = torch.tensor([ [cos, -sin], [sin, cos] ]) return rotation_matrix and apply this rotation for different m values(different positions) i.e 1,2,3,4,5\n# Input vector inp = Q[0,:2].detach().numpy() # [ 0.2782, 1.5109] # Create a figure and axis plt.figure() # Draw the coordinate axes plt.axhline(0, color=\u0026#34;black\u0026#34;, linewidth=1) # X-axis plt.axvline(0, color=\u0026#34;black\u0026#34;, linewidth=1) # Y-axis # Set axis limits plt.xlim(-2, 2) plt.ylim(-2, 2) # Plot the original input vector plt.plot([0, inp[0]], [0, inp[1]], color=\u0026#39;blue\u0026#39;, label=\u0026#39;Original\u0026#39;, marker=\u0026#39;o\u0026#39;) # Apply rotation and plot the rotated vectors for m in range(1, 5): # Compute the rotation matrix rot_mat = rotation_matrix(m, 0, d_model) # Apply the rotation to the input vector ans = rot_mat @ inp # Plot the rotated vector print(ans) plt.plot([0, ans[0]], [0, ans[1]], label=f\u0026#39;Rotated (m={m})\u0026#39;, marker=\u0026#39;o\u0026#39;) # Add labels and title plt.xlabel(\u0026#34;X-axis\u0026#34;) plt.ylabel(\u0026#34;Y-axis\u0026#34;) plt.title(\u0026#34;RoPE Rotation Visualization\u0026#34;) # Add a legend plt.legend() # Add a grid plt.grid(True, linestyle=\u0026#34;--\u0026#34;, alpha=0.7) # Display the plot plt.show() you can see how these are rotated by some big angles when m is changed.\nbut this isn\u0026rsquo;t the case for later embedding pairs (i.e i = 300, when d_model = 768).\nThe more we increase our i values the less angle it will rotate with. Lets see one example I\u0026rsquo;m keeping the value of m as a constant i.e 1 here and plotting for different i values for sample input embedding.\ninp = Q[0,:2].detach().numpy() # Create a figure and axis plt.figure() # Draw the coordinate axes plt.axhline(0, color=\u0026#34;black\u0026#34;, linewidth=1) # X-axis plt.axvline(0, color=\u0026#34;black\u0026#34;, linewidth=1) # Y-axis # Set axis limits plt.xlim(-2, 2) plt.ylim(-2, 2) # Plot the original input vector plt.plot([0, inp[0]], [0, inp[1]], color=\u0026#39;blue\u0026#39;, label=\u0026#39;Original\u0026#39;, marker=\u0026#39;o\u0026#39;) #Apply rotation and plot the rotated vectors for i in range(1, 20): # Compute the rotation matrix rot_mat = rotation_matrix(1,i , 40) # Apply the rotation to the input vector ans = rot_mat @ inp # Plot the rotated vector print(ans) plt.plot([0, ans[0]], [0, ans[1]], label=f\u0026#39;Rotated i={i} (m={0})\u0026#39;, marker=\u0026#39;o\u0026#39;) # Add labels and title plt.xlabel(\u0026#34;X-axis\u0026#34;) plt.ylabel(\u0026#34;Y-axis\u0026#34;) plt.title(\u0026#34;RoPE Rotation Visualization\u0026#34;) # Add a legend plt.legend() # Add a grid plt.grid(True, linestyle=\u0026#34;--\u0026#34;, alpha=0.7) # Display the plot plt.show() as you can see the angle between them becomes smaller and smaller when I is increased.\nNOTE: In the figure above our original plot (dark blue) lies in the same pos as the plot covered by i=19.\nI just wanted to show how angles change depending on m and i values.\nNow that we have basic intuition, lets implement it.\nrotating a embedding of dimension 2 is simple, but how do we do it for more than 2 dimension in just one operation?\nhere\u0026rsquo;s how. we construct matrix like this where the diagonal is rotation matrix (same matrix that we used above) and all the other elements are sparse i.e 0 and then multiply this big rotation matrix with our embedding of size d.\nBut the authors of this paper have derived a simple formula for doing the same operation but without the big matrix, because it would be computationally expensive where most of the elements are sparse.\nhere\u0026rsquo;s the simplified formula. lets apply this simplified version to our token \u0026ldquo;this\u0026rdquo; where it\u0026rsquo;s embedding is [ 0.2782, 1.5109, 0.1739, -0.7098]\nNOTE: this code below only applies the rotation to token \u0026ldquo;this\u0026rdquo; only for simplicity. m=0 because it occurs at position 0.\nIf you take a closer look the code below, it matches the formula shown above.\nm = 0 cos_dim = [math.cos(m*(10000**(-2*i/d_model))) for i in range(d_model//2) for _ in range(2)] sin_dim = [math.sin(m*(10000**(-2*i/d_model))) for i in range(d_model//2) for _ in range(2)] QT = torch.empty_like(Q[0]) QT[1::2] = Q[0][::2] QT[::2] = -Q[0][1::2] simplified_ans = Q[0]*torch.tensor(cos_dim) + QT*torch.tensor(sin_dim) # using m This code below applies the rotation to all 3 tokens in our sequence for all i values.\nthetai = 1.0 / (10000 ** (2*torch.arange(0, d_model//2).float() / d_model)) m = torch.arange(Q.shape[0], dtype=torch.float) sinusoid = m.unsqueeze(1) * thetai.unsqueeze(0) # Outer product freq = sinusoid.repeat_interleave(2,dim=1) cos= torch.cos(freq) sin = torch.sin(freq) Q_new = torch.empty_like(Q) Q_new[:,::2] = -Q[:,1::2] Q_new[:,1::2] = Q[:,::2] ans = Q*cos + Q_new*sin all these code were for matrix rotation. But there\u0026rsquo;s more to it.\nllama models don\u0026rsquo;t use these type of operation, because there seems to be too much tensor manipulation is a bit computationally expensive. There seems to be an elegant way of doing 2D rotation, and that is by using complex numbers.\nBefore going into the implementation, please read these paper notes.\nComplex plane is simply representing real numbers along the x-axis and imaginary numbers along y (i.e imaginary plane is perpendicular to the real plane)\nafter completely understanding the notes, now we you may have the notion that we can represent our embedding in complex numbers and then multiply by cos(theta) +i.sin(theta) to rotate it by theta.\nwhat we do now in the code below is represent our input in complex form, and represent the rotation angles in polar form and then multiply together to get the output.\ndef precompute_freq_cis(dim, end, theta = 10000): thetai = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)) m = torch.arange(end, device=thetai.device) mthetai = torch.outer(m,thetai) # m x thetai return torch.polar(torch.ones_like(mthetai),mthetai) # computes r * e^mthetai def apply_rotary_pe(xq, xk, cis): def unite_shape(pos_cis, x): ndim = x.ndim assert 0 \u0026lt;= 1 \u0026lt; ndim assert pos_cis.shape == (x.shape[1], x.shape[-1]) shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)] return pos_cis.view(*shape) xq_ = torch.view_as_complex(xq.view(*xq.shape[:-1],-1,2)) # converting to complex form xk_ = torch.view_as_complex(xk.view(*xq.shape[:-1],-1,2)) # converting to complex form cis = unite_shape(cis,xq_) xq_out = torch.view_as_real(xq_ * cis).flatten(3) xk_out = torch.view_as_real(xk_ * cis).flatten(3) return xq_out.type_as(xq), xk_out.type_as(xk) freq_cis = precompute_freq_cis(4,3) xq = torch.randn(5,3,2,4) xk = torch.randn(5,3,2,4) xq_out, xk_out = apply_rotary_pe(xq,xk,freq_cis) This is how it\u0026rsquo;s originally implemented in llama models as you can see here\nhttps://github.com/meta-llama/llama/blob/8fac8befd776bc03242fe7bc2236cdb41b6c609c/llama/model.py#L132\nReferences https://arxiv.org/pdf/2104.09864 RoPE Complex Numbers Fundamentals llama code implementation ","permalink":"https://cohlem.github.io/sub-notes/rope/","summary":"Recap of Absolute PE We previously used absolute positional embedding in our GPT-2 model.\nDisadvantages No notion of relative information between tokens doesn\u0026rsquo;t work for sequences larger than context length the model is trained with, because we run out of token embeddings for tokens that come at sequence larger than the context length. RoPE pre-requisites This is how we rotate a point by an angel theta in a two dimensional space and this is all we need in RoPE.","title":"RoPE"},{"content":"Watch list Mirror (1975) Recently Watched (my rating /10) Gladiator 2 (6) Braveheart (7) Saltburn (3) Wind River (8.5) Hell or High Water (8) Blue Ruin (8) The Place Beyond the Pines (8.5) 3:10 to Yuma (9.5) The Magnificent Seven (7) Bone Tomahawk (8) Unforgiven (6) True Grit (7) Personal Favorites (unranked) Stalker (1979) Parasite Mother (dir. Bong Joon ho) Memories of murder Dune 1 and 2 Snowpiercer Gravity Arrival Incendis Prisoners 3:10 to Yuma Series (ranked) I don\u0026rsquo;t follow series a lot but these are some that I\u0026rsquo;ve enjoyed watching.\nBreaking Bad Game of Thrones True Detective House of the Dragon Reviews 3:10 to Yuma I felt like I should write something about why I love this movie, and what aspects of the movie made me love it. First, the character build up was on the point. Every character makes sense and nothing out of place.\nThe story revolves around these main characters. Every character has their own principles and they operate by it, throughout the movie. Most importantly, I feel like the movie is about respect, and morality which becomes evident from the main characters Dan Evans and Ben Wade.\nDan Evans is highly moral character and tries to do the right thing, throughout the movie it\u0026rsquo;s about whether Dan Evans can hold up to his morality given the circumstances (he had family to feed, son to take care of).\nBen Wade isn\u0026rsquo;t without his principles too. He kills people who he has no respect for. We can see how he spares the life of McElroy in the beginning, because he values and respects him. In the end when McElroy provokes Ben, he loses his respect and then Ben kills him. He kills his own gang member because the gang member put greed over everything. Ben seems to test every character by engaging in talk with everyone as we can see he doesn\u0026rsquo;t like to keep his mouth shut. He is confronted with Evans who he seems is very exciting to talk to and challenges him and his morality on every conversation. The contrast that I find between both of the characters is that Ben acts according to the circumstances, but Evans strictly holds up to his principles whatever be the circumstances (eg. Ben offers $1000 to Evans which he rejects). Ben tests Evans and wants to see how far Evans can go. Evans wins the trust of Wade. Wade wants him to succeed, he wants Evan\u0026rsquo;s son to look up to him. In conclusion, Evans changes Ben completely.\nThe movie was more about the character study and the screenplay than anything else in my opinion. A solid 9.5 for this movie.\n","permalink":"https://cohlem.github.io/random/movies/","summary":"Watch list Mirror (1975) Recently Watched (my rating /10) Gladiator 2 (6) Braveheart (7) Saltburn (3) Wind River (8.5) Hell or High Water (8) Blue Ruin (8) The Place Beyond the Pines (8.5) 3:10 to Yuma (9.5) The Magnificent Seven (7) Bone Tomahawk (8) Unforgiven (6) True Grit (7) Personal Favorites (unranked) Stalker (1979) Parasite Mother (dir. Bong Joon ho) Memories of murder Dune 1 and 2 Snowpiercer Gravity Arrival Incendis Prisoners 3:10 to Yuma Series (ranked) I don\u0026rsquo;t follow series a lot but these are some that I\u0026rsquo;ve enjoyed watching.","title":"Movies"},{"content":"GPU physcial structure let\u0026rsquo;s first understand the structure of GPU.\nInside a GPU it has a chip named GA102 (depends on architecture, this is for ampere architecture) built from 28.3million transistors (semiconductor device that can switch or amplify electrical signals) and majority area covered by processing cores. processing core is divide into seven Graphics processing clusters (GPCs)\namong each GPC there are 12 Streaming Multiprocessors. Inside each SM there are 4 warps and 1 Raytracing core inside a warp there are 32 Cudas and 1 Tensor Core.\nAltogether there are\n10752 CUDA Cores 336 Tensor Cores 84 Ray Tracing Cores Each cores have different function.\nCuda Cores cuda core is like a basic calculator with multiplication and addition operations.\nMostly used for processing video frames. perform operations like A x B + C called fused multiply and add (FMA) half of the cuda cores perform FMA on 32-bit floating point numbers other half perform 32-bit integer FMA. other sections perform bit shifting and bit masking as well as collecting and queuing incoming operands, and accumulating output results. So it can be though of like a calculator. performs one 1 multiply and 1 add operation per 1 cycle so altogether 2 operations x 10752 cuda cores x 1.7 GhZ clock speed(10^9 cycles per second) = 35.6 trillion calculations per second. Ray tracing Cores used for executing ray tracing algorithms Depending on the amount of streaming multiprocessors that are damaged during manufacturing they are categoriezed and sold at different prices, for instance RTX 3090 ti has full 10752 cuda where as 3090 might have some damaged SMs. These cards might have different clock speed too.\nGraphics Memory GDDR6X SDRAM these 24GBs of GDDR6X surround the GPU chip In order to run the operations of GPU chip\nThey must be loaded from SSD to these graphics memory. They have certain bandwidth i.e the amount of data they can transfer per second to the GPU chip. They have 1.15 Terbytes/sec bandwidth. Similar to HBM memory in AI chips, would look something like this How are operations executed? Each instruction is executed by a thread (which is matched to a single cuda core) and these threads are combined into 32 cores called warp, 4 warps are combined to form thread blocks that are operated by Streaming Multiprocessor (SM). Similarly SM is combined together. All these are operated by Gigathread Engine If processor runs at 1Ghz, it can run 10^9 cycles per second, assuming 1cycle = 1 basic operation it can execute 10^9 operations.\nits basically like a unit of time, 10^9 cycles per 1 second means, one cycle takes 10^-9 seconds to run. Different operations take varying amount of cycles (i.e latency of the operation) to perform. - Global memory access (up to 80GB): ~380 cycles - L2 cache: ~200 cycles - L1 cache or Shared memory access (up to 128 kb per Streaming Multiprocessor): ~34 cycles - Fused multiplication and addition, a*b+c (FFMA): 4 cycles - Tensor Core matrix multiply: 1 cycle Tensor Cores (Most important) These cores are used for matrix multiplication and matrix additions.\npredominantly used in deep learning calculation. First lets start by understanding the precision formats that\u0026rsquo;s used.\n1. FP16 (Half-Precision Floating-Point) Full Name: Half-precision floating-point.\nSize: 16 bits (2 bytes).\nBit Allocation:\n1 bit for the sign.\n5 bits for the exponent.\n10 bits for the mantissa.\nPrecision: About 3 decimal digits, 2^10 = 1024, taking log10(1024) = 3 decimal digits.\n2. FP32 (Single-Precision Floating-Point) Full Name: Single-precision floating-point.\nSize: 32 bits (4 bytes).\nBit Allocation:\n1 bit for the sign.\n8 bits for the exponent.\n23 bits for the mantissa.\nPrecision: About 7 decimal digits.\nThe operation performed by Tensor Core is something like this.\n(picture) It performs 64 FMA operations per clock.\nUnderstanding matrix multiplication in tensor cores. Task:\nMatrix multiply A and B with each size 32 x 32\nlets say our tensor cores process 4x4 matrix multiplications per 1 cycle. The step to multiply A and B are\ndivide A and B into tiles of 4x4 matrices i.e 64 4x4 tile for each matrix load the tiles into shared memory (164KB) from global memory takes about 200 cycles. load the tiles into tensor core registers for operation from shared memory, take about 34 cycles. perform matrix multiplication on eight tensor cores all in parallel, take 1 cycle and then accumulate the values. total cycle 200+34 + 1 = 235 cycles. Memory Bandwidth we have seen that Tensor Cores are very fast. So fast, in fact, that they are idle most of the time as they are waiting for memory to arrive from global memory. For example, during GPT-3-sized training, which uses huge matrices — the larger, the better for Tensor Cores — we have a Tensor Core TFLOPS utilization of about 45-65%, meaning that even for the large neural networks about 50% of the time, Tensor Cores are idle.\nNvidia Ampere Architecture 108 streaming multiprocessors (SMs) 432 Tensor Cores 6,912 FP32 Cuda Cores 3,456 FP64 cuda cores (they share the same physical hardware, different in a sense that each require different clock speed and precision ) More details in the picture below.\nThese TFLOPS are calculated using this formula\nNumber of cores for that precision (FP64) x clock speed x Operations per clock (generally 1FMA or 2 Operations)\nTensor Float 32 (TF32):\n156 TFLOPS | 312 TFLOPS*: TF32 is a mixed-precision format optimized for AI workloads. It provides a balance between FP16 and FP32 precision. BFLOAT16:\n312 TFLOPS | 624 TFLOPS*: BFLOAT16 is a 16-bit floating-point format used in deep learning, offering a good trade-off between range and precision. FP16 Tensor Core:\n312 TFLOPS | 624 TFLOPS*: FP16 is used for deep learning training and inference, providing high throughput for lower-precision computations. INT8 Tensor Core:\n624 TOPS | 1248 TOPS*: INT8 is used for inference tasks, where lower precision is acceptable, and high throughput is critical. This figure shows the range and precision of each of these.\nPreferred:\nTF32 for training and mostly FP16 and BF16 for inference. How numbers are stored (example of fp16) lets store -8764.781267 in FP16 format\nfirst convert int and frac part to binary, we get\n(8764.781267)base10≈(10001000111100.110012)base2\nNormalize the binary number to the form 1.mantissa x 2^(exponent)\n10001000111100.11001​=1.000100011110011001​×2^13\nThe exponent is biased by 15 in FP16: i.e Exponent Bits=Actual Exponent+15=13+15=28\n(28)base10​=(11100)base2\nThe mantissa is the fractional part of the normalized number, truncated to 10 bits:\n(1.000100011110011001)base2→(00010001111.000100011110011001)base2​→(0001000111)base2\nThe number is negative, so the sign bit is:\nSign Bit=1Sign Bit=1\nCombine the sign bit, exponent bits, and mantissa bits:\nSign Bit: 1\nExponent Bits: 11100\nMantissa Bits: 0001000111\nThe FP16 representation is:\n11110000010001111111000001000111\n​\nTensor cores are optimized for matrix multiplications so it can peform more operations per clock rather than just 64 FMA per clock.\nSparsity Matrix contains large number of zeros in it, by using a fine-grained pruning algorithm to compress (essentially removing) small and zero-value matrices, the GPU saves computing resources, power, memory and bandwidth.\nForm Factor They define how the GPU is physically integrated into a system and how it connects to other components like the CPU and memory.\nPCIe is a standard interface used to connect GPUs, SSDs, network cards, and other peripherals to a computer’s motherboard SXM are not standalone cards and is used in data centers. Bandwidth PCIe Gen4: 64 GB/s (x16) NVLink: 600 GB/s (per GPU pair) References How do Graphics Cards Work? Exploring GPU Architecture NVIDIA A100 TENSOR CORE GPU ","permalink":"https://cohlem.github.io/sub-notes/gpus/","summary":"GPU physcial structure let\u0026rsquo;s first understand the structure of GPU.\nInside a GPU it has a chip named GA102 (depends on architecture, this is for ampere architecture) built from 28.3million transistors (semiconductor device that can switch or amplify electrical signals) and majority area covered by processing cores. processing core is divide into seven Graphics processing clusters (GPCs)\namong each GPC there are 12 Streaming Multiprocessors. Inside each SM there are 4 warps and 1 Raytracing core inside a warp there are 32 Cudas and 1 Tensor Core.","title":"GPUs"},{"content":"When we have enough resources we would want to train our neural networks in parallel, the way to do this is to train our NN with different data (different batches of data) in each GPU in parallel. For instance, if we have 8X A100 we run 8 different batches of data on each A100 GPU.\nThe way to do this in pytorch is to use DDP (take a look into their docs)\nThe important thing to be careful about is that when we train our NN in different GPUs, each GPU calculates gradient and that gradient is averaged among all the all the gradients calculated from each GPUs and then deposited on each of the gpu and then we do the descent step.\nWhy do we do this ?\nSo that our weights become consistent and mathematically equivalent to when we train on 8x the batch size but on same GPU.\nAt first I was confused how would this be equivalent to the gradients when we trained it on single GPU but with 8x the batch size.\nHere\u0026rsquo;s a simple mathematical formula.\nFIGURE1 But lets look at our manual backprogpagation which will help us understand better.\nForward pass n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 64 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) # Layer 1 W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) b1 = torch.randn(n_hidden, generator=g) * 0.1 # using b1 just for fun, it\u0026#39;s useless because of BN # Layer 2 W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1 b2 = torch.randn(vocab_size, generator=g) * 0.1 # BatchNorm parameters bngain = torch.randn((1, n_hidden))*0.1 + 1.0 bnbias = torch.randn((1, n_hidden))*0.1 # Note: I am initializating many of these parameters in non-standard ways # because sometimes initializating with e.g. all zeros could mask an incorrect # implementation of the backward pass. parameters = [C, W1, b1, W2, b2, bngain, bnbias] print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters: p.requires_grad = True batch_size = 32 n = batch_size # a shorter variable also, for convenience # construct a minibatch ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y # forward pass, \u0026#34;chunkated\u0026#34; into smaller steps that are possible to backward one at a time emb = C[Xb] # embed the characters into vectors embcat = emb.view(emb.shape[0], -1) # concatenate the vectors # Linear layer 1 hprebn = embcat @ W1 + b1 # hidden layer pre-activation # BatchNorm layer bnmeani = 1/n*hprebn.sum(0, keepdim=True) bndiff = hprebn - bnmeani bndiff2 = bndiff**2 bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel\u0026#39;s correction (dividing by n-1, not n) bnvar_inv = (bnvar + 1e-5)**-0.5 bnraw = bndiff * bnvar_inv hpreact = bngain * bnraw + bnbias # Non-linearity h = torch.tanh(hpreact) # hidden layer # Linear layer 2 logits = h @ W2 + b2 # output layer # cross entropy loss (same as F.cross_entropy(logits, Yb)) logit_maxes = logits.max(1, keepdim=True).values norm_logits = logits - logit_maxes # subtract max for numerical stability counts = norm_logits.exp() counts_sum = counts.sum(1, keepdims=True) counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can\u0026#39;t get backprop to be bit exact... probs = counts * counts_sum_inv logprobs = probs.log() loss = -logprobs[range(n), Yb].mean() # PyTorch backward pass for p in parameters: p.grad = None for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way norm_logits, logit_maxes, logits, h, hpreact, bnraw, bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani, embcat, emb]: t.retain_grad() loss.backward() loss Backprop # Exercise 1: backprop through the whole thing manually, # backpropagating through exactly all of the variables # as they are defined in the forward pass above, one by one # ----------------- # YOUR CODE HERE :) dlogprobs = torch.zeros_like(logprobs) dlogprobs[range(n), Yb] = -1.0*(1/logprobs.shape[0]) # 1 \u0026lt;=== look here dprobs = (1/probs)*dlogprobs # 2 dcounts_sum_inv = (dprobs*counts).sum(1, keepdim = True) dcounts = dprobs * counts_sum_inv dcounts_sum = -1.0*((counts_sum)**(-2.0))*dcounts_sum_inv dcounts += torch.ones_like(counts_sum)*dcounts_sum dnorm_logits = norm_logits.exp()*dcounts dlogit_maxes = (-1.0*dnorm_logits).sum(1,keepdim=True) dlogits = (1.0*dnorm_logits) dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1])*dlogit_maxes db2 = (dlogits*torch.ones_like(logits)).sum(0) dh = dlogits @ W2.T dW2 = h.T @ dlogits dhpreact = dh*(1-h**(2)) dbnbias = (dhpreact*torch.ones_like(bnraw)).sum(0, keepdim= True) dbngain = (dhpreact*bnraw*torch.ones_like(bnraw)).sum(0, keepdim=True) dbnraw = dhpreact*bngain*torch.ones_like(bnraw) dbnvar_inv = (dbnraw* (torch.ones_like(bndiff) * bndiff)).sum(0, keepdim=True) dbndiff = (dbnraw* (torch.ones_like(bndiff) * bnvar_inv)) dbnvar = dbnvar_inv* (-0.5)*(((bnvar + 1e-5))**(-1.5)) dbndiff2 = (1.0/(n-1) )*torch.ones_like(bndiff2) * dbnvar dbndiff += dbndiff2*2*(bndiff) dhprebn = dbndiff*1.0 dbnmeani = (torch.ones_like(hprebn)*-1.0*dbndiff).sum(0, keepdim = True) dhprebn += torch.ones_like(hprebn)*(1/n)*dbnmeani db1 = (torch.ones_like(dhprebn)*dhprebn).sum(0) dembcat = dhprebn @ W1.T dW1 = embcat.T @ dhprebn demb = dembcat.view(emb.shape[0],emb.shape[1],emb.shape[2]) dC = torch.zeros_like(C) for i in range(Xb.shape[0]): for j in range(Xb.shape[1]): dC[Xb[i,j]] += demb[i,j] # print(demb[i,j].shape) # ----------------- cmp(\u0026#39;logprobs\u0026#39;, dlogprobs, logprobs) cmp(\u0026#39;probs\u0026#39;, dprobs, probs) cmp(\u0026#39;counts_sum_inv\u0026#39;, dcounts_sum_inv, counts_sum_inv) cmp(\u0026#39;counts_sum\u0026#39;, dcounts_sum, counts_sum) cmp(\u0026#39;counts\u0026#39;, dcounts, counts) cmp(\u0026#39;norm_logits\u0026#39;, dnorm_logits, norm_logits) cmp(\u0026#39;logit_maxes\u0026#39;, dlogit_maxes, logit_maxes) cmp(\u0026#39;logits\u0026#39;, dlogits, logits) cmp(\u0026#39;h\u0026#39;, dh, h) cmp(\u0026#39;W2\u0026#39;, dW2, W2) cmp(\u0026#39;b2\u0026#39;, db2, b2) cmp(\u0026#39;hpreact\u0026#39;, dhpreact, hpreact) cmp(\u0026#39;bngain\u0026#39;, dbngain, bngain) cmp(\u0026#39;bnbias\u0026#39;, dbnbias, bnbias) cmp(\u0026#39;bnraw\u0026#39;, dbnraw, bnraw) cmp(\u0026#39;bnvar_inv\u0026#39;, dbnvar_inv, bnvar_inv) cmp(\u0026#39;bnvar\u0026#39;, dbnvar, bnvar) cmp(\u0026#39;bndiff2\u0026#39;, dbndiff2, bndiff2) cmp(\u0026#39;bndiff\u0026#39;, dbndiff, bndiff) cmp(\u0026#39;bnmeani\u0026#39;, dbnmeani, bnmeani) cmp(\u0026#39;hprebn\u0026#39;, dhprebn, hprebn) cmp(\u0026#39;embcat\u0026#39;, dembcat, embcat) cmp(\u0026#39;W1\u0026#39;, dW1, W1) cmp(\u0026#39;b1\u0026#39;, db1, b1) cmp(\u0026#39;emb\u0026#39;, demb, emb) cmp(\u0026#39;C\u0026#39;, dC, C) lets look at our last line of code in forward pass\nloss = -logprobs[range(n), Yb].mean() what we do is simply the cross entropy i.e normalize using softmax function and pluck out the log probability from the output token\u0026rsquo;s index.\nand calculate it\u0026rsquo;s derivative here in second line of code in the backward pass like this.\ndlogprobs[range(n), Yb] = -1.0*(1/logprobs.shape[0]) # 1 \u0026lt;=== look here what we\u0026rsquo;re doing is simply calculating the average over all the batches and that average is deposited in each element in [range(n), Yb] .\nWhy do we do the average? dL/dlogprobs = 1 (which comes from dL/dL) x d(dlogprobs[range(n), Yb])/dlogprobs\nsince the elements in this range [range(n), Yb]) is averaged, each element will get (1/total_number) x itself\nso the local derivative of (1/total_number) x iteself w.r.t itself is 1/total_number which will be now deposited into the elements of dlogprobs[range(n), Yb]\nlet\u0026rsquo;s stop right here and this matches our first equation in FIGURE1, and if we do this operation for 8 times more batches on single GPU we get the second equation in FIGURE1. If we look at this code, we can see that we are only increasing the n value here.\ndlogprobs[range(8*n), Yb] = -1.0*(1/logprobs.shape[0]) # 1 \u0026lt;=== look here So from this it\u0026rsquo;s safe to say that training on multiple GPUs and averaging their gradients is same as training on single GPU with 8 times more batches.\n","permalink":"https://cohlem.github.io/sub-notes/ddp/","summary":"When we have enough resources we would want to train our neural networks in parallel, the way to do this is to train our NN with different data (different batches of data) in each GPU in parallel. For instance, if we have 8X A100 we run 8 different batches of data on each A100 GPU.\nThe way to do this in pytorch is to use DDP (take a look into their docs)","title":"DDP and gradient sync"},{"content":"Training Neural Networks Karpathy\u0026rsquo;s advice while training NN\nDeep Learning Concepts Contains simple explanation for DL concepts\nHow to scale your LLM (Must read) https://jax-ml.github.io/scaling-book/\nThe Ultra-Scale Playbook: Training LLMs on GPU Clusters https://huggingface.co/spaces/nanotron/ultrascale-playbook\nGood coding style https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd\nHow to sample from LLM (top-k, top-p) https://huggingface.co/blog/how-to-generate\nKL Divergence https://www.youtube.com/watch?v=q0AkK8aYbLY\nThe meaning of Loss functions https://jiha-kim.github.io/posts/the-mean-ing-of-loss-functions/\n","permalink":"https://cohlem.github.io/notes/essential-blogs/","summary":"Training Neural Networks Karpathy\u0026rsquo;s advice while training NN\nDeep Learning Concepts Contains simple explanation for DL concepts\nHow to scale your LLM (Must read) https://jax-ml.github.io/scaling-book/\nThe Ultra-Scale Playbook: Training LLMs on GPU Clusters https://huggingface.co/spaces/nanotron/ultrascale-playbook\nGood coding style https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd\nHow to sample from LLM (top-k, top-p) https://huggingface.co/blog/how-to-generate\nKL Divergence https://www.youtube.com/watch?v=q0AkK8aYbLY\nThe meaning of Loss functions https://jiha-kim.github.io/posts/the-mean-ing-of-loss-functions/","title":"Essential blogs"},{"content":"Gradient Accumulation When we want to train a neural network with some predefined set of tokens, but don\u0026rsquo;t have enough GPU resources, what do we do?\nGradient Accumulation We simply accumulate the gradients. For instance, in order to reproduce GPT-2 124B, we need to train the model with 0.5 Million tokens in a single run with 1024 context length, we would need 0.5e6/ 1024 = 488 batches i.e B,T = (488,1024) to calculate the gradients and update them.\nWe don\u0026rsquo;t have the enough resources, to fit those batches in our GPU, now what we do is divide that 488 into multiple batches and then do the forward pass and calculate gradients and accumulate (+) the gradients but we don\u0026rsquo;t update the gradients until we reach the end of the the desired batch size. After that, we update the parameters once.\n","permalink":"https://cohlem.github.io/sub-notes/gradient-accumulation/","summary":"Gradient Accumulation When we want to train a neural network with some predefined set of tokens, but don\u0026rsquo;t have enough GPU resources, what do we do?\nGradient Accumulation We simply accumulate the gradients. For instance, in order to reproduce GPT-2 124B, we need to train the model with 0.5 Million tokens in a single run with 1024 context length, we would need 0.5e6/ 1024 = 488 batches i.e B,T = (488,1024) to calculate the gradients and update them.","title":"Gradient Accumulation"},{"content":"Precision The more the precision point the less operation (TFLOPS) is performed.\nFP64 used for scientific research purposes, where precision is a must. TF32 and BFLOAT16 are mostly used in NN training. INT8 is used for inference. Picture below shows specifications of A100 GPU.\nUsing these precision points may have some difference in code. See pytorch\u0026rsquo;s docs\ntorch.compile It works in a similar fashion like the GCC compiler. It works by reducing overheads introduced by the python interpreter and optimizing the GPU read and writes.\nFor instance\ndef gelu(x): \u0026#34;\u0026#34;\u0026#34; Applies the GELU activation function to the input. \u0026#34;\u0026#34;\u0026#34; return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) First this operation resides in GPU\u0026rsquo;s HBM memory, and this part of calculation \u0026ldquo;torch.pow(x, 3)\u0026rdquo; is passed to GPU and it performs the operations, one by one, the instructions are sent from HBM to GPU cores and transferred back to HBM one by one. But torch.compiles evaluates that the code is simply operation on input x and some +,* and transfers the code to GPU once and does all the operation and send it back to HBM, in this way it optimizes the training process.\nFlash attention It is somewhat similar to torch.compile\u0026rsquo;s process but torch.compile itself cannot comprehend our code(shown below) to perform the optimization.\naw = (Q @ torch.transpose(K, -2,-1)) # for matmul dim of q should be B,T,C and k should be B,C,T aw = aw/(K.shape[-1] **0.5) mask = self.tril[:,:,:T,:T] == 0 # generate mask aw = aw.masked_fill_(mask, float(\u0026#39;-inf\u0026#39;)) # apply mask i.e fill true values with -inf aw = torch.softmax(aw,dim=-1) # -inf values are converted to 0 and then each row is normalized cv = aw @ V # context vector We have to call torch.nn.functional.scaled_dot_product_attention combined with torch.compile to use FlashAttention in our code.\nRemove ugly numbers. Always include numbers in our code that have powers of 2 in it.\nfor instance 16,32,64 work best.\nImprovements\nfor instance, while training GPT-2 our vocab_size is 50257\nif we factorize it has divisors\n1 | 29 | 1733 50257 None of it have powers of 2, so the GPU performs operation on that matrix by truncating til the last powers of 2 and then doing the operation on the remaining parts, which is inefficient. We can simply increase that number to be a closed number that has powers of 2 such as 50304 = 2^7 × 3 x 131 which has high number of power of 2.\nWe can simply increase the training speed by making our numbers in our code have more powers of 2.\n","permalink":"https://cohlem.github.io/sub-notes/training-speed-optimization/","summary":"Precision The more the precision point the less operation (TFLOPS) is performed.\nFP64 used for scientific research purposes, where precision is a must. TF32 and BFLOAT16 are mostly used in NN training. INT8 is used for inference. Picture below shows specifications of A100 GPU.\nUsing these precision points may have some difference in code. See pytorch\u0026rsquo;s docs\ntorch.compile It works in a similar fashion like the GCC compiler. It works by reducing overheads introduced by the python interpreter and optimizing the GPU read and writes.","title":"Training Speed Optimization"},{"content":"Skip connections are simply skipping the layers by adding the identity of input to it\u0026rsquo;s output as shown in the figure below.\nWhy add the identity of input x to the output ? We calculate the gradients of parameters using chain rule, as shown in figure above. For deeper layers the gradient start to become close to 0 and the gradient stops propagating, which is a vanishing gradient problem in a deep neural networks.\nWhen we add the identity of input to it\u0026rsquo;s output like this hl+1​=F(hl​)+hl​\nand when we backpropagate, we get this type of equation. So even when d(F(h1))/dh1 becomes close to 0 the previous gradient dL/dhl+1 is propagated as it is multiplied with 1 which is the outcome of adding the identity of input to its output.\nEventually, skip connections stop vanishing gradient problem, and the other thing they help with is that, when going to a residual block(could be attention block or feedforward block) neural network may loose previous information when going through that transformation, so adding identity of input to its output will take into consideration the new learned features + the previous features.\n","permalink":"https://cohlem.github.io/sub-notes/skip-connections/","summary":"Skip connections are simply skipping the layers by adding the identity of input to it\u0026rsquo;s output as shown in the figure below.\nWhy add the identity of input x to the output ? We calculate the gradients of parameters using chain rule, as shown in figure above. For deeper layers the gradient start to become close to 0 and the gradient stops propagating, which is a vanishing gradient problem in a deep neural networks.","title":"skip-connections"},{"content":"The simplest algorithm is the gradient descent in which we simply calculate loss over all the training data and then update our parameters, but it would be too slow and would consume too much resources. A faster approach is to use SGD where we calculate loss over every single training data and then do the parameter update, but the gradient update could be fuzzy. A more robust approach is to do mini batch SGD.\nThere are different types of optimizers with distinct capabilities.\nSGD with momentum We have one addition in this optimizer i.e velocity term, it accumulates the velocity of previous gradient and move forward with that momentum. I\u0026rsquo;ve tried to see how it builds up velocity in the image below. As we can see gradient update depends not only on it\u0026rsquo;s current gradient but also it\u0026rsquo;s previous weights which provide the instance (t+1) with some momentum i.e gradient from previous steps (t, t-1, t-2 and so on). As we move towards higher iterations (t=100000) the effect of initial gradients i.e t=0, t=1 and so on becomes 0 because beta term is raised to the power t=100000, and only the closest gradients are fully taken into considerations. For instance, we can see in the image below how low priority is given to the previous weights i.e 0.729 to W0, and 0.081 to W1 and so on.\nWeakness same learning rate is applied to all the parameters because of which the update to the parameters isn\u0026rsquo;t precise. RMSprop This optimizer tries to solve the problem of SGD with momentum i.e it tunes the learning rate based on it\u0026rsquo;s gradient. It sets the effect of learning rate based on it\u0026rsquo;s v term which is simply the accumulation of previous weights.\nAs you can see in the calculation done in left hand side the gradient accumulation i.e v is small so the effect of learning rate is bigger (we take bigger jumps), we take bigger jumps because we don\u0026rsquo;t want our convergence to be too slow or to stagnate, so we take bigger jumps.\nBut when the gradient accumulation i.e v term is bigger (as shown in right hand side) we take smaller steps, because in those cases if we take a big jump we might miss the global minima, so the effect of learning rate in this case is decreased. Adam This optimizer is simply the combination of both the momentum and RMSprop. It has its own speed determined by momentum and the learning rate adjustment provided by RMSprop.\nThe only modification is the addition of M hat and V hat i.e we scale of M and V, because initially we set the value of M and V to 0. The explanation about why we this is also provided in the image below. ","permalink":"https://cohlem.github.io/sub-notes/optimization-algorithms/","summary":"The simplest algorithm is the gradient descent in which we simply calculate loss over all the training data and then update our parameters, but it would be too slow and would consume too much resources. A faster approach is to use SGD where we calculate loss over every single training data and then do the parameter update, but the gradient update could be fuzzy. A more robust approach is to do mini batch SGD.","title":"Optimization Algorithms (SGD with momentum, RMSProp, Adam)"},{"content":"Main code n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 64 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) # Layer 1 W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) b1 = torch.randn(n_hidden, generator=g) * 0.1 # using b1 just for fun, it\u0026#39;s useless because of BN # Layer 2 W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1 b2 = torch.randn(vocab_size, generator=g) * 0.1 # BatchNorm parameters bngain = torch.randn((1, n_hidden))*0.1 + 1.0 bnbias = torch.randn((1, n_hidden))*0.1 # Note: I am initializating many of these parameters in non-standard ways # because sometimes initializating with e.g. all zeros could mask an incorrect # implementation of the backward pass. parameters = [C, W1, b1, W2, b2, bngain, bnbias] print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters: p.requires_grad = True batch_size = 32 n = batch_size # a shorter variable also, for convenience # construct a minibatch ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y # forward pass, \u0026#34;chunkated\u0026#34; into smaller steps that are possible to backward one at a time emb = C[Xb] # embed the characters into vectors embcat = emb.view(emb.shape[0], -1) # concatenate the vectors # Linear layer 1 hprebn = embcat @ W1 + b1 # hidden layer pre-activation # BatchNorm layer bnmeani = 1/n*hprebn.sum(0, keepdim=True) bndiff = hprebn - bnmeani bndiff2 = bndiff**2 bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel\u0026#39;s correction (dividing by n-1, not n) bnvar_inv = (bnvar + 1e-5)**-0.5 bnraw = bndiff * bnvar_inv hpreact = bngain * bnraw + bnbias # Non-linearity h = torch.tanh(hpreact) # hidden layer # Linear layer 2 logits = h @ W2 + b2 # output layer # cross entropy loss (same as F.cross_entropy(logits, Yb)) logit_maxes = logits.max(1, keepdim=True).values norm_logits = logits - logit_maxes # subtract max for numerical stability counts = norm_logits.exp() counts_sum = counts.sum(1, keepdims=True) counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can\u0026#39;t get backprop to be bit exact... probs = counts * counts_sum_inv logprobs = probs.log() loss = -logprobs[range(n), Yb].mean() # PyTorch backward pass for p in parameters: p.grad = None for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way norm_logits, logit_maxes, logits, h, hpreact, bnraw, bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani, embcat, emb]: t.retain_grad() loss.backward() loss Initially we have this forward pass of a NN, how do we backpropagate through this? We simply call loss.backward() which is an abstraction of pytorch\u0026rsquo;s autograd engine, it\u0026rsquo;ll construct computation graph and calculate gradients for all the nodes under the hood.\nHow can we do it manually?\nhere\u0026rsquo;s how\nManual Backprop # Exercise 1: backprop through the whole thing manually, # backpropagating through exactly all of the variables # as they are defined in the forward pass above, one by one # ----------------- # YOUR CODE HERE :) dlogprobs = torch.zeros_like(logprobs) dlogprobs[range(n), Yb] = -1.0*(1/logprobs.shape[0]) # 1 dprobs = (1/probs)*dlogprobs # 2 dcounts_sum_inv = (dprobs*counts).sum(1, keepdim = True) dcounts = dprobs * counts_sum_inv dcounts_sum = -1.0*((counts_sum)**(-2.0))*dcounts_sum_inv dcounts += torch.ones_like(counts_sum)*dcounts_sum dnorm_logits = norm_logits.exp()*dcounts dlogit_maxes = (-1.0*dnorm_logits).sum(1,keepdim=True) dlogits = (1.0*dnorm_logits) dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1])*dlogit_maxes db2 = (dlogits*torch.ones_like(logits)).sum(0) dh = dlogits @ W2.T dW2 = h.T @ dlogits dhpreact = dh*(1-h**(2)) dbnbias = (dhpreact*torch.ones_like(bnraw)).sum(0, keepdim= True) dbngain = (dhpreact*bnraw*torch.ones_like(bnraw)).sum(0, keepdim=True) dbnraw = dhpreact*bngain*torch.ones_like(bnraw) dbnvar_inv = (dbnraw* (torch.ones_like(bndiff) * bndiff)).sum(0, keepdim=True) dbndiff = (dbnraw* (torch.ones_like(bndiff) * bnvar_inv)) dbnvar = dbnvar_inv* (-0.5)*(((bnvar + 1e-5))**(-1.5)) dbndiff2 = (1.0/(n-1) )*torch.ones_like(bndiff2) * dbnvar dbndiff += dbndiff2*2*(bndiff) dhprebn = dbndiff*1.0 dbnmeani = (torch.ones_like(hprebn)*-1.0*dbndiff).sum(0, keepdim = True) dhprebn += torch.ones_like(hprebn)*(1/n)*dbnmeani db1 = (torch.ones_like(dhprebn)*dhprebn).sum(0) dembcat = dhprebn @ W1.T dW1 = embcat.T @ dhprebn demb = dembcat.view(emb.shape[0],emb.shape[1],emb.shape[2]) dC = torch.zeros_like(C) for i in range(Xb.shape[0]): for j in range(Xb.shape[1]): dC[Xb[i,j]] += demb[i,j] # print(demb[i,j].shape) # ----------------- cmp(\u0026#39;logprobs\u0026#39;, dlogprobs, logprobs) cmp(\u0026#39;probs\u0026#39;, dprobs, probs) cmp(\u0026#39;counts_sum_inv\u0026#39;, dcounts_sum_inv, counts_sum_inv) cmp(\u0026#39;counts_sum\u0026#39;, dcounts_sum, counts_sum) cmp(\u0026#39;counts\u0026#39;, dcounts, counts) cmp(\u0026#39;norm_logits\u0026#39;, dnorm_logits, norm_logits) cmp(\u0026#39;logit_maxes\u0026#39;, dlogit_maxes, logit_maxes) cmp(\u0026#39;logits\u0026#39;, dlogits, logits) cmp(\u0026#39;h\u0026#39;, dh, h) cmp(\u0026#39;W2\u0026#39;, dW2, W2) cmp(\u0026#39;b2\u0026#39;, db2, b2) cmp(\u0026#39;hpreact\u0026#39;, dhpreact, hpreact) cmp(\u0026#39;bngain\u0026#39;, dbngain, bngain) cmp(\u0026#39;bnbias\u0026#39;, dbnbias, bnbias) cmp(\u0026#39;bnraw\u0026#39;, dbnraw, bnraw) cmp(\u0026#39;bnvar_inv\u0026#39;, dbnvar_inv, bnvar_inv) cmp(\u0026#39;bnvar\u0026#39;, dbnvar, bnvar) cmp(\u0026#39;bndiff2\u0026#39;, dbndiff2, bndiff2) cmp(\u0026#39;bndiff\u0026#39;, dbndiff, bndiff) cmp(\u0026#39;bnmeani\u0026#39;, dbnmeani, bnmeani) cmp(\u0026#39;hprebn\u0026#39;, dhprebn, hprebn) cmp(\u0026#39;embcat\u0026#39;, dembcat, embcat) cmp(\u0026#39;W1\u0026#39;, dW1, W1) cmp(\u0026#39;b1\u0026#39;, db1, b1) cmp(\u0026#39;emb\u0026#39;, demb, emb) cmp(\u0026#39;C\u0026#39;, dC, C) Results logprobs | exact: True | approximate: True | maxdiff: 0.0 probs | exact: True | approximate: True | maxdiff: 0.0 counts_sum_inv | exact: True | approximate: True | maxdiff: 0.0 counts_sum | exact: True | approximate: True | maxdiff: 0.0 counts | exact: True | approximate: True | maxdiff: 0.0 norm_logits | exact: True | approximate: True | maxdiff: 0.0 logit_maxes | exact: True | approximate: True | maxdiff: 0.0 logits | exact: True | approximate: True | maxdiff: 0.0 h | exact: True | approximate: True | maxdiff: 0.0 W2 | exact: True | approximate: True | maxdiff: 0.0 b2 | exact: True | approximate: True | maxdiff: 0.0 hpreact | exact: True | approximate: True | maxdiff: 0.0 bngain | exact: True | approximate: True | maxdiff: 0.0 bnbias | exact: True | approximate: True | maxdiff: 0.0 bnraw | exact: True | approximate: True | maxdiff: 0.0 bnvar_inv | exact: True | approximate: True | maxdiff: 0.0 bnvar | exact: True | approximate: True | maxdiff: 0.0 bndiff2 | exact: True | approximate: True | maxdiff: 0.0 bndiff | exact: True | approximate: True | maxdiff: 0.0 bnmeani | exact: True | approximate: True | maxdiff: 0.0 hprebn | exact: True | approximate: True | maxdiff: 0.0 embcat | exact: True | approximate: True | maxdiff: 0.0 W1 | exact: True | approximate: True | maxdiff: 0.0 b1 | exact: True | approximate: True | maxdiff: 0.0 emb | exact: True | approximate: True | maxdiff: 0.0 C | exact: True | approximate: True | maxdiff: 0.0 The result verifies that our gradients matches pytorch\u0026rsquo;s.\nStep-by-step calculation Backpropagating on scalars is pretty straightforward as we did in our first note but when it comes to tensors, we need to make sure every element\u0026rsquo;s gradient in a tensor is calculated precisely.\nlet\u0026rsquo;s understand it line by line.\nloss = -logprobs[range(n), Yb].mean() now we calculate the derivative of loss (L) w.r.t logprobs, (NOTE: d(loss)/d(loss) is 1). here\nwe have: d(L)/dL to find: d(L)/dlogprobs\nd(L)/dlogprobs = d(L)/dL x d(L)/dlogprobs # d(L)/dlogprobs is local gradient d(L)/dlogprobs = 1.0 * d(L)/dlogprobs Now what could be the d(L)/dlogprobs? let\u0026rsquo;s break it down by representing it into a simple matrix. let\u0026rsquo;s say logprobs is a matrix and using indexing [range(n), Yb] we pluck out it\u0026rsquo;s corresponding values and then we average it. let\u0026rsquo;s consider the plucked out values are\na1 , b1 , c1 it\u0026rsquo;s mean would be\n1/3 x a1 + 1/3 x b1 + 1/3 x c1 the derivative of d(L)/da1 = 1/3 , d(L)/db1 = 1/3 , d(L)/dc1 = 1/3 we see a pattern here, derivate of every element is 1/total_no_of_elements.\nso\ndlogprobs = torch.zeros_like(logprobs) # because all the other elements will have 0 gradient as they\u0026#39;ll be considered constant dlogprobs[range(n), Yb] = 1.0 * 1/(-logprobs).shape[0] logprobs = probs.log() to find : dprobs\nwe know d(logx)/dx = 1/x, so its fairly simple.\ndprobs = 1/probs * dlogprobs # don\u0026#39;t forget to add the dlogprobs because its the gradient\u0026#39;s that propagated probs = counts * counts_sum_inv let\u0026rsquo;s find dcounts_sum_inv we need to make sure that the gradient of any tensor should have the same size as that tensor. the shape of counts_sum_inv is (32,1)\ndcounts_sum_inv = (dprobs *torch.ones_like(count) * counts).sum(1, keepdim = True) why do we sum it across rows? this is because in probs = counts * counts_sum_inv,\ncounts has shape (32,27) and counts_sum_inv has (32,1), so first the counts_sum_inv is broadcasted and is made into shape (32,27) by copying the column and then finally is multiplied with counts. There are two operations that take place in order (broadcasting and addition). So, when we backpropagate through this equation the order should be addition and broadcasting. so the dcounts_sum_inv is (dprobs * torch.ones_like(count) * counts), but this is of shape (32,27), as we have seen the columns in counts_sum_inv are broadcasted, which mean one column is used 27 times, so we know that from our first note that when a variable is used more than once it\u0026rsquo;s derivate is added up, so we sum across the rows (sum it 27 times).\ndcounts_sum_inv = (dprobs* torch.ones_like(count) * counts).sum(1, keepdim = True) Similarly, we can now calculate the gradients for tensors that were broadcasted in our forward pass. All the other gradient calculation is relatively straightforward except this equation\nlogits = h @ W2 + b2 # output layer why? because if we go deep into matrix multiplication, we see there are two operations involved i.e multiplication and addition. The formula for calculating gradient for the equation above is derived in the picture below.\nWe come up with a simple equation.\ndh = dlogits @ W2.T dW2 = h.T @ dlogits I believe these were the main gradient calculation steps and gradients for other nodes can be calculated in a similar manner.\nA more detailed code can be found here\n","permalink":"https://cohlem.github.io/sub-notes/manual-backpropagation-on-tensors/","summary":"Main code n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 64 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) # Layer 1 W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) b1 = torch.randn(n_hidden, generator=g) * 0.1 # using b1 just for fun, it\u0026#39;s useless because of BN # Layer 2 W2 = torch.","title":"manual-backpropagation-on-tensors"},{"content":"In deep learning, it\u0026rsquo;s important to visualize a matrix and how it is represented in a dimension space because the operations that we perform on those matrix becomes very much intuitive afterwards.\nVisualizing two dimensional matrix. This has to be the most intuitive visualization.\n[ [12, 63, 10, 42, 70, 31, 34, 8, 34, 5], [10, 97, 100, 39, 64, 25, 86, 22, 31, 25], [28, 44, 82, 61, 70, 94, 22, 88, 89, 56] ] We can simply imagine rows are some examples and columns as those examples\u0026rsquo; features.\nVisualizing three dimensional matrix. let\u0026rsquo;s imagine we have a matrix M with the values\nM = [ [ [1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12] ], [ [13, 14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24] ] ] it is of dimension (2,4,3). How do we visualize it? The key to visualizing a three dimensional matrix is to image it like a two dimensional matrix but only that it\u0026rsquo;s first dimension is stacked behind one after another.\nfor instance, for the same matrix image it like it is (4,3) matrix but two of these are stacked behind one after another.\nlet\u0026rsquo;s understand it using an image.\nas you can see from this size of the view we can mostly see its last two dimensions i.e (4,3).\nLet\u0026rsquo;s take another view. as you can see that 2 examples are stacked behind one another.\nTo visualize it in 3D you can take visit this website https://array-3d-viz.vercel.app\n","permalink":"https://cohlem.github.io/sub-notes/matrix-visualization/","summary":"In deep learning, it\u0026rsquo;s important to visualize a matrix and how it is represented in a dimension space because the operations that we perform on those matrix becomes very much intuitive afterwards.\nVisualizing two dimensional matrix. This has to be the most intuitive visualization.\n[ [12, 63, 10, 42, 70, 31, 34, 8, 34, 5], [10, 97, 100, 39, 64, 25, 86, 22, 31, 25], [28, 44, 82, 61, 70, 94, 22, 88, 89, 56] ] We can simply imagine rows are some examples and columns as those examples\u0026rsquo; features.","title":"Matrix Visualization"},{"content":"source: Building makemore Part 3: Activations \u0026amp; Gradients, BatchNorm\nThings to look out for while training NN Take a look at previous notes to understand this note better\nconsider we have this simple 6 layer NN\n# Linear Layer g = torch.Generator().manual_seed(2147483647) # for reproducibility class Layer: def __init__(self,fan_in, fan_out, bias=False): self.w = torch.randn((fan_in, fan_out),generator = g) / (fan_in)**(0.5) # applying kaiming init self.bias = bias if bias: self.b = torch.zeros(fan_out) def __call__(self, x): y = x @ self.w self.out = y + self.b if self.bias else y return self.out def parameters(self): return [self.w] + [self.b] if self.bias else [self.w] class Tanh: def __call__(self, x): self.out = torch.tanh(x) return self.out def parameters(self): return [] class BatchNormalization1: def __init__(self,nf, eps= 1e-5, mom=0.1): self.bngain = torch.ones(nf) self.bnbias = torch.zeros(nf) self.out = None self.mom = mom self.training = True self.running_mean = torch.ones(nf) self.running_var = torch.zeros(nf) self.eps = eps def __call__(self,x): if self.training: meani = x.mean(0, keepdim = True) vari = x.var(0, keepdim = True) else: meani = self.running_mean vari = self.running_var if self.training: with torch.no_grad(): self.running_mean = (1-self.mom)*self.running_mean + self.mom*meani self.running_var = (1-self.mom)*self.running_var + self.mom*vari self.out = self.bngain *((x - meani)/ torch.sqrt(vari + self.eps)) + self.bnbias return self.out def parameters(self): return [self.bngain, self.bnbias] Structure\nimport torch.nn.functional as F x = torch.randn(32, 30, generator = g) y = torch.tensor([random.randint(0,26) for _ in range(32)] ) # Embedding layer, n_embd = 10 n_vocab = 27 n_dim = 100 batch_size = 32 C = torch.randn((n_vocab,n_embd)) st = [ # x shape = 32, 30 Layer(n_embd*block_size,n_dim), Tanh(), Layer(n_dim, n_dim), Tanh(), Layer(n_dim, n_dim) , Tanh(), Layer(n_dim, n_dim), Tanh(), Layer(n_dim, n_dim), Tanh(), Layer(n_dim, n_vocab),BatchNormalization1(n_vocab) ] with torch.no_grad(): st[-1].bngain *= 0.1 for layer in st[:-1]: if isinstance(layer, Layer): layer.w *= 5/3 parameters = [C] + [p for l in st for p in l.parameters()] for p in parameters: p.requires_grad = True Training Loop\nfor iteration in range(200000): # for iteration in range(2000): idx = torch.randint(0,Xtr.shape[0], (batch_size,)) x_emb = C[Xtr[idx]].view(-1, block_size * n_embd) x = x_emb for idx,item in enumerate(st): # print(idx) x = item(x) loss = F.cross_entropy(x,y) for layer in st: layer.out.retain_grad() for p in parameters: p.grad = None loss.backward() lr = 0.1 if iteration \u0026lt; 150000 else 0.01 for p in parameters: p.data += -lr*p.grad if iteration % 10000 ==0: print(loss.data) # if iteration \u0026gt;= 1000: # break let\u0026rsquo;s look at our activations before initializing weights using kaiming init.\n# these are just part of modified code from the code that\u0026#39;s given above. class Layer: def __init__(self,fan_in, fan_out, bias=False): self.w = torch.randn((fan_in, fan_out),generator = g) # / (fan_in)**(0.5) # commenting our the kaiming init # part of code with torch.no_grad(): st[-1].bngain *= 0.1 for layer in st[:-1]: if isinstance(layer, Layer): layer.w *= 1.0 # setting gains to 1.0 (no gain) Activation plot As you can see almost all the pre activations are saturated, this is because our weight is initialized in such a way that after applying tanh, most of our output values lie in -1 and 1, which will stop gradient propagation.\nNow applying kaiming init with with no gain.\n# these are just part of modified code from the code that\u0026#39;s given above. class Layer: def __init__(self,fan_in, fan_out, bias=False): self.w = torch.randn((fan_in, fan_out),generator = g) / (fan_in)**(0.5) # applying the kaiming init # part of code with torch.no_grad(): st[-1].bngain *= 0.1 for layer in st[:-1]: if isinstance(layer, Layer): layer.w *= 1.0 # setting gains to 1.0 (no gain) The plot is starting to look nicer, because there is less saturation, because now values don\u0026rsquo;t lie in the extreme values of tanh, and gradient will be propagated. But we still have issue, as we can see the standard deviation is decreasing this is because of the property of tanh, i.e it squashes the values, initially (blue plot) the output was decent but in later layers, the distribution is being shrinked that because of the property of tanh.\nnow let\u0026rsquo;s apply kaiming init with gain too, for tanh the gain is 5/3.\nNow the values are being evenly distributed, and the standard deviation is stable (doesn\u0026rsquo;t decrease with iteration).\nWe have to precisely measure the gains to have a stable training. But the introduction of batch normalization changes the case, and we don\u0026rsquo;t have to be that much aware for precisely initializing weights.\nLet\u0026rsquo;s now apply the batch normalization but without kaiming init and see the same plot.\nst = [ # x shape = 32, 30 Layer(n_embd*block_size,n_dim), BatchNormalization1(n_dim), Tanh(), Layer(n_dim, n_dim), BatchNormalization1(n_dim), Tanh(), Layer(n_dim, n_dim), BatchNormalization1(n_dim), Tanh(), Layer(n_dim, n_dim), BatchNormalization1(n_dim), Tanh(), Layer(n_dim, n_dim), BatchNormalization1(n_dim), Tanh(), Layer(n_dim, n_vocab),BatchNormalization1(n_vocab) ] The output values are properly distributed, with very less saturation and a constant standard deviation.\nGradient plot The gradient distribution at each layers would look like this when the pre activations are batch normalized. Gradient to data ratio plot This is what the ratio of gradient (calculated after backprop) to data plot looks like. x-axis represent iterations, y represent the exponents. Ideally, 1e-3 is suitable and that ratio should lie around that line. If the ratio is below that line it means, we need to step up our learning rate, and if it is higher than that line we need to lower our learning rate.\nThe gain that we add during kaiming init has direct correlation with this plot.\nwith torch.no_grad(): # last layer: make less confident layers[-1].gamma *= 0.1 #layers[-1].weight *= 0.1 # all other layers: apply gain for layer in layers[:-1]: if isinstance(layer, Linear): layer.weight *= 0.3 as you can see, when I make gain to 0.3 the ratio significantly varies, i.e ratio for later layers are around 1e-1.5, which mean we would have to lower our learning rate because of this gain change.\nSo the gain significantly affects our learning rate, but it doesn\u0026rsquo;t affect other plots that we plot above, because it\u0026rsquo;s controlled by batch normalization.\nSo we don\u0026rsquo;t get a free pass to assign these gains arbitrarily, because it affects our gradients (as seen from the ratio plot). If we don\u0026rsquo;t worry about these gains, we have to tune these learning rates properly (by increasing or decreasing the learning rate).\nThese data is analyzed throughout the training of NN\nNOTE to myself after any operation look out for how the output\u0026rsquo;s standard deviation changes, we should always maintain the std of 1\nfor instance while doing the dot production attention,\nQ @ K.T\nthe output\u0026rsquo;s std grows by sqrt of last embedding or head dimension, which is the reason why we scale it by the sqrt of that last embedding dimension.\nSimilarly, in skip connections too the addition of x back to the output introduces increase in std, we should scale that down too as i\u0026rsquo;ve mentioned here\n","permalink":"https://cohlem.github.io/sub-notes/diagnostic-tool-while-training-nn/","summary":"source: Building makemore Part 3: Activations \u0026amp; Gradients, BatchNorm\nThings to look out for while training NN Take a look at previous notes to understand this note better\nconsider we have this simple 6 layer NN\n# Linear Layer g = torch.Generator().manual_seed(2147483647) # for reproducibility class Layer: def __init__(self,fan_in, fan_out, bias=False): self.w = torch.randn((fan_in, fan_out),generator = g) / (fan_in)**(0.5) # applying kaiming init self.bias = bias if bias: self.b = torch.","title":"Diagnostic-tool-while-training-nn"},{"content":"As we saw in our previous note how important it is to have the pre-activation values to be roughly gaussian (0 mean, and unit std). We saw how we can initialize our weights that make our pre-activation roughly gaussian by using Kaiming init. But, how do we always maintain our pre-activations to be roughly gaussian?\nAnswer: BatchNormalization\nBenefits\nstable training preserves vanishing gradients BatchNormalization As the name suggests, batches are normalized (across batches), by normalizing across batches we preserve the gaussian property of our pre-activations.\nThis is our initial code\n# same optimization as last time max_steps = 200000 batch_size = 32 lossi = [] for i in range(max_steps): # minibatch construct ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y # forward pass emb = C[Xb] # embed the characters into vectors embcat = emb.view(emb.shape[0], -1) # concatenate the vectors # Linear layer hpreact = (embcat @ W1 + b1) # hidden layer pre-activation # Non-linearity h = torch.tanh(hpreact) # hidden layer logits = h @ W2 + b2 # output layer loss = F.cross_entropy(logits, Yb) # loss function # backward pass for p in parameters: p.grad = None hpreact.retain_grad() logits.retain_grad() loss.backward() # update lr = 0.1 if i \u0026lt; 100000 else 0.01 # step learning rate decay for p in parameters: p.data += -lr * p.grad # track stats if i % 10000 == 0: # print every once in a while print(f\u0026#39;{i:7d}/{max_steps:7d}: {loss.item():.4f}\u0026#39;) lossi.append(loss.log10().item()) We add batch normalization just before the all activation layers. Right now, we only have one linear layer so only one tanh activation is applied, in a big NN, we have to add those batch normalization before applying activations.\nApplying batch normalization is quite simple.\nhpreact = (hpreact - hpreact.mean(0, keepdim = True)) / hpreact.std(0, keepdim =True) but by doing this we are forcing it to lie is a particular space. To add a little bit of entropy (randomness) and let to model learn itself (by backpropagating) where it wants to go, we introduce scaling and shifting. The model will learn itself the direction it wants to go, by back propagating because these scaling and shifting are differentiable.\n# BatchNorm parameters bngain = torch.ones((1, n_hidden)) bnbias = torch.zeros((1, n_hidden)) hpreact = bngain*(hpreact - hpreact.mean(0, keepdim = True)) / hpreact.std(0, keepdim =True) + bnbias by introducing batchNormalization, we are making our pre-activation of one input depend on all the other input, this is because we are subtracting the mean from it and this mean is the depended on all the other inputs.\nNow that we have introduced BatchNormalization, inorder to do inference, we would need the mean and std of the whole dataset, which we can keep track of in running manner, and it is used while doing inference.\n# same optimization as last time max_steps = 200000 batch_size = 32 lossi = [] for i in range(max_steps): # minibatch construct ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y # forward pass emb = C[Xb] # embed the characters into vectors embcat = emb.view(emb.shape[0], -1) # concatenate the vectors # Linear layer hpreact = (embcat @ W1 + b1) # hidden layer pre-activation bnmeani = hpreact.mean(0, keepdim = True) bnstdi = hpreact.std(0, keepdim =True) hpreact = bngain*((hpreact - bnmeani) / bnstdi) + bnbias ## --------------------- keeping track of whole mean and std ------------ with torch.no_grad(): bnmean_running = 0.999*bnmean_running + 0.001*bnmeani bnstd_running = 0.999*bnstd_running + 0.001*bnstdi # ------------------------------------------------------------------------ # Non-linearity h = torch.tanh(hpreact) # hidden layer logits = h @ W2 + b2 # output layer loss = F.cross_entropy(logits, Yb) # loss function # backward pass for p in parameters: p.grad = None hpreact.retain_grad() logits.retain_grad() loss.backward() # update lr = 0.1 if i \u0026lt; 100000 else 0.01 # step learning rate decay for p in parameters: p.data += -lr * p.grad # track stats if i % 10000 == 0: # print every once in a while print(f\u0026#39;{i:7d}/{max_steps:7d}: {loss.item():.4f}\u0026#39;) lossi.append(loss.log10().item()) ","permalink":"https://cohlem.github.io/sub-notes/batchnormalization/","summary":"As we saw in our previous note how important it is to have the pre-activation values to be roughly gaussian (0 mean, and unit std). We saw how we can initialize our weights that make our pre-activation roughly gaussian by using Kaiming init. But, how do we always maintain our pre-activations to be roughly gaussian?\nAnswer: BatchNormalization\nBenefits\nstable training preserves vanishing gradients BatchNormalization As the name suggests, batches are normalized (across batches), by normalizing across batches we preserve the gaussian property of our pre-activations.","title":"BatchNormalization"},{"content":" ","permalink":"https://cohlem.github.io/sub-notes/maximum-likelihood-estimate-as-loss/","summary":" ","title":"Maximum likelihood estimate as loss function"},{"content":"Source: The spelled-out intro to neural networks and backpropagation: building micrograd\nBackpropagation on paper It implements backpropagation for two arithmetic operation (multiplication and addition) which are quite straightforward.\nImplementation is for this equation.\na = Value(2.0, label=\u0026#39;a\u0026#39;) b = Value(-3.0, label=\u0026#39;b\u0026#39;) c = Value(10.0, label=\u0026#39;c\u0026#39;) e = a*b; e.label = \u0026#39;e\u0026#39; d = e + c; d.label = \u0026#39;d\u0026#39; f = Value(-2.0, label=\u0026#39;f\u0026#39;) L = d * f; L.label = \u0026#39;L\u0026#39; L The most important thing to note here is the gradient accumulation step (shown at the bottom-left). If a node takes part two times building up to the final node. The gradient for that node is accumulated. For instance, in the figure node b takes part two times. First, it is involved in equation e = a * b, and another is e = b + m (not in the equation above).\nCode\nclass Value: def __init__(self,data, _children = (), _op = \u0026#39;\u0026#39;, label=None): self.data = data self.label = label self.grad = 0.0 self._prev = set(_children) self._op = _op self._backward = lambda: None def __repr__(self): return f\u0026#34;Value({self.label}, {self.data})\u0026#34; def __add__(self, other): result = Value(self.data + other.data, (self, other), \u0026#39;+\u0026#39;) def _backward(): self.grad = 1.0 * result.grad other.grad = 1.0 * result.grad result._backward = _backward return result def __sub__(self,other): result = Value(self.data - other.data) result._prev = [self, other] result._op = \u0026#39;-\u0026#39; return result def __mul__(self,other): result = Value(self.data * other.data, (self, other), \u0026#39;*\u0026#39;) def _backward(): self.grad = other.data * result.grad other.grad = self.data * result.grad result._backward = _backward return result def backward(self): topo = [] visited = set() self.grad = 1 def build_topo(v): if v not in visited: visited.add(v) for child in v._prev: build_topo(child) topo.append(v) build_topo(self) topo = list(reversed(topo)) print(\u0026#39;gg\u0026#39;, topo) for i in topo: print(i) i._backward() ","permalink":"https://cohlem.github.io/sub-notes/backpropagation-from-scratch/","summary":"Source: The spelled-out intro to neural networks and backpropagation: building micrograd\nBackpropagation on paper It implements backpropagation for two arithmetic operation (multiplication and addition) which are quite straightforward.\nImplementation is for this equation.\na = Value(2.0, label=\u0026#39;a\u0026#39;) b = Value(-3.0, label=\u0026#39;b\u0026#39;) c = Value(10.0, label=\u0026#39;c\u0026#39;) e = a*b; e.label = \u0026#39;e\u0026#39; d = e + c; d.label = \u0026#39;d\u0026#39; f = Value(-2.0, label=\u0026#39;f\u0026#39;) L = d * f; L.label = \u0026#39;L\u0026#39; L The most important thing to note here is the gradient accumulation step (shown at the bottom-left).","title":"Backpropagation from scratch"},{"content":" it penalizes the weights, and prioritizes uniformity in weights. How does it penalize the weights? Now when we do the backprop and gradient descent.\nThe gradient of loss w.r.t some weights become as we can see it penalizes the weight by reducing the weights\u0026rsquo;s value by some higher amount compared to the some minimial weight update when we only used loss function.\nSo overall, the model tries to balance the Loss (L) as well as keep the weights small. This balance prevents the model from relying excessively on any particular weight.\n","permalink":"https://cohlem.github.io/sub-notes/why-we-need-regularization/","summary":"it penalizes the weights, and prioritizes uniformity in weights. How does it penalize the weights? Now when we do the backprop and gradient descent.\nThe gradient of loss w.r.t some weights become as we can see it penalizes the weight by reducing the weights\u0026rsquo;s value by some higher amount compared to the some minimial weight update when we only used loss function.\nSo overall, the model tries to balance the Loss (L) as well as keep the weights small.","title":"Why we add regularization in loss function"},{"content":"BackStory This is a simple fun little project that I did almost a year ago, At that time I used to see a lot of CodeBullet\u0026rsquo;s videos and wanted to learn the gist behind these evolutionary algorithms, and I can\u0026rsquo;t get into my head if I don\u0026rsquo;t do it from scratch so I wanted to implement this project from scratch. At that time, I wanted to document the learning and the implementation process, I even thought of making a youtube video about this topic but could not complete it, at that time, I had made some video animations about this process but could not complete it because I started doing something else and when I can back it was already too much mess and could not complete the video animations, but I\u0026rsquo;ll include the video animations that I had made earlier.\nSnake using Neural Network and Genetic Algorithm This is a simple demonstration of how neural network can be used along with genetic algorithm to play snake game. Now when I say its a snake game, this is just basically learning how to optimize a function, just like what we do with gradient descent. But that\u0026rsquo;s the interesting part we\u0026rsquo;re not using Gradient Descent, no fancy optimizers, even the initial weights were initialized randomly, I now wish(at the time of writing this blog) that I had used some initialization techniques like Xavier initializations, it would have learned to play so much faster. But even the random initialization works. Let\u0026rsquo;s get to the point.\nWe are playing a snake game let\u0026rsquo;s imagine that has a brain, So the brain here are weights which we initialize using Feed-forward layer and then we use genetic algorithm to evolve that brain(weights) and use this brain(weights) to predict which move to take(up, down, left or right) at every step in the game. So the term Neural Network here only represents the feed-forward layer, we don\u0026rsquo;t use any of the backpropogation.\nThe whole code for this project can be found here: Github\nDemo Video Explanation Following are the explanations that I\u0026rsquo;ve implemented for this project.\nSnake\u0026rsquo;s Eye It is what snake sees at each step while it\u0026rsquo;s playing game. Before performing anything, it has to see and pass it to the brain inorder to process what it has seen. So let\u0026rsquo;s understand what it sees at each step by looking at the video below.\nOur snake sees in 8 different directions(d1 to d8) as shown in the figure above. And at each direction it sees 3 different things Wall, Food, Body(it\u0026rsquo;s own body)\nSnake\u0026rsquo;s Brain Now that we have our snake eyes let\u0026rsquo;s make the brain to process the things that it has seen. We use a simple two layer neural network. The first layer has 8 hidden layers and the second is output layer of 4 units. We only use the feed forward part of neural network and discard all the other backpropagation part. We will later use genetic algorithm to improve our weights.\nThe figure below contains two parts\nSnake\u0026rsquo;s eye(descibed above), and Snake\u0026rsquo;s brain Input to neural network Now that we now our snake sees a body, wall and food. Let\u0026rsquo;s assign values to these metrics.\nSince the neural networks only understand numbers we represent them with a metric, 1 for favorable conditions and 0 for unfavorable.\nMetric for body: One is given when no body is present in that direction and Zero is given when no blocks are present in between Head and Body.\n$body = number of blocks between head and body/(total number of blocks - 1 )$.\nMetric for Wall: One is given when the number of blocks between Head and Wall are maximum and Zero is given when no blocks are present in between Head and Wall.\n$Wall = number of blocks between head and wall/(total number of blocks - 1 )$.\nMetric for Food: It is good for snake to move in the direction of food, so One is given when no blocks are present in between Head and Food and Zero is given when no food is present in that direction.\n$Food = ( total number of blocks - number of blocks between head and wall - 1)/(total number of blocks - 1 )$.\nThe another input to our snake is the it\u0026rsquo;s directions. It should keep. track of where it currently is to make move in another direction.\nValue for Head Direction: This is a simple one-hot encoding for the direction.\nBrain of Snake The brain of our snake is the weights inside our hidden layers and output layers.\nOur first hidden layer will have 8 * 28 weights with 8 bias. Our second layer will have 4 * 8 weights with 4 bias.\nSo, the total weights + bias counts to 268 which is the actual brain of our snake. So our neural network uses that brain to make a prediction in 4 directions. [Up, Right, Down, Left]\nGenetic Algorithm Since we do not use the backpropagation of our neural network to improve weights we will use use Genetic algorithm to improve weights.\nFive phases are considered in a genetic algorithm. You should first read these steps below and then you can come back to the video below.\nInitial brain (Population) We randomly generate the brain of size (1,268). Remember this is the initial phase where we initialize our weigths (brain) for our snake which described above. Later, this shape (1,268) is flattened into the structure of our feed-forward neural network which is described above.\nnp.random.choice(np.arange(-1,1, step =0.001),size = (population_size,weight_size), replace= True)\nweight size is 268 and population_size is the total number of snakes we want to train. I\u0026rsquo;ve trained 500 snakes in each generations. Thing to remember here is, each snake will have different brain. The random function above generates a linear vector which is later converted to matrices of sizes (8,28) for weights, (8,1) for bias and (4,8) for weights and (4,1) for bias using vector_to_matrix() function. The first two matrices is for hidden layer whereas the other two are for output layer.\nFitness function Now that we have 500 different snakes in each generation. We have to differentiate the great Snake from weak. Snake\u0026rsquo;s fitness is based on its score and number of steps it taken to achieve that score, so we created a function by using Steps and Score as variables which will helps the Snake in getting maximum score in less steps. I\u0026rsquo;ve used the following fitness function that evaluates fitness of snakes relative to score and steps taken.\nIf two snakes have the same score then the snake that achieves the score in less number of steps is considered.\nSelection This process selects the best fit snakes from the entire population according to their fitness value described above. It is then used produce a new population of offspring which will be used in a next generation. I\u0026rsquo;ve selected top 50 snakes initially according to their fitness value. The selected snakes will be called parents.\nCrossover We take the parents selected from the selection process to produce a new set of offsprings. We take 50 parents and iterate over (population_size - parent_length) times and use uniform crossover method to produce new offspring from these parents. we later add our 50 parents to the population set. This process will preserve the best fit snakes, even if the crossover and mutation yield bad set of population. The unifrom crossover can be explained likewise.\nMutation This example explains mutating by flipping bits.\n1\t0\t1\t0\t0\t1 0\n↓\n1\t0\t1\t0\t1\t1\t0\nBut in our case we change the value of our snake\u0026rsquo;s brain. Among 268 brain cells we will change 13 of them randomly between -0.5 to 0.5.\nRunning We run the game through many generation(similar to epoch) to evolve our snake\u0026rsquo;s brain by applying the method explained above. We do it until the snake has learned to score desired number of points.\nI know this is shitty writing but still Thank You for reading till the end. ","permalink":"https://cohlem.github.io/sub-notes/using-genetic-algorithm-for-weights-optimization/","summary":"BackStory This is a simple fun little project that I did almost a year ago, At that time I used to see a lot of CodeBullet\u0026rsquo;s videos and wanted to learn the gist behind these evolutionary algorithms, and I can\u0026rsquo;t get into my head if I don\u0026rsquo;t do it from scratch so I wanted to implement this project from scratch. At that time, I wanted to document the learning and the implementation process, I even thought of making a youtube video about this topic but could not complete it, at that time, I had made some video animations about this process but could not complete it because I started doing something else and when I can back it was already too much mess and could not complete the video animations, but I\u0026rsquo;ll include the video animations that I had made earlier.","title":"Using Genetic Algorithm for Weights Optimization"},{"content":"# We always start with a dataset to train on. Let\u0026#39;s download the tiny shakespeare dataset !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt with open(\u0026#39;input.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: data = f.read() from torch import nn import torch vocab = sorted(list(set(data))) len(data) stoi = {s:i for i,s in enumerate(vocab)} itos = {i:s for s,i in stoi.items()} encode = lambda x: [stoi[i] for i in x] decode = lambda x: \u0026#39;\u0026#39;.join([itos[i] for i in x]) type(data) str Xtr = data[:int(0.9*len(data))] Xval = data[int(0.9*len(data)):] block_size = 8 batch_size = 32 def get_split(X): idx = torch.randint(0,len(X) - block_size, (batch_size,)) # we subtract block_size from total len of X, because w\u0026#39;ll be taking next characters starting from the idx to the total len of block_size Xb = torch.tensor([encode(X[i:i+block_size]) for i in idx]) # now our d should be 32,8 Yb = torch.tensor([encode(X[i+1:i+1+block_size]) for i in idx]) return Xb,Yb A simple bigram language model with only embedding parameters n_vocab = len(stoi) # emb_dim = 64 class BigramLM(nn.Module): def __init__(self): super().__init__() self.emb_layer = nn.Embedding(n_vocab, n_vocab) def forward(self,x,targets=None): loss = None logits = self.emb_layer(x) # logits.view(emb_dim) if targets is not None: B,T,C = logits.shape logits = logits.view(B*T,C) targets = targets.view(B*T) loss = nn.functional.cross_entropy(logits,targets) return logits,loss def generate(self, idx, max_new_tokens): for i in range(max_new_tokens): logits, _ = self(idx) # idx is shape (B,T), logits is B,T,C probs = logits[:,-1,:] #probs is shape (B,C) probs = F.softmax(probs, dim = 1) idx_new = torch.multinomial(probs, num_samples=1) idx = torch.cat((idx,idx_new), dim = 1) return idx model = BigramLM() Mini-batch gradient descent for idx in range(10000): Xb,Yb = get_split(Xtr) logits,loss = model(Xb,Yb) for p in model.parameters(): p.grad = None # backprop loss.backward() #update the parameters lr = 0.1 # mini-batch gradient descent for p in model.parameters(): p.data += -lr*p.grad print(loss) tensor(2.8175, grad_fn=\u0026lt;NllLossBackward0\u0026gt;) Adam optimizer Manually m = {idx: torch.zeros_like(p) for idx,p in enumerate(model.parameters())} v = {idx: torch.zeros_like(p) for idx, p in enumerate(model.parameters())} b1,b2 = 0.9, 0.999 e = 1e-8 for idx in range(10000): Xb,Yb = get_split(Xtr) logits,loss = model(Xb,Yb) for p in model.parameters(): p.grad = None # backprop loss.backward() #update the parameters lr = 0.1 # Adam optimizer for i,p in enumerate(model.parameters()): m[i] = b1*m[i] + (1-b1)*(p.grad) v[i] = b2*v[i] + (1-b2)*(p.grad**2) m_corrected = m[i]/ (1-b1**(idx+1)) v_corrected = v[i]/ (1-b2**(idx+1)) p.data += (-lr*m_corrected)/ ((v_corrected + e)**0.5) print(loss) tensor(2.5091, grad_fn=\u0026lt;NllLossBackward0\u0026gt;) Adam from pytorch optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) for idx in range(10000): Xb,Yb = get_split(Xtr) logits,loss = model(Xb,Yb) optimizer.zero_grad(set_to_none=True) # backprop loss.backward() optimizer.step() print(loss) tensor(2.5577, grad_fn=\u0026lt;NllLossBackward0\u0026gt;) import torch.nn.functional as F print(decode(model.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=1000)[0].tolist())) zDe her C, pinG: bluiqQPZmaJhe bQZ;jarEW;t fOLtoul, tkYSvJu melmad my myoJDtLjz3ag cat haslZfbspJtour3vkO;NK BhQr CZQouObZf?L-QV\u0026amp;OJGW Ad a O t. ZMDJJ'ncARFxS thean, FFsqARICpmedUuvWShoureenure ckDn'qJDkhaha:xRbZQouIZ. ven ha woure ise aloEWLKme.QMCsMXXtheaGrilkEYjQSvehourVpCin wateesVy N.B'Z.Hse u'. -y pRClisto wher hiTTRL-!, hoRinout n emeaHmarorne ilRVA, IOpmZot\u0026amp;TAlDYLqppJ'it\u0026amp;Zitr s pligGWGJt gMn b: x, t al: I tIO!Ic, Qf tinE:RriFfsWfs?Bvhou ss -Ej: FxHPhe ingeuredve th$Drbe, t Ox'e sthem. Cs thitoFnWBu o se sTTQxRrahera: T! I l oFFDADP Briter: mouureIN-?CU3 tXceiW$g o ithen;bAql$, g txpr w. u SOq-Nawhbinded pr, ;gpr-'cWDno wRun wiead3lveeLZf pIShadOLven w?., tha hisedt NCs stVMG. o3SRPhie3thaYJWQIsthou ngrREJ-tpe;WMXpdeeatLreditFCXPer'Phe thOqL-, t, hJWqrscQ3toubeOxQ3y'ZHIvA3 Hry VIfandm\u0026amp;et cGWbeN-HNarelQver3p as t ir.-YK: ILHN?ly n ZLYCMh l aClle oungjAMHY,de hef amiRikn sc?c, KI QwP, ??e 3SSUl bSIYS, t, fO;SKLg m lktror ffiriMherrour D ll ORt m ar ast E-d mee ely hoing Attention experimentation Xb.shape torch.Size([32, 8]) v = torch.ones(5,4) v tensor([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) v = torch.softmax(v, dim =1) v tensor([[0.2500, 0.2500, 0.2500, 0.2500], [0.2500, 0.2500, 0.2500, 0.2500], [0.2500, 0.2500, 0.2500, 0.2500], [0.2500, 0.2500, 0.2500, 0.2500], [0.2500, 0.2500, 0.2500, 0.2500]]) lets say we have v vector with 5 characters and each character has 3 feature\nnow we need to also include the relationship of one character with it\u0026rsquo;s previous characters. How do we do it?\nfind the average of one with it\u0026rsquo;s previous characters. we need to make sure that a character doesn\u0026rsquo;t see it\u0026rsquo;s future characters and only see the previous characters.\nFor instance, char at ind 0 can only look at itself and char at 1 can only look at char at ind 0 and itself, and so on.\nv = torch.randn(5,3) v tensor([[-0.1586, -0.5878, -1.0289], [ 0.1123, 2.1602, 1.1508], [-0.7969, -2.1239, 1.4866], [ 1.0644, 1.1567, -0.5879], [-0.2015, -1.6920, -0.0972]]) 1 0 0 0 0 [-0.5134, -0.3769, -0.6881] 1 1 0 0 0 [ 0.1477, 0.1931, -0.4826] 1 1 1 0 0 X [ 1.0117, 0.4637, -0.9426] 1 1 1 1 0 [-0.0454, -0.7803, 0.0046] 1 1 1 1 1 [-0.3021, -0.0271, 1.1680] What do we get? sum across each columns with limited to its previous rows.\ni = torch.ones(v.shape[0], v.shape[0]) i tensor([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]]) i = torch.tril(i) i tensor([[1., 0., 0., 0., 0.], [1., 1., 0., 0., 0.], [1., 1., 1., 0., 0.], [1., 1., 1., 1., 0.], [1., 1., 1., 1., 1.]]) i = i/i.sum(1, keepdim= True) i tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.5000, 0.5000, 0.0000, 0.0000, 0.0000], [0.3333, 0.3333, 0.3333, 0.0000, 0.0000], [0.2500, 0.2500, 0.2500, 0.2500, 0.0000], [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]]) z = i @ v z tensor([[-0.1586, -0.5878, -1.0289], [-0.0232, 0.7862, 0.0610], [-0.2811, -0.1838, 0.5362], [ 0.0553, 0.1513, 0.2551], [ 0.0039, -0.2174, 0.1847]]) so this i is similar to what we have in transformer i.e attention weights (dot product of Q and K). In i we can see equal weights are given to all the all the elements, but in attention weights the weights are different which is intuitive for instance some specific words have strong relations with specific words and weak realtions with others. The weights represent how much of a focus should be given to specific words (character in our case)\nv tensor([[-0.1586, -0.5878, -1.0289], [ 0.1123, 2.1602, 1.1508], [-0.7969, -2.1239, 1.4866], [ 1.0644, 1.1567, -0.5879], [-0.2015, -1.6920, -0.0972]]) v.T tensor([[-0.1586, 0.1123, -0.7969, 1.0644, -0.2015], [-0.5878, 2.1602, -2.1239, 1.1567, -1.6920], [-1.0289, 1.1508, 1.4866, -0.5879, -0.0972]]) aw = v @ v.T aw tensor([[ 1.4293, -2.4717, -0.1546, -0.2439, 1.1266], [-2.4717, 6.0035, -2.9667, 1.9416, -3.7895], [-0.1546, -2.9667, 7.3556, -4.1787, 3.6097], [-0.2439, 1.9416, -4.1787, 2.8163, -2.1144], [ 1.1266, -3.7895, 3.6097, -2.1144, 2.9129]]) masking was be done using torch.tril but for normalization, we can\u0026rsquo;t simply call softmax on the above aw becuase exp(0) = some value.\nwe need to replace those zeros with some values that when exponetiated becomes 0. and that is -infinity\nblock_size = 8 tril = torch.tril(torch.ones(aw.shape[0], aw.shape[0])) tril tensor([[1., 0., 0., 0., 0.], [1., 1., 0., 0., 0.], [1., 1., 1., 0., 0.], [1., 1., 1., 1., 0.], [1., 1., 1., 1., 1.]]) aw tensor([[ 1.4293, -2.4717, -0.1546, -0.2439, 1.1266], [-2.4717, 6.0035, -2.9667, 1.9416, -3.7895], [-0.1546, -2.9667, 7.3556, -4.1787, 3.6097], [-0.2439, 1.9416, -4.1787, 2.8163, -2.1144], [ 1.1266, -3.7895, 3.6097, -2.1144, 2.9129]]) mask = tril[:block_size, :block_size] mask == 0 tensor([[False, True, True, True, True], [False, False, True, True, True], [False, False, False, True, True], [False, False, False, False, True], [False, False, False, False, False]]) aw = aw.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) torch.softmax(aw, dim= 1) tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00], [2.0852e-04, 9.9979e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00], [5.4715e-04, 3.2873e-05, 9.9942e-01, 0.0000e+00, 0.0000e+00], [3.2004e-02, 2.8466e-01, 6.2566e-04, 6.8271e-01, 0.0000e+00], [5.2653e-02, 3.8584e-04, 6.3070e-01, 2.0601e-03, 3.1420e-01]]) Head from torch import nn Scaling after Q.K\nWe suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by √1 . dk\nemb_dim = 128 block_size = 8 class Head(nn.Module): def __init__(self,h_dim): super().__init__() self.wq = nn.Linear(emb_dim, emb_dim, bias=False) self.wk = nn.Linear(emb_dim, emb_dim, bias=False) self.wv = nn.Linear(emb_dim, emb_dim, bias=False) self.register_buffer(\u0026#39;tril\u0026#39;, torch.tril(torch.ones(block_size, block_size))) def forward(self,x): B,T,C = x.shape Q,K,V = self.wq(x), self.wk(x), self.wv(x) # comment out if using multi head attention ### ------ multi-head ---------------- n_heads = emb_dim // h_dim Q = Q.view(B,T,n_heads, h_dim) K = K.view(B,T,n_heads, h_dim) V = V.view(B,T,n_heads, h_dim) Q = torch.transpose(Q, 1,2) # transposing (n_head, block_size) cause we\u0026#39;ll do matmul operation on block_size and h_dim K = torch.transpose(K, 1,2) # transposing (n_head, block_size) cause we\u0026#39;ll do matmul operation on block_size and h_dim V = torch.transpose(V, 1,2) # transposing (n_head, block_size) cause we\u0026#39;ll do matmul operation on block_size and h_dim ### ------ multi-head ---------------- aw = Q @ torch.transpose(K, -2,-1) # for matmul dim of q should be B,T,C and k should be B,C,T aw = aw/(emb_dim **0.5) mask = self.tril[:T,:T] == 0 # generate mask aw = aw.masked_fill_(mask, float(\u0026#39;-inf\u0026#39;)) # apply mask i.e fill true values with -inf aw = torch.softmax(aw,dim=-1) # -inf values are converted to 0 and then each row is normalized cv = aw @ V # context vector cv = torch.transpose(cv, 1,2) # bring it back to (B,T,n_heads, h_dim) cv = cv.contiguous().view(B,T,-1) return cv ans.shape torch.Size([32, 8, 128]) heads = inp.view(inp.shape[0],inp.shape[1], n_heads, h_dim).transpose(-2,-3) another = inp.view(inp.shape[0],inp.shape[1], n_heads, h_dim).transpose(-2,-3) heads.shape torch.Size([32, 4, 8, 32]) another.shape torch.Size([32, 4, 8, 32]) heads @ another.transpose(-2,-1) tensor([[[[ 32.1742, 3.5751, -3.5662, ..., -2.2413, -1.7000, 3.9770], [ 3.5751, 32.8265, -3.9191, ..., 1.1454, 0.2817, 1.4899], [ -3.5662, -3.9191, 49.9453, ..., 4.7897, 9.8889, -3.4950], ..., [ -2.2413, 1.1454, 4.7897, ..., 24.6098, 10.0202, 5.2172], [ -1.7000, 0.2817, 9.8889, ..., 10.0202, 28.6793, 1.9070], [ 3.9770, 1.4899, -3.4950, ..., 5.2172, 1.9070, 36.0254]], [[ 22.7656, -2.3078, 9.4733, ..., 6.2143, 4.7908, 4.0378], [ -2.3078, 28.7329, -6.9455, ..., 3.3861, 1.6332, -2.1739], [ 9.4733, -6.9455, 31.9591, ..., 5.3679, -0.4923, 1.1153], ..., [ 6.2143, 3.3861, 5.3679, ..., 37.6356, 5.0559, 0.7349], [ 4.7908, 1.6332, -0.4923, ..., 5.0559, 24.6002, 2.4460], [ 4.0378, -2.1739, 1.1153, ..., 0.7349, 2.4460, 18.2817]], [[ 31.6518, 5.8290, 6.3662, ..., 0.6282, -5.5420, 2.3825], [ 5.8290, 32.2843, -3.0601, ..., 2.2674, 2.9054, -2.9447], [ 6.3662, -3.0601, 30.9779, ..., 7.3485, -2.3613, -2.9850], ..., [ 0.6282, 2.2674, 7.3485, ..., 17.7225, 1.8768, -7.2604], [ -5.5420, 2.9054, -2.3613, ..., 1.8768, 28.1906, -4.7192], [ 2.3825, -2.9447, -2.9850, ..., -7.2604, -4.7192, 21.8654]], [[ 37.2365, -3.7616, -9.2002, ..., -7.2032, -4.0573, 10.2151], [ -3.7616, 21.1687, 0.4588, ..., -2.2633, 3.6384, 1.9324], [ -9.2002, 0.4588, 33.7734, ..., -2.4673, 1.9866, -5.8346], ..., [ -7.2032, -2.2633, -2.4673, ..., 31.7197, 0.5054, -5.8715], [ -4.0573, 3.6384, 1.9866, ..., 0.5054, 35.4172, -6.8851], [ 10.2151, 1.9324, -5.8346, ..., -5.8715, -6.8851, 44.4438]]], [[[ 45.6578, 3.4671, 9.5396, ..., 3.6376, 5.9042, 5.0866], [ 3.4671, 31.2397, -1.2889, ..., -3.2443, -1.7427, 6.0298], [ 9.5396, -1.2889, 18.8146, ..., 0.3659, 0.9351, -0.3261], ..., [ 3.6376, -3.2443, 0.3659, ..., 41.2272, 5.0229, 9.9165], [ 5.9042, -1.7427, 0.9351, ..., 5.0229, 47.7995, 4.8620], [ 5.0866, 6.0298, -0.3261, ..., 9.9165, 4.8620, 29.9559]], [[ 34.8406, -1.0729, -2.4909, ..., -15.5392, 2.4406, 3.9956], [ -1.0729, 50.9441, -1.6156, ..., -0.8506, 6.1251, -0.7462], [ -2.4909, -1.6156, 29.7794, ..., 17.3859, 5.3271, -0.5394], ..., [-15.5392, -0.8506, 17.3859, ..., 50.3442, 1.7252, -5.7926], [ 2.4406, 6.1251, 5.3271, ..., 1.7252, 30.1659, 7.9424], [ 3.9956, -0.7462, -0.5394, ..., -5.7926, 7.9424, 24.4478]], [[ 32.2363, 4.5509, 0.6994, ..., 3.8885, 3.2419, -3.5590], [ 4.5509, 44.1102, 0.2451, ..., 8.3753, 9.8859, 10.8134], [ 0.6994, 0.2451, 18.9293, ..., 3.8037, -4.0057, -0.4459], ..., [ 3.8885, 8.3753, 3.8037, ..., 30.4812, 9.0369, -0.4821], [ 3.2419, 9.8859, -4.0057, ..., 9.0369, 32.8905, 6.6835], [ -3.5590, 10.8134, -0.4459, ..., -0.4821, 6.6835, 25.8820]], [[ 34.1055, -2.9306, 0.6626, ..., 11.1533, -4.0041, 9.7521], [ -2.9306, 27.5158, -3.4747, ..., 0.1160, 11.5869, -6.7454], [ 0.6626, -3.4747, 28.3060, ..., -3.1335, 0.3503, 5.5252], ..., [ 11.1533, 0.1160, -3.1335, ..., 33.5075, -4.5291, 2.0836], [ -4.0041, 11.5869, 0.3503, ..., -4.5291, 32.7881, 2.4404], [ 9.7521, -6.7454, 5.5252, ..., 2.0836, 2.4404, 40.9715]]], [[[ 32.0416, 0.0788, 6.5413, ..., -6.5551, -3.2799, -7.8908], [ 0.0788, 31.7799, 2.8320, ..., 0.6807, 0.5974, -5.8561], [ 6.5413, 2.8320, 36.3656, ..., 5.4645, 1.2012, -7.6004], ..., [ -6.5551, 0.6807, 5.4645, ..., 30.7091, -5.8358, -2.6483], [ -3.2799, 0.5974, 1.2012, ..., -5.8358, 42.6208, 3.1208], [ -7.8908, -5.8561, -7.6004, ..., -2.6483, 3.1208, 38.9232]], [[ 37.5842, 4.2069, 6.1104, ..., -5.6760, 3.6003, 2.9112], [ 4.2069, 25.2379, 12.2942, ..., -5.3360, -9.7890, -9.9670], [ 6.1104, 12.2942, 41.9160, ..., -6.0786, 1.5327, -12.3278], ..., [ -5.6760, -5.3360, -6.0786, ..., 35.6687, -3.0921, 2.0084], [ 3.6003, -9.7890, 1.5327, ..., -3.0921, 39.4154, 8.4038], [ 2.9112, -9.9670, -12.3278, ..., 2.0084, 8.4038, 36.2056]], [[ 46.8421, -0.4574, 1.4663, ..., -11.0569, 4.3132, 2.6288], [ -0.4574, 29.3338, 2.6641, ..., -4.7041, -6.4938, -0.8643], [ 1.4663, 2.6641, 32.9588, ..., -4.2076, -7.0425, -1.0215], ..., [-11.0569, -4.7041, -4.2076, ..., 20.5085, 3.6718, 6.4799], [ 4.3132, -6.4938, -7.0425, ..., 3.6718, 29.9442, 4.7719], [ 2.6288, -0.8643, -1.0215, ..., 6.4799, 4.7719, 39.3404]], [[ 30.1868, -18.2841, -3.8556, ..., 0.1748, -11.6281, 7.9357], [-18.2841, 33.2663, -0.5568, ..., 4.7933, 7.4713, -1.5922], [ -3.8556, -0.5568, 33.3571, ..., -5.5591, -1.8302, -2.3288], ..., [ 0.1748, 4.7933, -5.5591, ..., 42.2914, -3.1656, -0.0794], [-11.6281, 7.4713, -1.8302, ..., -3.1656, 37.9287, -2.7775], [ 7.9357, -1.5922, -2.3288, ..., -0.0794, -2.7775, 20.1431]]], ..., [[[ 27.1805, 1.7525, 1.2874, ..., -0.7088, -5.4087, -2.1454], [ 1.7525, 37.2680, -7.7314, ..., -5.4847, -0.3849, 10.4835], [ 1.2874, -7.7314, 35.9060, ..., 5.6001, 3.8431, -0.8432], ..., [ -0.7088, -5.4847, 5.6001, ..., 21.4171, 4.0599, 1.9034], [ -5.4087, -0.3849, 3.8431, ..., 4.0599, 38.6673, -15.6571], [ -2.1454, 10.4835, -0.8432, ..., 1.9034, -15.6571, 46.7569]], [[ 22.2746, -1.5875, 2.0462, ..., -4.3275, 1.6363, 3.8162], [ -1.5875, 24.4395, -6.1820, ..., 6.4587, -4.6774, 0.1828], [ 2.0462, -6.1820, 30.9477, ..., -0.0719, 5.0252, 2.4537], ..., [ -4.3275, 6.4587, -0.0719, ..., 28.9538, -1.4443, -3.5094], [ 1.6363, -4.6774, 5.0252, ..., -1.4443, 26.9600, 4.2082], [ 3.8162, 0.1828, 2.4537, ..., -3.5094, 4.2082, 20.0763]], [[ 40.1019, 1.1669, -7.5864, ..., 4.5362, 4.3235, -0.7608], [ 1.1669, 38.2663, 2.0250, ..., 3.9493, -2.9794, 3.8610], [ -7.5864, 2.0250, 26.2510, ..., 4.9898, -2.2455, 4.8609], ..., [ 4.5362, 3.9493, 4.9898, ..., 25.6067, 0.0972, 1.2879], [ 4.3235, -2.9794, -2.2455, ..., 0.0972, 15.2263, 0.2495], [ -0.7608, 3.8610, 4.8609, ..., 1.2879, 0.2495, 24.2671]], [[ 43.5244, -6.5812, 6.9048, ..., -16.0361, 4.4655, -10.7278], [ -6.5812, 30.5437, -2.1458, ..., 6.1033, 5.0552, -4.5733], [ 6.9048, -2.1458, 29.7066, ..., -8.8169, -0.4333, -11.8848], ..., [-16.0361, 6.1033, -8.8169, ..., 41.8999, 10.9827, 11.8192], [ 4.4655, 5.0552, -0.4333, ..., 10.9827, 35.2686, 6.1599], [-10.7278, -4.5733, -11.8848, ..., 11.8192, 6.1599, 33.8409]]], [[[ 32.0467, 7.0052, -3.2416, ..., 8.1532, -0.2888, -6.5871], [ 7.0052, 25.7955, -8.1634, ..., 7.4437, -1.6669, -2.2903], [ -3.2416, -8.1634, 23.1504, ..., -9.1981, -1.7050, 3.3145], ..., [ 8.1532, 7.4437, -9.1981, ..., 40.4675, 1.8017, 2.0262], [ -0.2888, -1.6669, -1.7050, ..., 1.8017, 23.5782, -6.3969], [ -6.5871, -2.2903, 3.3145, ..., 2.0262, -6.3969, 28.7649]], [[ 21.0464, 0.6068, -1.7797, ..., 2.1086, -2.0139, -3.8878], [ 0.6068, 27.2985, 3.3299, ..., -4.8720, 0.4252, -3.0846], [ -1.7797, 3.3299, 30.7588, ..., 3.5386, -0.9481, -9.6544], ..., [ 2.1086, -4.8720, 3.5386, ..., 35.5470, 6.3743, 0.3064], [ -2.0139, 0.4252, -0.9481, ..., 6.3743, 41.8936, 14.2506], [ -3.8878, -3.0846, -9.6544, ..., 0.3064, 14.2506, 40.4118]], [[ 26.6901, -1.6078, -0.9821, ..., -0.6358, -7.5112, 1.0814], [ -1.6078, 33.0559, 6.3802, ..., 4.5554, 5.6757, -3.5736], [ -0.9821, 6.3802, 22.2404, ..., -3.5796, 4.5527, -4.9432], ..., [ -0.6358, 4.5554, -3.5796, ..., 20.0463, 4.5129, 1.3352], [ -7.5112, 5.6757, 4.5527, ..., 4.5129, 30.3458, 2.0281], [ 1.0814, -3.5736, -4.9432, ..., 1.3352, 2.0281, 26.2497]], [[ 34.7860, 1.1120, 1.4674, ..., 2.8627, -3.8121, -4.6312], [ 1.1120, 27.1577, -8.0268, ..., -4.4698, 1.6233, -9.6746], [ 1.4674, -8.0268, 37.0896, ..., 6.1201, 2.8779, 4.9796], ..., [ 2.8627, -4.4698, 6.1201, ..., 17.8634, 3.2803, 0.4458], [ -3.8121, 1.6233, 2.8779, ..., 3.2803, 20.7591, -2.5509], [ -4.6312, -9.6746, 4.9796, ..., 0.4458, -2.5509, 23.5079]]], [[[ 28.4154, 2.7645, -10.1010, ..., 0.0741, -13.8531, 1.3159], [ 2.7645, 21.8306, -6.2272, ..., -2.9116, -3.2048, -3.4533], [-10.1010, -6.2272, 31.9452, ..., -1.5767, 10.7117, 6.3105], ..., [ 0.0741, -2.9116, -1.5767, ..., 25.0870, 2.7461, 1.8960], [-13.8531, -3.2048, 10.7117, ..., 2.7461, 26.7191, -0.1613], [ 1.3159, -3.4533, 6.3105, ..., 1.8960, -0.1613, 33.3621]], [[ 25.6422, -3.1636, -2.0446, ..., 7.4708, 2.9027, 1.4148], [ -3.1636, 20.2049, 0.1776, ..., -4.4680, -3.0735, 2.2424], [ -2.0446, 0.1776, 26.3722, ..., -13.9194, -3.2235, -1.9986], ..., [ 7.4708, -4.4680, -13.9194, ..., 32.6086, -2.0325, -4.2793], [ 2.9027, -3.0735, -3.2235, ..., -2.0325, 26.0867, 4.6897], [ 1.4148, 2.2424, -1.9986, ..., -4.2793, 4.6897, 22.4440]], [[ 42.5938, -7.6218, -9.7269, ..., -2.8314, -10.9695, 7.8923], [ -7.6218, 28.2443, 0.2591, ..., -5.1364, 4.3846, -3.9063], [ -9.7269, 0.2591, 32.0604, ..., 0.5306, 10.6320, -1.2622], ..., [ -2.8314, -5.1364, 0.5306, ..., 35.8231, -1.7779, -6.9608], [-10.9695, 4.3846, 10.6320, ..., -1.7779, 28.1199, 3.4098], [ 7.8923, -3.9063, -1.2622, ..., -6.9608, 3.4098, 34.7994]], [[ 45.7010, -4.8081, 6.9660, ..., 12.1433, -0.8634, 8.6209], [ -4.8081, 33.8269, -2.8159, ..., 0.0864, 5.1836, -3.6970], [ 6.9660, -2.8159, 48.0178, ..., 4.8133, -4.5906, 8.6223], ..., [ 12.1433, 0.0864, 4.8133, ..., 34.4357, -9.2983, -4.1290], [ -0.8634, 5.1836, -4.5906, ..., -9.2983, 25.1852, -10.2649], [ 8.6209, -3.6970, 8.6223, ..., -4.1290, -10.2649, 39.4607]]]]) Combining our BigramLM with our heads n_vocab = len(stoi) emb_dim = 128 class BigramLM(nn.Module): def __init__(self, h_dim): super().__init__() self.emb_layer = nn.Embedding(n_vocab, emb_dim) self.mha = Head(h_dim) self.proj = nn.Linear(emb_dim, n_vocab, bias = False) def forward(self,x,targets=None): loss = None x_embed = self.emb_layer(x) # print(\u0026#39;embed\u0026#39;, x_embed) x_attn = self.mha(x_embed) # print(\u0026#39;attn\u0026#39;, x_attn) logits = self.proj(x_attn) # print(\u0026#39;logits\u0026#39;, logits) # logits.view(emb_dim) if targets is not None: B,T,C = logits.shape logits = logits.view(B*T,C) targets = targets.view(B*T) loss = nn.functional.cross_entropy(logits,targets) return logits,loss def generate(self, idx, max_new_tokens): for i in range(max_new_tokens): logits, _ = self(idx[:,-block_size]) # idx is shape (B,T), logits is B,T,C probs = logits[:,-1,:] #probs is shape (B,C) probs = F.softmax(probs, dim = 1) idx_new = torch.multinomial(probs, num_samples=1) idx = torch.cat((idx,idx_new), dim = 1) return idx model = BigramLM(32) optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) for idx in range(10000): Xb,Yb = get_split(Xtr) logits,loss = model(Xb,Yb) optimizer.zero_grad(set_to_none=True) # backprop loss.backward() optimizer.step() print(loss) tensor(2.2740, grad_fn=\u0026lt;NllLossBackward0\u0026gt;) print(decode(model.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=1000)[0].tolist())) --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) Cell In[466], line 1 ----\u0026gt; 1 print(decode(model.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=1000)[0].tolist())) Cell In[464], line 33, in BigramLM.generate(self, idx, max_new_tokens) 31 def generate(self, idx, max_new_tokens): 32 for i in range(max_new_tokens): ---\u0026gt; 33 logits, _ = self(idx) # idx is shape (B,T), logits is B,T,C 34 probs = logits[:,-1,:] #probs is shape (B,C) 35 probs = F.softmax(probs, dim = 1) File ~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs) 1734 return self._compiled_call_impl(*args, **kwargs) # type: ignore[misc] 1735 else: -\u0026gt; 1736 return self._call_impl(*args, **kwargs) File ~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs) 1742 # If we don't have any hooks, we want to skip the rest of the logic in 1743 # this function, and just call forward. 1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks 1745 or _global_backward_pre_hooks or _global_backward_hooks 1746 or _global_forward_hooks or _global_forward_pre_hooks): -\u0026gt; 1747 return forward_call(*args, **kwargs) 1749 result = None 1750 called_always_called_hooks = set() Cell In[464], line 16, in BigramLM.forward(self, x, targets) 13 x_embed = self.emb_layer(x) 14 # print('embed', x_embed) ---\u0026gt; 16 x_attn = self.mha(x_embed) 17 # print('attn', x_attn) 19 logits = self.proj(x_attn) File ~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs) 1734 return self._compiled_call_impl(*args, **kwargs) # type: ignore[misc] 1735 else: -\u0026gt; 1736 return self._call_impl(*args, **kwargs) File ~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs) 1742 # If we don't have any hooks, we want to skip the rest of the logic in 1743 # this function, and just call forward. 1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks 1745 or _global_backward_pre_hooks or _global_backward_hooks 1746 or _global_forward_hooks or _global_forward_pre_hooks): -\u0026gt; 1747 return forward_call(*args, **kwargs) 1749 result = None 1750 called_always_called_hooks = set() Cell In[463], line 35, in Head.forward(self, x) 32 aw = aw/(emb_dim **0.5) 34 mask = self.tril[:T,:T] == 0 # generate mask ---\u0026gt; 35 aw = aw.masked_fill_(mask, float('-inf')) # apply mask i.e fill true values with -inf 38 aw = torch.softmax(aw,dim=-1) # -inf values are converted to 0 and then each row is normalized 40 cv = aw @ V # context vector RuntimeError: The size of tensor a (9) must match the size of tensor b (8) at non-singleton dimension 3 Combining the previous model with Feedforward network torch.relu(torch.tensor(0)) tensor(0) class FFN(nn.Module): def __init__(self): super().__init__() self.layer1 = nn.Linear(emb_dim, 4*emb_dim, bias= True) self.layer2 = nn.Linear(4*emb_dim, emb_dim, bias = True) def forward(self,x): x = self.layer1(x) x = torch.relu(x) x = self.layer2(x) x = torch.relu(x) return x n_vocab = len(stoi) emb_dim = 128 class BigramLM(nn.Module): def __init__(self, h_dim): super().__init__() self.emb_layer = nn.Embedding(n_vocab, emb_dim) self.mha = Head(h_dim) self.FFN = FFN() self.proj = nn.Linear(emb_dim, n_vocab, bias = False) def forward(self,x,targets=None): loss = None x_embed = self.emb_layer(x) # print(\u0026#39;embed\u0026#39;, x_embed) x_attn = self.mha(x_embed) # print(\u0026#39;attn\u0026#39;, x_attn) x_ffn = self.FFN(x_attn) logits = self.proj(x_ffn) # print(\u0026#39;logits\u0026#39;, logits) # logits.view(emb_dim) if targets is not None: B,T,C = logits.shape logits = logits.view(B*T,C) targets = targets.view(B*T) loss = nn.functional.cross_entropy(logits,targets) return logits,loss def generate(self, idx, max_new_tokens): for i in range(max_new_tokens): logits, _ = self(idx[:,-block_size]) # idx is shape (B,T), logits is B,T,C probs = logits[:,-1,:] #probs is shape (B,C) probs = F.softmax(probs, dim = 1) idx_new = torch.multinomial(probs, num_samples=1) idx = torch.cat((idx,idx_new), dim = 1) return idx model = BigramLM(32) optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) for idx in range(10000): Xb,Yb = get_split(Xtr) logits,loss = model(Xb,Yb) optimizer.zero_grad(set_to_none=True) # backprop loss.backward() optimizer.step() print(loss) tensor(1.9205, grad_fn=\u0026lt;NllLossBackward0\u0026gt;) Layernormalization class LayerNormalization(nn.Module): def __init__(self,emb_dim, eps= 1e-5, mom=0.1): super().__init__() self.bngain = nn.Parameter(torch.ones(emb_dim)) self.bnbias = nn.Parameter(torch.zeros(emb_dim)) self.out = None self.eps = eps def forward(self,x): meani = x.mean(-1, keepdim = True) vari = x.var(-1, keepdim = True) self.out = self.bngain *((x - meani)/ torch.sqrt(vari + self.eps)) + self.bnbias return self.out ln = LayerNormalization(emb_dim) len(list(ln.parameters())) 2 ans = ln(torch.randn(32,8,128)) ans[-1,-1,:].std(), ans[-1,-1,:].mean() (tensor(1.0000), tensor(0.)) combine previous model with layer normalization and skip connections + positional embedding class Block(nn.Module): def __init__(self,h_dim): super().__init__() self.mha = Head(h_dim) self.FFN = FFN() self.ln1 = LayerNormalization(emb_dim) self.ln2 = LayerNormalization(emb_dim) def forward(self,x): x = self.mha(self.ln1(x)) + x x = self.FFN(self.ln2(x)) + x return x block_size 8 n_vocab = len(stoi) emb_dim = 128 class BigramLM(nn.Module): def __init__(self, h_dim): super().__init__() self.emb_layer = nn.Embedding(n_vocab, emb_dim) self.pos_emb = nn.Embedding(block_size, emb_dim) self.ln = LayerNormalization(emb_dim) self.proj = nn.Linear(emb_dim, n_vocab, bias = False) ## NEW self.block = Block(h_dim) def forward(self,x,targets=None): loss = None x_embed = self.emb_layer(x) x_pos = self.pos_emb(torch.ones_like(x) * torch.arange(x.shape[1])) x_block = self.block(x_embed + x_pos) x_ln = self.ln(x_block) logits = self.proj(x_ln) # print(\u0026#39;logits\u0026#39;, logits) # logits.view(emb_dim) if targets is not None: B,T,C = logits.shape logits = logits.view(B*T,C) targets = targets.view(B*T) loss = nn.functional.cross_entropy(logits,targets) return logits,loss def generate(self, idx, max_new_tokens): for i in range(max_new_tokens): # print(\u0026#39;idx\u0026#39;, idx.shape) logits, _ = self(idx[:,-block_size:]) # idx is shape (B,T), logits is B,T,C # print(\u0026#39;logits\u0026#39;, logits.shape) probs = logits[:,-1,:] #probs is shape (B,C) probs = F.softmax(probs, dim = 1) idx_new = torch.multinomial(probs, num_samples=1) idx = torch.cat((idx,idx_new), dim = 1) return idx model = BigramLM(32) optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) for idx in range(10000): Xb,Yb = get_split(Xtr) logits,loss = model(Xb,Yb) optimizer.zero_grad(set_to_none=True) # backprop loss.backward() optimizer.step() print(loss) tensor(1.7881, grad_fn=\u0026lt;NllLossBackward0\u0026gt;) print(decode(model.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=10000)[0].tolist())) To the delies. BRUTUS: Prifting king head! And neved somes drelose, been of his cannot lord, our you know; Now! A provoses; But king, out in that them! Lord well the eath this acan that I beaunture will to a cher forsely, that lord, of Julietteruse. VOLIO: Nor most. Ret. The now, sir: our thing he whom Tybarn for your head Tis many amply cutdendereizen: bear, buldown'd to so world crain as room meet, what heave misprus im; That blowisdoms finour to. LADUKE VINCENTIO: If Juchamfect better's Mayour Tray have prite warre. I ploy: RICHARD: When not now: Shall Than whoming will of heave goant dart? be is from lies with we that to in her your ful us; Sheechy, my with the most beaute you, With that you way this ma dare showed,--it held: An him ne'er. DUMERCENTIO: At Too curse,--but in crown needy? RIVERCUTIO: An would have chie, For courable he uncoth! GLOUCESTER: Now you ear threw wost play, as muciefuied to kind of gods naskly Eaductione, and and grow thite I cawares, as a son,. Gentlemy lieger is rail. JULIET: If thire torm: I our How not the his: grace have pun in soues of this turnclelaime, that sain wor good, look! And gold ward I; wilcompelse letwen dition Wors; For when your a the cisee of as me, my gived. AUTOLYCUS: The him true me kilgood us. KING HENRY Duke: Abanifemself. POLIXENES: Kore his curagese way their vowlyss, And the county my most thy be grace, feful advice you his traimsself. DUKE VINGH: Unto do, Is hath may grate: him you are drowning Help Tyband to have is shall and rums be as to his true of thee, let then heads. My look suments Cown: he well bedumfor wife; If not thought rly at siguers: To with: Hear-profes wift our the ento knew! say, no him of have I Cluld banish wefore in babiend full plieve on hip neverd, as touccretor. FOLAURENCENTIO: Which them all melecton mydel pegame, traed gold bling to it in confall what, 'erd her nore in bid in these is to the feely praily Cruay no lesty pries, why drould wrut beauntonessy good been prince an to hate goddeity were besir, Get in besters Of gate by crue, wear sorrow: Cher you licks. NORTESBY: God, Feath the straugh faity, To greep and word, A her; I love tearly yourses and his his ever ERWICK been this murds mine end an you to panit their plaimself. For break her, so been mest before, If Herm is nothing againe a what we quading by the const again, leave you ware Eork, thee; we it? The was that have to dow! were womaniught sorraw our noughtly hath thunnituded, swouldie, I that smoth, with lossion Of to crownce On with your shion; For Loncetters drown, Auf: that, Laster, my charink, He is being that for thoughtiviles abace lost towarchemself All not a is curs: your lathere, and spawn why the fries, sire wiserved, sweek to and not a'es Balmaties up woman of no law no him that himself life. YORK: He paintames it. PARIS: Richar life, And prop. I dring of noble too live swey couse; And on, to to grave clows. SICINIUS: Warwick'd at an and man Rome stry notion beat, and hath and guee. Fith od the lrover, And too! what it hoarrate, birt. LADY CAPULET: And end. Seconce; Did of him a trity To dedue exts. LUCIO: Ay, gentlemany the sway. WARD IV: His fools the heart! AUTOLYCUSHXENENIUS: Let dream! ANTIO: The be to death: My fresir; The dotwy Freath such ecry seem of tune why couse thee: he city To breath The him. Fhretly that be suph show. FRIAR TCompt seeceitacking Here. NORTIUS: Nor Lord thy his are I lo: My theeisue Warwice headly. The entone: This pretebeardy; How his coung lovery thand inshries To my knal awe him! NORTENSIO: Now true, limes answelcous? ROMEO: Husin! You cosise, With us, And pare she swifter's to him, your king of the an old for you purmptrese gently Will Lieve your bread of with am this Yoint litted. She honours: Whereign, And to sattereith it you Snothird noble taabbegitle by blew, we live but than train. LUMNIE: NencAnger intake bries, dead wear her: And for of your the Will to grace shall not ilse would custoly caperfell nry: 't, Thou world tone bloody upon they Lord friecto his me the ily? I stand one, our again'd fice, What not. LADY CARIANA: Tas For have irituation my not her made wrrange a now, An I way Capiece? Pray, your reaking my Will dowrongifformenten all at boot there all'd ara were headagentlemany libeand wron, ror made handraw heat, Causurpaciouth grace a what, doth his unperfeecome blead accame, our sward on hath, safe. MENE: There they fal straitagen. BUCHII: The the mine with for whod not upon yet plighform of the her licessighte we Straighted, a hastomberate, daught, These not are all every lord not dean that your creeemember'st living That feath. Now My Rome. For to no my has but Rive me no day, as Wrom the intriber gipful handGause previngbrood's we pleave thy king for heave we house have you nut and chould Dearis; and I ward in off this of unands exceedied life shing morcauself, As to they welcome I will for Hunbry of thy be slay? howbread, and grace a befied banise deaute is befrom cation. CLAUDIO: When, I am, I wear'd peoply tongue to Pet: Bounmen a before gook known: By we office, We to his clam: the king liff. HY: What? If You knew loved muctand you maded him, perouse thou beare not cretory we the not unjusty he reterefore is do well to her die down left me our much: Had b'd thy as ban abour will Anclose fair by to may of Qaint you had and this deed Eveth triatureing our'd and But show that hand, And inster? GLOUCESTENSIO: A whom come than even, You are thy brothery youty, I a gentle should's was way turoung most, ast all ie not in hey To Lordad no soul, servantale thy be narly; And good agaffect, colse I may is king? If crown'd ear Volsun ender'd life An hearts pedine thou blang! I mism pare saidf all in face? Was and melcower audio. Hand infalse? First we; in Serving be to her: No,tis stommone: when way, BusHe Where cure in eltal much will my prever alm in to-myself spear ing beg and in serving cong in all wighter me you deful of thy biends Boing it wave wronBy are none: Then this way holy. GLOUCESTER: Hare! Now'd his nonour she hous sham of a die, up make would hought heir ha,--O lose to refitsed for prom a grave To cannot lips. But bettence, To most put dead; thou wrong is unto truld Lance to it tabmand with to where jut in if have I couse, As to pue deson. CLIFFORTEGBROKE: If Lord do. The gads I was peacefore womber must your councie again. Wepty you vike ear swence a name too lady as honour great him as nread? GLOUCESTER: Pauly, Kneet, wither: Angelo; Wher your shion. KINGHA: The duspire me an ay, if turetory wron abound, And to casoland hall confaithe body their chood grouble! Our Nay, we devile him; And have of our raim. ESCALUS: Nor yow To perd Frand mee lords, tell Which his can fair a mand kead Ke too live the lice; I her sceecond be duke? To pretbeen hit love, No Let him ly know: Histend 'twas I love: my down the mertia; trunt, him breard goold to Wack the sajes. Comfor horse: I henose amany of they waunt, And I can in they out weep of I'll tirrought, I Plonds; Whinher,' bate fight ent is a scept to my talk see long As I know word; Of you, by why are pail the but man Than on us mall betry Rage re to out surp, my Duke I queen the battly: Iven thy banishould life, gentlize: In gethat my will to not Benouuts and Laint were wife well gume head at eart, and Ge: hy came none, if I landlay this part my bodanced: we what now. BRAMENE: You holy very; life No, uncady have us, Aways threak out, Nor grains Dears, hearewe of herd! A like enour us? LADY CARIANA: The us, boot thy hong Warwice asquested me and Messervy. Frothungent Behalt. HASAR: And for thank God Kenews, trofes, incies, shall so tell: know. GLOUCESTER: Thout, am name swear his, Nay, somes, if aughter, whence, And prence to see joing one,' delish a eximieT; For But hand. FRIAR TGAUNT: For labeing presence. JULIET: The grow. BISHARD GAUNT: You they do his perfee I shall prater cled With in at my leads Whamontway, up mase gh; untard to begs. QUEEN RICHAM: I will book his how Parisors; For I bleave I prepeding Romeo her down. Seconfore, as breams' dear of you man's will peecemes, Auffacenty all consmialt accould, what hall will what me how, I'll better chade and there would sure; Very noble He I week to no slay all Richarison? Seces. ROMEO: We Pault Tune all bear of thes hast they lish you with thy forturisign, And his war abin, Servan: Anborn pile as to Puscause? BUCHOTCESTIO: It such fercy, And tune your bold, The for they homined Gentleman, bein the boy! Speadowar, and you And ty? LUCIO: No end? Por man Off your loftious, Thou good Cepray your the deady! Of OF YORK: My from me dreaty powere with to kinship wronger: Whom bears; be all Clainsman: What veinst. VOLIO: Romfore, And traught were flamo's life, And her Some let comperch, that straise? FLORIOLANUS: And wea, I been this feart her all perouse all goo-me! MARGARET: Untor younsed but conums death sleave as be, weetaitold Of amost And be noble not suffer: How at I'll huson or have her were trould were an with debook! As lorderst what do, I hace an what him! So of heaven erein him should not where tim: I be which should Leadies, to at a be tot the home, if you; her I for else the not the ruitest majesty sunce; For it. See do the entestan of in godly ow Had--owisdies, And shear posity, Here lendly. MENIUS: Affatter beggard die, Our breed, and Mon's persmand Goding brraation: And such'd aff in to cours, But life; and for the lance garewell did them to Madam this nable not holk to despet ailse earies, Would mreadfing Most, me true have Or Corion? she enou harm in. QUEEN: Ay? Ha's with woman's son. OVERDITA: Abaservanty not there him trains An I the city too is ghood leign canners, poof more that on advici him? their perful of gualf, acd to your bayou: And without, in of where'er writy. For'd by ha; and not mine. BURY: Gresty let's IUS: O leep-bawful be; alse our dead this disceet speak his the guilt to ANGELO: Tulaties sealn forge p as we can see there\u0026rsquo;s high quality output after the addition of positional embedding\nsample_emb = nn.Embedding(block_size, n_vocab) a = torch.randn(32,8) b = torch.ones_like(a)*torch.arange(8) b.shape torch.Size([32, 8]) sample_emb B,T each T\u0026rsquo;th dimension should have the numbers between 0, block_sizem\nLog of all losses initial using adamW 10K iter\n2.601\nAfter multi-head-attentnion 10k iter\n2.316\nAfter FFN 10k iter\n1.9205\nAfter LayerNormalization 1.9252\nAfter skip-connections 2.0718\nAfter positional embedding 1.7881\nPutting it all together # We always start with a dataset to train on. Let\u0026#39;s download the tiny shakespeare dataset !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt with open(\u0026#39;input.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: data = f.read() from torch import nn import torch vocab = sorted(list(set(data))) len(data) stoi = {s:i for i,s in enumerate(vocab)} itos = {i:s for s,i in stoi.items()} encode = lambda x: [stoi[i] for i in x] decode = lambda x: \u0026#39;\u0026#39;.join([itos[i] for i in x]) Xtr = data[:int(0.9*len(data))] Xval = data[int(0.9*len(data)):] device = \u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39; device 'cpu' batch_size = 32 def get_split(X): idx = torch.randint(0,len(X) - block_size, (batch_size,)) # we subtract block_size from total len of X, because w\u0026#39;ll be taking next characters starting from the idx to the total len of block_size Xb = torch.tensor([encode(X[i:i+block_size]) for i in idx]) # now our d should be 32,8 Yb = torch.tensor([encode(X[i+1:i+1+block_size]) for i in idx]) return Xb.to(device),Yb.to(device) eval_iter = 200 @torch.no_grad() def evaluate_loss(): out = dict() model.eval() for item in [\u0026#39;train\u0026#39;, \u0026#39;val\u0026#39;]: if item == \u0026#39;train\u0026#39;: losses = torch.zeros(eval_iter) for k in range(eval_iter): Xb,Yb = get_split(Xtr) _, loss = model(Xb,Yb) losses[k] = loss out[item] = losses.mean() if item == \u0026#39;val\u0026#39;: losses = torch.zeros(eval_iter) for k in range(eval_iter): Xb,Yb = get_split(Xval) _, loss = model(Xb,Yb) losses[k] = loss out[item] = losses.mean() model.train() return out emb_dim = 128 block_size = 8 class Head(nn.Module): def __init__(self,h_dim): super().__init__() self.wq = nn.Linear(emb_dim, emb_dim, bias=False) self.wk = nn.Linear(emb_dim, emb_dim, bias=False) self.wv = nn.Linear(emb_dim, emb_dim, bias=False) self.register_buffer(\u0026#39;tril\u0026#39;, torch.tril(torch.ones(block_size, block_size))) def forward(self,x): B,T,C = x.shape Q,K,V = self.wq(x), self.wk(x), self.wv(x) # comment out if using multi head attention ### ------ multi-head ---------------- n_heads = emb_dim // h_dim Q = Q.view(B,T,n_heads, h_dim) K = K.view(B,T,n_heads, h_dim) V = V.view(B,T,n_heads, h_dim) Q = torch.transpose(Q, 1,2) # transposing (n_head, block_size) cause we\u0026#39;ll do matmul operation on block_size and h_dim K = torch.transpose(K, 1,2) # transposing (n_head, block_size) cause we\u0026#39;ll do matmul operation on block_size and h_dim V = torch.transpose(V, 1,2) # transposing (n_head, block_size) cause we\u0026#39;ll do matmul operation on block_size and h_dim ### ------ multi-head ---------------- aw = Q @ torch.transpose(K, -2,-1) # for matmul dim of q should be B,T,C and k should be B,C,T aw = aw/(emb_dim **0.5) mask = self.tril[:T,:T] == 0 # generate mask aw = aw.masked_fill_(mask, float(\u0026#39;-inf\u0026#39;)) # apply mask i.e fill true values with -inf aw = torch.softmax(aw,dim=-1) # -inf values are converted to 0 and then each row is normalized cv = aw @ V # context vector cv = torch.transpose(cv, 1,2) # bring it back to (B,T,n_heads, h_dim) cv = cv.contiguous().view(B,T,-1) return cv class FFN(nn.Module): def __init__(self): super().__init__() self.layer1 = nn.Linear(emb_dim, 4*emb_dim, bias= True) self.layer2 = nn.Linear(4*emb_dim, emb_dim, bias = True) def forward(self,x): x = self.layer1(x) x = torch.relu(x) x = self.layer2(x) x = torch.relu(x) return x class LayerNormalization(nn.Module): def __init__(self,emb_dim, eps= 1e-5, mom=0.1): super().__init__() self.bngain = nn.Parameter(torch.ones(emb_dim)) self.bnbias = nn.Parameter(torch.zeros(emb_dim)) self.out = None self.eps = eps def forward(self,x): meani = x.mean(-1, keepdim = True) vari = x.var(-1, keepdim = True) self.out = self.bngain *((x - meani)/ torch.sqrt(vari + self.eps)) + self.bnbias return self.out class Block(nn.Module): def __init__(self,h_dim): super().__init__() self.mha = Head(h_dim) self.FFN = FFN() self.ln1 = LayerNormalization(emb_dim) self.ln2 = LayerNormalization(emb_dim) def forward(self,x): x = self.mha(self.ln1(x)) + x x = self.FFN(self.ln2(x)) + x return x n_vocab = len(stoi) emb_dim = 128 block_size = 16 h_dim = 32 n_blocks = 4 class BigramLM(nn.Module): def __init__(self, h_dim): super().__init__() self.emb_layer = nn.Embedding(n_vocab, emb_dim) self.pos_emb = nn.Embedding(block_size, emb_dim) self.ln = LayerNormalization(emb_dim) self.proj = nn.Linear(emb_dim, n_vocab, bias = False) ## NEW self.blocks = nn.Sequential(*[Block(h_dim) for _ in range(4)]) def forward(self,x,targets=None): loss = None x_embed = self.emb_layer(x) x_pos = self.pos_emb(torch.ones_like(x) * torch.arange(x.shape[1])) x_block = self.blocks(x_embed + x_pos) x_ln = self.ln(x_block) logits = self.proj(x_ln) # print(\u0026#39;logits\u0026#39;, logits) # logits.view(emb_dim) if targets is not None: B,T,C = logits.shape logits = logits.view(B*T,C) targets = targets.view(B*T) loss = nn.functional.cross_entropy(logits,targets) return logits,loss def generate(self, idx, max_new_tokens): for i in range(max_new_tokens): # print(\u0026#39;idx\u0026#39;, idx.shape) logits, _ = self(idx[:,-block_size:]) # idx is shape (B,T), logits is B,T,C # print(\u0026#39;logits\u0026#39;, logits.shape) probs = logits[:,-1,:] #probs is shape (B,C) probs = F.softmax(probs, dim = 1) idx_new = torch.multinomial(probs, num_samples=1) idx = torch.cat((idx,idx_new), dim = 1) return idx model = BigramLM(h_dim).to(device) optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) all_losses = {\u0026#39;train\u0026#39; : [], \u0026#39;val\u0026#39; : []} total_iter = 10000 for ind in range(total_iter): Xb,Yb = get_split(Xtr) logits,loss = model(Xb,Yb) if ind % eval_iter == 0 or ind == total_iter - 1: with torch.no_grad(): eloss = evaluate_loss() all_losses[\u0026#39;train\u0026#39;].append(eloss[\u0026#39;train\u0026#39;].item()) all_losses[\u0026#39;val\u0026#39;].append(eloss[\u0026#39;val\u0026#39;].item()) print(f\u0026#39; step {ind}: losses: {eloss}\u0026#39;) optimizer.zero_grad(set_to_none=True) # backprop loss.backward() optimizer.step() print(loss) step 0: losses: {'train': tensor(4.0128), 'val': tensor(4.0197)} step 200: losses: {'train': tensor(2.2937), 'val': tensor(2.3103)} step 400: losses: {'train': tensor(2.1003), 'val': tensor(2.1448)} step 600: losses: {'train': tensor(1.9851), 'val': tensor(2.0530)} step 800: losses: {'train': tensor(1.9177), 'val': tensor(2.0016)} step 1000: losses: {'train': tensor(1.8552), 'val': tensor(1.9664)} step 1200: losses: {'train': tensor(1.8357), 'val': tensor(1.9464)} step 1400: losses: {'train': tensor(1.7834), 'val': tensor(1.9312)} step 1600: losses: {'train': tensor(1.7782), 'val': tensor(1.8990)} step 1800: losses: {'train': tensor(1.7337), 'val': tensor(1.8853)} step 2000: losses: {'train': tensor(1.7283), 'val': tensor(1.8673)} step 2200: losses: {'train': tensor(1.7080), 'val': tensor(1.8764)} step 2400: losses: {'train': tensor(1.6945), 'val': tensor(1.8577)} step 2600: losses: {'train': tensor(1.6876), 'val': tensor(1.8383)} step 2800: losses: {'train': tensor(1.6763), 'val': tensor(1.8354)} step 3000: losses: {'train': tensor(1.6648), 'val': tensor(1.8294)} step 3200: losses: {'train': tensor(1.6585), 'val': tensor(1.8239)} step 3400: losses: {'train': tensor(1.6514), 'val': tensor(1.8005)} step 3600: losses: {'train': tensor(1.6405), 'val': tensor(1.8000)} step 3800: losses: {'train': tensor(1.6208), 'val': tensor(1.7985)} step 4000: losses: {'train': tensor(1.6374), 'val': tensor(1.7847)} step 4200: losses: {'train': tensor(1.6226), 'val': tensor(1.7880)} step 4400: losses: {'train': tensor(1.6164), 'val': tensor(1.7778)} step 4600: losses: {'train': tensor(1.6057), 'val': tensor(1.7920)} step 4800: losses: {'train': tensor(1.6025), 'val': tensor(1.7803)} step 5000: losses: {'train': tensor(1.6009), 'val': tensor(1.7762)} step 5200: losses: {'train': tensor(1.5881), 'val': tensor(1.7756)} step 5400: losses: {'train': tensor(1.5792), 'val': tensor(1.7609)} step 5600: losses: {'train': tensor(1.5851), 'val': tensor(1.7565)} step 5800: losses: {'train': tensor(1.5715), 'val': tensor(1.7503)} step 6000: losses: {'train': tensor(1.5762), 'val': tensor(1.7453)} step 6200: losses: {'train': tensor(1.5695), 'val': tensor(1.7463)} step 6400: losses: {'train': tensor(1.5738), 'val': tensor(1.7371)} step 6600: losses: {'train': tensor(1.5639), 'val': tensor(1.7313)} step 6800: losses: {'train': tensor(1.5496), 'val': tensor(1.7253)} step 7000: losses: {'train': tensor(1.5621), 'val': tensor(1.7312)} step 7200: losses: {'train': tensor(1.5579), 'val': tensor(1.7246)} step 7400: losses: {'train': tensor(1.5555), 'val': tensor(1.7399)} step 7600: losses: {'train': tensor(1.5465), 'val': tensor(1.7323)} step 7800: losses: {'train': tensor(1.5550), 'val': tensor(1.7437)} step 8000: losses: {'train': tensor(1.5515), 'val': tensor(1.7444)} step 8200: losses: {'train': tensor(1.5386), 'val': tensor(1.7312)} step 8400: losses: {'train': tensor(1.5342), 'val': tensor(1.7294)} step 8600: losses: {'train': tensor(1.5440), 'val': tensor(1.7240)} step 8800: losses: {'train': tensor(1.5454), 'val': tensor(1.7259)} step 9000: losses: {'train': tensor(1.5388), 'val': tensor(1.7214)} step 9200: losses: {'train': tensor(1.5282), 'val': tensor(1.7161)} step 9400: losses: {'train': tensor(1.5255), 'val': tensor(1.7225)} step 9600: losses: {'train': tensor(1.5300), 'val': tensor(1.7152)} step 9800: losses: {'train': tensor(1.5304), 'val': tensor(1.7066)} step 9999: losses: {'train': tensor(1.5280), 'val': tensor(1.6964)} tensor(1.6668, grad_fn=\u0026lt;NllLossBackward0\u0026gt;) print(decode(model.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=10000)[0].tolist())) Is you rathed in love as not. HENRY HENRY VI QUEEN ELIZABETH: By Till inevice the told, And one gall be so chaperted A friender'd misi' the time earth and put to land, he counting to pray. 'Tis man's ma attemp From your seatter's this verienge: His mothers her hate, till do The lorkwin the forwards what his quarrel piece: The father's, comfort to be love that more speak one on courts living worthy, and in this of rumpts and inque against distroting. ROMEO: As Risichion: my power by my right, And me then we do the duke. POLIXENES: And surelvey did life once abelitter but of houseing cause As that he to tell plobb'd from the heart, And Did bearthy cousin here? ISABELLA: Sweet so bist gaze together, I would seem on Aumerle. Let think iI'd a goodness of frail from our revonaminate him curse maid, in this suit, for all askites As that a death. I'll never warshed our truth; O! offect late intercure Of seem upon hy gates. 'Text God unreisold Which mine holds no monfaciars! say, damneta leave. I'll make thou calledHed are the title than theirs Gill dame follower. I'll know the worl, Did fortune and paucil'ht thou must be do not a Captes: If resign the hand thy hands Regentle, now, that cry thou shalt as crown, it without Rutland in the shouts it of my langue of strivinger with sweets the fasterporate summer's pance, And as flowers as even he spremd Say bloody labour graves? will stabb'd Speak no keep to comfort, as a cure, his that flouded, And fear now our mother, hy gose Not in good herds it. Clown: When-changed up the queen! lost circutio more beseech. AUFIBALT: I'll not, I table. PAULINA: Not to your poor hours be a brother's most with his fallow death this? his is turn your batter His both sad, be shall die. MENENIUS: Let's this hihe hath conque time for hwoford. GLOUCESTER: Go treach of: But, thou king, which the cit, You wonder'st coasters, Tigue so her thinks with your blood of the duke, of the son-nonegly for war Both, Richard whose are we not hath replain, She's hat true throus must you being woivern, in, sir, but let him for, death. Long hath forth water Where not; as the atches as one, Lordon, our giirly. Nurse: Besides, innocence extremainted follow me of late Romeo, whither from his that draid Which death, on, death stir. GLOUCESTER: Duke of Norford NORFITZWARD IV: Come, do he hath been suffe prever, 'tis a randers for tLuckinfarencation? CAMILLO: It is not the sads and well! DORSET: What But so the forth bile blot die very come, And we livise all visit With by the unclusure both, poor I think His very nothinging some someetness! KING RICHARD II: It what lose them You wasque Whiles ye here for a foul tallige tempt: Reggarding words Thee bun the seizes and it with to hither solume done: good for Paris! in you if all be but I have. TUS: You obey, In Thank your sood lovy; and, and come, that we revenged agree done, to speak: pall. Clown: My mooning-hative thus Is cause, To find as I task. GREMINGS: It is, thy cornater, As years s'That I am suit pave by curginary Keptry thee, that true times to us dire But if that before the pite of his bares her rounded. Romeo, English! on' that for a very's harm, KING RICHARD II: Thou now to the enemies; But not peace, to see that we say rimorally and raumer with brave in none; And allower's master, sir, encle breathing lust, home Is for soul dreath; if why, and, think themself, yet we, at name sorry vengeance camition an upon his kin myself! Yon rathe. You have tried help no father scolds him, Thou having enemies, let me, my your praise And that beenefit evocliate for this majesty? My hateful voice To perish the Romeo will Will die a gaint in anothing whither than sudden the trouble, women die, not who, I speak, pardon thou hast never come Made your dam libund in, That lestrice ble Rome die For I am in a summanch? be good stood is. BEPLANUS: Doy, sir? Tell me unright, go youth promised to the valmouragy: but ble thou but he, sweet their pardon You have he mes the virds will'd here-flowise. CORIOLANUS: Office,' death hill mine of labour; Pastes to revious And, not put upon's bloodight. ABHOMENES: I'll Montague: you no, deathe all day prince, pickingham my torsest makes, Rome newd is down; Werping and go as here, There'll wist go what laid with me, Madam. LUCIO: O you not. For not to shame resolume that fair you. God But given to do a day ois graving 'twith me unto that you. First Murderer: And, my haste, my soul slick, committed, in set's shook which your struck and yea do not be your deave me: Look's honour of your brave be rieven-with you beg, not mere, as? We having to the holy that I spoke for the win joint of your honouration; and wouldst eiterity death she will thensuited! For it is his head, Coriolanus, and hath proclaccamaster means, lin for my daying together. FRIAR LAURENCE: Your God's haste, as realm When I bear build Shall your hated, When all so this trust poss were A blothy soul the curse maties are as hereather of my dayage, And moving, but in your house, Cenry with him thinker her tent.' lay, thou lives a putish the feaster-should hither his life; Not sir; he's suddenly Whose seo scoves be no butely home, came this airs, that founds in his for the helds came? Nurse: And this me sorrow, bid a hestress That naturacles my late, conscail his love it down in that should is by this, holy tride a gyour story protected a wretched but do seat able, Agoing Angelowninges, as thy to thou art effrem. That is Lay'st give himself, as now, Till it long but speak I thy voice, If they slew my trainhern; And whose statute' the neudest, most all go fault our flier me; And I drate again plame; If, him we could like, in the Veion's? Has the tell for this athing goodness! he buried Of hear the mean Pilause with's all. It was not man can Richard, And with but prock on your ringelo go. ROMEO: I bed; and, I it would but And sell the ear as Jaughter's horse book For these cieving Before conscience was. To petty are od vauquin his acting rourse Hight is as her? KIVUTUS: As your, fail shall we do cormby nothing: that overse fatable before The warried may slinhame, herAth thou rights king: Up he shall ame love herd whence That's hither oher to heat. Not our puities, Proclaimins but is moved to be Sisters o' For that serviced me honour? GLOUCESTER: This is command mates? I dward thousand 'tis to tthy blood Which is your quincule, And let me honourable wenchequired by thy till steel win 'tis my our awry contiruis live my love? Time eason! Give no for your And the grave me? your comfortly crows to such which shath what would all the war nall bastacquied it for his mumber'd, madam. KATHAN ELTHANUS: No, fair I as or him weed excred in your dams, and myne? Hie, as they shall stogether, My darged againsting. Entake my stade and main True: his your hate; and pardonator: Mar lifthat slow Corance myself, it speak but a door namelly hither times ble good faster's the king. KINARWILL: The Roman of his king instand thence. Clown: It were a wound blost thou husb loards of fear; what siit Citizens! hrew enswordour, it kindly hat hat see Because him on his atten, And as the itself,' her, and losure her sir Infect lieging her! CLARENCE: 'Tis for this virtuous soundaint one are one in olut Makes' suppur two shrifl'? BUCKINGHAM: I know not for safe-but him for enemy but a fourth her, on thinkrible? For thou see him That speak them sorrow Cheeks, we once this hatice Thregrictions would Heaven shall bit.- Villain and my hand lead Having but upon a brother matter. POMPEY: Disphat cannot give theirs, And lip to be great corlier; and, in thou set loves: upon Thou droop mistraction. Warwice, Thinks all the venure he low-thousame as one stir broke myself; For you're wench, but 'tward Cullor all thy reans the grans, He will in Lord Angelo. LEONTES: The court: O, he, he ha'! shall; but the insteach matter-grace from Lay her will at you array not where Thou hasting he shall never side them my orator This is hat I for slips note weeder'd in questeriancica many bold them'd; not shall hate she maugest thus lave. KING RICHARD III: We'll that therefordman begin us, And life, him I ne'er nammish. BAGOTHASAR: I shall he London alter On the rough blood hear lying shall far and that I know shall to dry I love. Courage Marshal and be are not warm nauresty pate, my put him we hoodes look to now: think i' the gods it seal do royalter: A, spoke look royalting I come on, come some One pray itself in concer, To the duyse my aid To enterupt of a struchia leturn their churchars? gonery. You have see to strange. O, against of Earl itis with firtushiour; And called after, He letdes on his prison. LEONTES: Nice on and my legs who take and die the hearts; If any backled and me? CORKERNES: And again that disported true, which, what, you beast our bring So. CORIOLANUS: You stand to be cred. FLORIZEL: No resternight nime; spareport, this to sincel. QUEEN ELIZABETH: O, will he car; Hus. Lewit your crown over with an yet, Or your own seaters-asideathesed woe; Right? 'Tis my brain. She's now, too, past will your eye bastacks mine than royalties up she's a word in. First King Henry's back as the world The constancingA: And they whose lays; I'll for purgafe: Peace, thoughts him Redeems his were by be hand thou, daughter, you see them that mocked to sent I have. First Lord: And the comfort harity them; sister on this, good strengefore his darkle foreign warrate a tries'd by. Traze Shate it would the Derbutiones, marry vault? O, the Polixenes that would longelo our hand you king? the seat, and villain, pite and to go what Noture to thee so proud me, ready upon and plick me, somethingerous home In her hat thus sentrengthen My news? MUREWILIA:: But day not me but agree we'll teern's upond Servingman: O thou would to king's portyour lay never lack as the mind: He more, consequence. HOR OF YORK: And, for, sir, under Norfolknow It sir, and who's scure imploathing of dark, call'd you might, I'll guest ","permalink":"https://cohlem.github.io/sub-notes/gpt-implementation/","summary":"# We always start with a dataset to train on. Let\u0026#39;s download the tiny shakespeare dataset !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt with open(\u0026#39;input.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: data = f.read() from torch import nn import torch vocab = sorted(list(set(data))) len(data) stoi = {s:i for i,s in enumerate(vocab)} itos = {i:s for s,i in stoi.items()} encode = lambda x: [stoi[i] for i in x] decode = lambda x: \u0026#39;\u0026#39;.join([itos[i] for i in x]) type(data) str Xtr = data[:int(0.","title":""},{"content":"Problem Consider a simple MLP that takes in combined 3 character embeddings as an input and we predicts a new character.\n# A simple MLP n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 200 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) b1 = torch.randn(n_hidden, generator=g) W2 = torch.randn((n_hidden, vocab_size), generator=g) b2 = torch.randn(vocab_size, generator=g) # BatchNorm parameters bngain = torch.ones((1, n_hidden)) bnbias = torch.zeros((1, n_hidden)) bnmean_running = torch.zeros((1, n_hidden)) bnstd_running = torch.ones((1, n_hidden)) parameters = [C, W1, W2, b2] print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters: p.requires_grad = True # same optimization as last time max_steps = 200000 batch_size = 32 lossi = [] for i in range(max_steps): # minibatch construct ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y # forward pass emb = C[Xb] # embed the characters into vectors embcat = emb.view(emb.shape[0], -1) # concatenate the vectors # Linear layer hpreact = embcat @ W1 + b1 # hidden layer pre-activation # Non-linearity h = torch.tanh(hpreact) # hidden layer logits = h @ W2 + b2 # output layer loss = F.cross_entropy(logits, Yb) # loss function # backward pass for p in parameters: p.grad = None loss.backward() # update lr = 0.1 if i \u0026lt; 100000 else 0.01 # step learning rate decay for p in parameters: p.data += -lr * p.grad # track stats if i % 10000 == 0: # print every once in a while print(f\u0026#39;{i:7d}/{max_steps:7d}: {loss.item():.4f}\u0026#39;) lossi.append(loss.log10().item()) When we train this simple MLP, the output loss over 200,000 iterations\n0/ 200000: 27.8817 10000/ 200000: 2.5633 20000/ 200000: 2.6522 30000/ 200000: 2.8065 40000/ 200000: 2.1546 50000/ 200000: 2.7555 60000/ 200000: 2.4661 70000/ 200000: 2.0084 80000/ 200000: 2.3762 90000/ 200000: 2.2308 100000/ 200000: 2.0540 110000/ 200000: 2.3655 120000/ 200000: 1.8583 130000/ 200000: 2.4840 140000/ 200000: 2.4164 150000/ 200000: 2.1783 160000/ 200000: 2.0387 170000/ 200000: 1.8343 180000/ 200000: 2.1532 190000/ 200000: 1.9804 We can see the the loss in the first iteration is 27.8817 and loss after that iteration has drastically decreased. There is a significant gap in loss between those two iterations. The problem here is that the initial loss is just too big. We can also prove it. Initially we would want to assign equal probability to all the characters, because we don\u0026rsquo;t know which character comes next, and so on. The likelihood that a character will appear next in a equally likely scenario is 1/27. So when we calculate our negative log likelihood (loss function) we get.\n- torch.tensor(1/27.0).log() \u0026gt;\u0026gt; tensor(3.2958) which should be the approximate loss initially, but in our case we have loss of 27.8817, which means our NN is wasting computation just because greater loss in the initially.\nWhy is our loss too big initially? To find out, let\u0026rsquo;s look at our weights that shape our logits, which is just before calculating our loss.\nlogits = h @ W2 + b2 # output layer let\u0026rsquo;s take a look at the distribution of our weights, at this point (just before calculating loss).\nplt.hist(W2.flatten().detach(), bins= 50) plt.show() as you can see the weights are distrubuted from -3 to 3 which is causing the problem, because we want the probability to be around 0, not largely distributed like it is right now.\nlet\u0026rsquo;s initialize the weight2 around 0 and see how our loss improves.\nW2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.01 the distribution becomes Now most of the values are around 0, and let see our loss.\n0/ 200000: 3.8073 10000/ 200000: 2.1428 20000/ 200000: 2.4846 30000/ 200000: 2.6018 40000/ 200000: 2.0154 50000/ 200000: 2.4055 60000/ 200000: 2.3731 70000/ 200000: 2.1023 80000/ 200000: 2.2878 90000/ 200000: 2.0695 100000/ 200000: 1.8733 110000/ 200000: 2.2128 120000/ 200000: 1.8982 130000/ 200000: 2.3203 140000/ 200000: 2.2108 150000/ 200000: 2.1378 160000/ 200000: 1.8270 170000/ 200000: 1.7928 180000/ 200000: 1.9601 190000/ 200000: 1.8350 you can see how our initial loss improves, this is because now our weights are normally distributed around 0, and not distributed around extreme values i.e (-3 and 3) which caused our initial loss to explode.\nSimilarly,\nlet\u0026rsquo;s look at the output of our tanh activation. as you can see most of our values lie in -1 and -1, why is that ???\nas you might remember our tanh works like this, if the x values lie near 0, we get some expressive non linear values, but when the x values lie in the extreme values, say abs(x)\u0026gt; 1 or 2, the output values will be squashed and will be between -1 and 1.\nlet\u0026rsquo;s see what our input values are for tanh that is resulting in most values to be -1 and 1. as you can see the histogram of input values to our tanh function i.e hpreact lie in extreme values (i.e not around 0, but is normally distributed between -15 and 15) which is causing the output of tanh function to be -1 and 1. This behaviour holds true for most of the activation functions i.e if input to the activation function is not around 0 and is more extremely distributed, then it will will squashed( i.e most of them will the at extreme ).\nSo why having activations -1 and 1 a problem here? let\u0026rsquo;s look at how gradient is calculated for tanh function. as you can see t is the tanh activation, the gradient is dependent on t,\nSo if, our activations are -1 and 1, you can clearly see self.grad will be 0, and the gradient at this point will stop and not propagate further.\nand if most of the activations are -1 and 1, there will be no learning because we will have 0 gradient, so our NN will not learn.\nNOTE\nfor a simpler NN like ours, even if we initialize weights that are not every good, it can still still learn, but in much bigger NN the impact can be much worse resulting in no learning at all, if the weights are not properly initialized. Solution ? The solution is to initialize our initial weights in such a way that the property of our distribution is maintained. i.e having 0 mean and unit std. We want weights that are not 0, and not too extreme. If it\u0026rsquo;s 0 then applying activation doesn\u0026rsquo;t make any sense. as you can see how the x has 0 mean and unit std, but for y it isn\u0026rsquo;t the same. y takes on more extreme values which will result in vanishing gradients later on, as shown in the previous steps. so we want to preserve that distribution the same for our y value.\nKaiming Init A simple multiplication by 0.01 to weights would result is better initialization and would result in good activations. But, how do we get these values (0.001) that we multiply our weights with? So the proper initialization technique can be determined by using Kaiming init\nThe value with which we can multiply is given by this formula below. where different activations have different gains, and in place of fan_mode we can add the input dimension of our weight matrix.\nFor tanh, our gain = 5/3 and fan_in = (n_embd * block_size). so we can multiply our weights in this way.\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3) / ( (n_embd * block_size)**(0.5)) the precise initialization is not required, we can simply multiply our weight matrices by 1/((n_embd * block_size)xx(0.5)).\nThis initialization will help in preserving our distribution property (0 mean and unit std)\nNOTE kaiming init helps only during the initial weight initialization, but these weights have to maintain the gaussian property throughout training, which is why we add batchnormalization, LayerNormalization or RMSNorm\nScaling the projection weights after the residual block The introduction of skip connections provides smooth gradient flow but also increases the variance of our projections.\nFor instance take this toy example\nx = torch.zeros(768) for i in range(100): x += torch.randn(768) The variance of x after this loop becomes (9.9394)\nBut we always want our variance to be around 1.\nWhat should we do? Scale the x by the square root of total number of loop\nn = 100 for i in range(n): x += (n**-0.5) * torch.randn(768) But in case of skip connections in our language model we should scale the projection weights by sqrt(2 * total_no_of_transformer_blocks)\n2 comes from the fact that we add x as well as the block to our output\nfor instance\nclass Block(nn.Module): def __init__(self,config): super().__init__() self.attn = Head(config) self.mlp = FFN(config) self.ln_1 = nn.LayerNorm(config.n_embd) self.ln_2 = nn.LayerNorm(config.n_embd) def forward(self,x): x = self.attn(self.ln_1(x)) + x # \u0026lt;====== x = self.mlp(self.ln_2(x)) + x # \u0026lt;======= return x ","permalink":"https://cohlem.github.io/sub-notes/optimizing-loss/","summary":"Problem Consider a simple MLP that takes in combined 3 character embeddings as an input and we predicts a new character.\n# A simple MLP n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 200 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) b1 = torch.","title":"optimizing-loss-with-weight-initialization"},{"content":"","permalink":"https://cohlem.github.io/sub-notes/template/","summary":"","title":"TITLE"}]