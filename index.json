[{"content":"Image Source: https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts\nBasic MoE structure Experts are FFNN themselves, instead of passing input representation to only one dense FFNN we now have option to route them to more FFNNs. Since most LLMs have several decoder blocks, a given text will pass through multiple experts before the text is generated.\nDown the line it could use multiple experts but at different blocks i.e (layers)\nA routing layer is set to choose experts depending on how many experts are selected MoE are categorized into two i.e dense MoE in which almost all the experts are selected and sparse MoE only some experts are selected.\nNot only will there be an uneven distribution of experts chosen, but some experts will hardly be trained at all. This results in issues during both training and inference.\nInstead, we want equal importance among experts during training and inference, which we call¬†load balancing. In a way, it‚Äôs to prevent overfitting on the same experts.\nLoad Balancing To balance the importance of experts, we will need to look at the router as it is the main component to decide which experts to choose at a given time.\nKeepTopK By introducing trainable (gaussian) noise, we can prevent the same experts from always being picked. It\u0026rsquo;ll help router to distribute experts and not restrict to some specific experts.\nCapacity Factor Distributing experts is not enough because distribution of expert happens close to no.of.steps times but there are a lot of batch of tokens that are processed in a single step. An expert could be assigned more than the others but it can also be assigned less tokens as compared to others. The solution is to equally divide the number of tokens to all the expert using capacity factor given by this formula. Implementation Now that we know what MoE is, let\u0026rsquo;s implement it from scratch.\nImplementation of Adaptive Mixture of Local Experts The MoE was defined as a set of independent¬†experts¬†(feed-forward networks) alongside a¬†gating network¬†(also a feed-forward network,¬†). All the experts and the gating network receive the same input¬†. The gating network outputs the distribution of each expert relevance/importance for the given input and is defined as by¬†Softmax(x@Wg)¬†in its simplest form, where Wg¬†is a (optional) learnable transformation. Finally, the output of the system is the sum of the outputs of all experts weighted by the output of the gating network.\nfrom dataclasses import dataclass torch.manual_seed(42) @dataclass class Config(): n_embd:int = 10 block_size:int = 5 expert_size:int = 2 vocab_size:int = 65 class Router(nn.Module): def __init__(self,config): super().__init__() self.config = config self.router = nn.Sequential(nn.Linear(self.config.n_embd,self.config.expert_size)) def forward(self,x): return self.router(x) class FFN(nn.Module): def __init__(self,config): super().__init__() self.config = config self.ffn = nn.Sequential(nn.Linear(self.config.n_embd, self.config.vocab_size)) def forward(self,x): return self.ffn(x) class MoE(nn.Module): def __init__(self, config): super().__init__() self.config = config self.router = Router(config) self.experts = nn.ModuleList([FFN(config) for _ in range(config.expert_size)]) def forward(self, x): # we = self.we(x) # batch\u0026#39;s embeddings (B,T,C) ep = self.router(x).softmax(dim=-1) # expert probability (B,T,E) ep = ep.unsqueeze(-1) # adding one dim to each of our experts, (B,T,E,1) exp_out = torch.stack([out(x) for out in self.experts], dim=-2) # (B,T,E,C) out = exp_out * ep # (B,T,E,C) x (B,T,E,1) out = out.sum(-2) # (B,T,C) return out Implementation of OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER Using all the experts for inputs will be computationally expensive. A way to reduce that is to implement noise_gating + topK method specified in the paper OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER\nlet\u0026rsquo;s understand these equation with reference to the code\nclean_logits = self.router(x) # corresponds to (x.W) in the equation 4 This is simply using the gating function to calculate the probability of experts for each input tokens. (this self.router(x) is an object of a class Router defined above)\nnoise = torch.randn_like(clean_logits)*F.softplus(self.wnoise(x)) here torch.randn_like(clean_logits) resembles StandardNormal(), where we choose a random gaussian noise (mean = 0, std=1) to be added as a noise. Adding this will introduce some noise which encourages model to choose other experts.\nself.wnoise is a noise but it\u0026rsquo;s input dependent learnable parameter, because we don\u0026rsquo;t want to choose experts completely randomly, instead it has to be input dependent, and it is learned during backprop.\nand by adding F.softplus we are capping the output of noise to be greater than 0. it\u0026rsquo;s approximately similar to relu.\nh = clean_logits + noise we add the clean logits. and the noise, so we now encourage models to explore other experts too.\nNow, let\u0026rsquo;s make our MoE compute efficient i.e by choosing only the topK models for specific tokens.\ntopk_val, topk_ind = torch.topk(h,self.config.top_k, dim = -1) it will choose the topK experts for each token. topK is taken along the last dimension i.e -1 because experts probability is along the last axis. i.e h shape is (B,T,expert_size)\nout = torch.full_like(h, fill_value=float(\u0026#39;-inf\u0026#39;)) out = out.scatter(dim=-1,index=topk_ind, src=topk_val) out = torch.softmax(out, dim=-1) we are now creating a new tensor with all the values with negative infinity and setting the topK values with their original values and for other there will be negative infinity. Now normalizing using softmax we get normalized probabilities and 0 in place of negative infinity.\nnow that we have our expert\u0026rsquo;s probability for each input. Let\u0026rsquo;s now pass the input through expert and take the weighted sum because we have topk probability assigned to a token input.\ncalculating weighted sum for the input tokens can be a difficult in terms of implementation.\nThe general idea is to iterate over all the experts to first create a mask from our router\u0026rsquo;s output probabilities for each expert. (i.e creating a mask of True if this input probability in within topk for that specific expert) and flatten that mask, and pluck out the inputs from the flattened input using that mask (mask will help us pluck out input tokens with specific index where mask value is true) and pass that plucked out input to the expert layer and then multiply the expert layer\u0026rsquo;s output with the router\u0026rsquo;s probability for that specific expert and then keep adding these values for all the experts because we are doing the weighted sum.\nThe code to do that is given below (it can take some time to understand, but it\u0026rsquo;s relative easy if you understand this explanation)\ndef forward(self, x): # we = self.we(x) # batch\u0026#39;s embeddings (B,T,C) out,topk_val,topk_ind = self.router(x) # out is size (B,T,experts) experts (B,T,top_k) flat_x = x.view(-1, x.shape[-1]) #(B*T,C) final_output = torch.zeros_like(flat_x) # for i,expert in enumerate(self.experts): expert_mask = (out[:,:,i] != 0) # (B,T) expert_mask = expert_mask.view(-1) # (B*T) if expert_mask.any(): # pass through expert layer only if flattened expert has any one true value select_x = flat_x[expert_mask] expert_x = expert(select_x) * (out[:,:,i]).view(-1)[expert_mask].unsqueeze(-1) #unsqueeze for broadcasting across columns final_output[expert_mask] += expert_x # add because our inp is combination of experts, i.e one token can take can top2 prob final_output = final_output.view(x.shape) return final_output Full code til here class Router(nn.Module): def __init__(self,config): super().__init__() self.config = config self.router = nn.Sequential(nn.Linear(self.config.n_embd,self.config.expert_size)) self.wnoise = nn.Linear(self.config.n_embd, self.config.expert_size) def forward(self,x): clean_logits = self.router(x) noise = torch.randn_like(clean_logits)*nn.Softplus(self.wnoise(x)) h = clean_logits + noise topk_val, topk_ind = torch.topk(h,self.config.top_k, dim = -1) out = torch.full_like(h, fill_value=float(\u0026#39;-inf\u0026#39;)) out = out.scatter(dim=-1,index=topk_ind, src=topk_val) out = torch.softmax(out, dim=-1) return out,topk_val, topk_ind class MoE(nn.Module): def __init__(self, config): super().__init__() self.config = config self.router = Router(config) self.experts = nn.ModuleList([FFN(config) for _ in range(config.expert_size)]) def forward(self, x): # we = self.we(x) # batch\u0026#39;s embeddings (B,T,C) out,topk_val,topk_ind = self.router(x) # out is size (B,T,experts) experts (B,T,top_k) flat_x = x.view(-1, x.shape[-1]) #(B*T,C) final_output = torch.zeros_like(flat_x) for i,expert in enumerate(self.experts): expert_mask = (out[:,:,i] != 0) # (B,T) expert_mask = expert_mask.view(-1) # (B*T) print(em_new == expert_mask) if expert_mask.any(): select_x = flat_x[expert_mask] expert_x = expert(select_x) * (out[:,:,i]).view(-1)[expert_mask].unsqueeze(-1) #unsqueeze for broadcasting across columns final_output[expert_mask] += expert_x # add because our inp is combination of experts, i.e one token can take can top2 prob final_output = final_output.view(x.shape) return final_output This implementation assigns expert for each no.of.steps in our training step, here even if the experts are assigned equally, the total number of tokens assigned to those expert could be different, for that reason we would want to distribute input tokens equally to all the experts. We can do that using expert capacity explained above in the Load Balancing section above.\nExpert Capacity implementation we define expert capacity as total number of tokens in a batch divided by number of experts times the capacity factor (which controls the scale).\nexp_cap = int(B*T*self.config.top_k / self.config.expert_size * self.config.capacity_factor) select the indices of input where this specific expert is applied and limit the inputs to be processed to be within the expert capacity if it is greater than expert capacity and truncate other tokens.\nselected_indices = torch.nonzero(expert_mask).squeeze(-1) limited_indices = selected_indices[:exp_cap] if selected_indices.numel() \u0026gt; exp_cap else selected_indices Full implementation class Router(nn.Module): def __init__(self,config): super().__init__() self.config = config self.router = nn.Sequential(nn.Linear(self.config.n_embd,self.config.expert_size)) self.wnoise = nn.Linear(self.config.n_embd, self.config.expert_size) def forward(self,x): clean_logits = self.router(x) noise = torch.randn_like(clean_logits)*F.softplus(self.wnoise(x)) h = clean_logits + noise topk_val, topk_ind = torch.topk(h,self.config.top_k, dim = -1) out = torch.full_like(h, fill_value=float(\u0026#39;-inf\u0026#39;)) out = out.scatter(dim=-1,index=topk_ind, src=topk_val) out = torch.softmax(out, dim=-1) return out,topk_val, topk_ind class MoE(nn.Module): def __init__(self, config): super().__init__() self.config = config self.router = Router(config) self.experts = nn.ModuleList([FFN(config) for _ in range(config.expert_size)]) self.wimportance = 1.0 def forward(self, x): # we = self.we(x) # batch\u0026#39;s embeddings (B,T,C) B,T,C = x.shape out,topk_val,topk_ind = self.router(x) # out is size (B,T,experts) experts (B,T,top_k) flat_x = x.view(-1, x.shape[-1]) #(B*T,C) final_output = torch.zeros_like(flat_x) exp_cap = int(B*T*self.config.top_k / self.config.expert_size * self.config.capacity_factor) for i,expert in enumerate(self.experts): expert_mask = (out[:,:,i] != 0) # (B,T) expert_mask = expert_mask.view(-1) # (B*T) selected_indices = torch.nonzero(expert_mask).squeeze(-1) limited_indices = selected_indices[:exp_cap] if selected_indices.numel() \u0026gt; exp_cap else selected_indices if expert_mask.any(): select_x = flat_x[limited_indices] expert_x = expert(select_x) * (out[:,:,i]).view(-1)[limited_indices].unsqueeze(-1) #unsqueeze for broadcasting across columns final_output[limited_indices] += expert_x # add because our inp is combination of experts, i.e one token can take can top2 prob final_output = final_output.view(x.shape) # Importance scores for experts i.e batchwise sum of the router\u0026#39;s output importance = out.sum(dim=0) # loss that needs to be added to encourage model to choose diverse experts imp_std,imp_mean = importance.std(), importance.mean() loss_moe = ((imp_std/imp_mean)**2)*self.wimportance return final_output,loss_moe Auxiliary Loss To encourage models to make expert\u0026rsquo;s probability uniform (choosing all the experts and not restricting to some experts) We add to our main loss another loss term that we get from our MoE layer. It is calculated by first calculating the importance which is simply calculating the batch sum over the inputs for the router\u0026rsquo;s output and calculating the square of the coefficient of variation from the importance and then multiplying with a hand tuned scaling factor called Wimportance.\nThe implementation from this auxiliary loss is already implemented in the code above.\nVisualizations what\u0026rsquo;s the point of adding the MoE losses and noises if we can\u0026rsquo;t visualize it\u0026rsquo;s effect. I\u0026rsquo;ve trained my GPT + MoE architecture with variations to visualize what they actually do.\nNo Noise and No MoE loss I did not add the noise term i.e in the equation 4 in the picture above and did not add MoE loss to our original loss function.\nx axis = no of steps y axis = no of tokens assigned to each expert Even though the plot seems to fluctuate too much, we can see that there is inappropriate distribution of tokens among the experts i.e expert 0 is assigned less tokens and and expert 1 and 3 are assigned more tokens and the curve relatively stays the same because we did not add the loss function too.\nNoise but no MoE loss There is relatively small gap between number of tokens assigned to the experts because this time we added gaussian noise which reduces the gap, but the number of tokens assigned to them remains constantly fluctuating.\nNo Noise but MoELoss included As you can see there was inappropriate distribution of tokens in the beginning but the model seems to have learned to distribute the tokens among the experts after training for some time.\nIncluding Noise and MoE Loss it looks similar to the previous one, i.e big variation in the beginning but learns to distribute afterwards, its different from the previous one in that initially the range of tokens assigned to experts are between (150, 350) but in the previous plot it was (100,400). This less variation in this plot can be attributed to the addition of gaussian noise.\nIn conclusion, addition of loss seems to be more important than including the Noise because it seems to become stable in the later iterations.\nWhole code with GPT architecture and this MoE implementation can be found here: https://github.com/CohleM/deep_learning/blob/master/MoE/moe.ipynb\nImprovements Replace for loop with matrix multiplication while calculating the weighted sum of experts. Improve Block class\u0026rsquo; implementation, it doesn\u0026rsquo;t look neat, we return the modified x and the loss from the moe Switch Transformers (paper) Key points model improves as the parameters are increase. Following the same pattern they increase the parameter count but with same the FLOP used for a token in previous implementations. Instead of routing tokens to topK \u0026gt; 1 experts they route tokens to k=1 experts (this preserves model quality, reduces routing computation and performs better) since gate function is a softmax layer it is perfectly differentiable. (out of this paper: discrete operations are not differentiable i.e choosing max from a list, cause derivative of constant is 0 so gradient propagation stops)\nChoosing right capacity factor is important as shown in figure below. as you can see the when CF is 1 only one token is truncated (not evaluated) but is passed through to next layers through residual connection.\nwhen CF is 1.5 3 memory spaces are wasted. So there\u0026rsquo;s a tradeoff. They find ensuring lower rates of dropped tokens are important for the scaling of sparse expert-models. The auxiliary loss is given by this equation Since we seek uniform routing of the batch of tokens across the N experts, we desire both vectors to have values of 1/N\n1/N is a percentage term i.e if there are 4 experts then each experts should be assigned 25% i.e 1/4 tokens.\nsimilarly for Pi it should be 1/n because each router should assign equal probability i.e 25% to promote uniform distribution.\nThe implementation of loss is likewise.\nF vector consider this sample is our router probability for all 5 tokens and 4 experts (5x4) matrix\nsample = torch.randn(5,4) sample tensor([[ 0.0384, 0.3811, -0.9004, 0.0853], [ 0.2770, 0.1141, -0.6625, 0.4889], [ 0.7854, 0.7123, -0.3660, -1.2273], [ 0.9355, 1.9071, 0.7386, -0.3621], [ 0.8633, -0.5028, -1.0617, -1.2414]]) val,ind = torch.topk(sample,k=1,dim=-1) val,ind (tensor([[0.3811], [0.4889], [0.7854], [1.9071], [0.8633]]), tensor([[1], [3], [0], [1], [0]])) f_vector = torch.zeros_like(sample) ones = torch.ones_like(sample) f_vector tensor([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) f_vector.scatter_(dim=-1,index=ind, src=ones) tensor([[0., 1., 0., 0.], [0., 0., 0., 1.], [1., 0., 0., 0.], [0., 1., 0., 0.], [1., 0., 0., 0.]]) f_vector.sum(dim=0) tensor([2., 2., 0., 1.]) f_vector = f_vector.sum(dim=0)/f_matrix.shape[0] f_vector tensor([0.4000, 0.4000, 0.0000, 0.2000]) see the imbalance? each expert should get 1/4= 0.25 tokens, we need to minimize this inappropriate distribution by adding this in loss function.\nP vector sample tensor([[ 0.0384, 0.3811, -0.9004, 0.0853], [ 0.2770, 0.1141, -0.6625, 0.4889], [ 0.7854, 0.7123, -0.3660, -1.2273], [ 0.9355, 1.9071, 0.7386, -0.3621], [ 0.8633, -0.5028, -1.0617, -1.2414]]) sample = sample.softmax(dim=-1) sample tensor([[0.2599, 0.3661, 0.1016, 0.2724], [0.2877, 0.2444, 0.1124, 0.3555], [0.4203, 0.3907, 0.1329, 0.0562], [0.2111, 0.5578, 0.1734, 0.0577], [0.6567, 0.1675, 0.0958, 0.0800]]) p_vector = sample.sum(dim=0)/sample.shape[0] p_vector tensor([0.3671, 0.3453, 0.1232, 0.1644]) as you can see the experts have imbalanced distribution of probabilities, the objective of including P vector is to make this p vector\u0026rsquo;s distribution uniform\nloss function N = 4 alpha = 1.0 loss = alpha * N * (p_vector*f_vector).sum() loss tensor(1.2714) DeepSeekMoE (paper) One of the recent MoE paper.\nThere were these limitations.\nKnowledge Hybridity: models utilize small experts (N), information is shared among these experts, and experts can\u0026rsquo;t be specialized. Knowledge Redundancy: They may train same tokens resulting in experts that learn same concepts, ultimately there is redundancy of knowledge. In this paper, they propose two changes.\nFie-grained Expert Segmentation: i.e divide the hidden dimension of current MoE layer to 1/m and create separate mxN number of experts (total parameters remains the same). Doing so will result in greater possibility of choosing experts for a token and experts can be specialized. Shared Expert Isolation: There must be expert that should process some general knowledge task. for that reason they separate out some experts for this knowledge sharing. By sharing knowledge, the fine grained experts don\u0026rsquo;t need to acquire extra knowledge, enabling them to specialize in specific tasks. The output representation for a batch, will look like this.\nThe loss is similar to what we read in switch transformers but incorporating the changes that we made i.e dividing expert into m expert and assigning a common expert. The also include device level balance loss, which is for balancing load across devices. I haven\u0026rsquo;t read more into how load is balanced in devices, which I leave it for future studies.\nReferences A Visual Guide to Mixture of Experts (MoE) Adaptive Mixture of Local Experts OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models) ","permalink":"https://cohlem.github.io/sub-notes/mixture-of-experts/","summary":"Image Source: https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts\nBasic MoE structure Experts are FFNN themselves, instead of passing input representation to only one dense FFNN we now have option to route them to more FFNNs. Since most LLMs have several decoder blocks, a given text will pass through multiple experts before the text is generated.\nDown the line it could use multiple experts but at different blocks i.e (layers)\nA routing layer is set to choose experts depending on how many experts are selected MoE are categorized into two i.","title":"Mixture of Experts"},{"content":"conversational open data: https://huggingface.co/datasets/OpenAssistant/oasst1/viewer/default/train?row=0\nsynthetic data https://github.com/thunlp/UltraChat?tab=readme-ov-file\nTo teach models to say certain things we either train it on sft datasets, or we put it in system message i.e put it in it\u0026rsquo;s context windows (usually hidden from users)\nolmo hard coded sft mixture data https://huggingface.co/datasets/allenai/olmo-2-hard-coded?row=2\n2**19 = 524288\nset grad_accumulation_step = 2 power 19/(512* 64) = 16\nassuming gpu can handle 64 batches\ngrad_accumulation_step will be reduced again by number of gpus available.\ntotal_iters = 20000\neval it on every 200 steps\n","permalink":"https://cohlem.github.io/sub-notes/post-training-strategies/","summary":"conversational open data: https://huggingface.co/datasets/OpenAssistant/oasst1/viewer/default/train?row=0\nsynthetic data https://github.com/thunlp/UltraChat?tab=readme-ov-file\nTo teach models to say certain things we either train it on sft datasets, or we put it in system message i.e put it in it\u0026rsquo;s context windows (usually hidden from users)\nolmo hard coded sft mixture data https://huggingface.co/datasets/allenai/olmo-2-hard-coded?row=2\n2**19 = 524288\nset grad_accumulation_step = 2 power 19/(512* 64) = 16\nassuming gpu can handle 64 batches\ngrad_accumulation_step will be reduced again by number of gpus available.","title":"Post Training Strategies"},{"content":"Pre-training Document packing while pretraining, different documents could be packed inside a sequence. For instance, a model with context_length 1024 can have 256 tokens from one doc and rest from the other. Demilited by EOS token.\nThe samples may contaminate the attention, for which cross sample attention masking is used. But, it isn\u0026rsquo;t used by DeepSeek v3, lets not use it.\nwhile packing documents. we simply pack them as they appear in order and then add EOS token (used by GPT-2,3). But DeekSeek also uses FIM (Fill in middle) strategy using this Prefix-Suffix-Middle (PSM) framework.\n\u0026lt;|fim_begin|\u0026gt; ùëìpre \u0026lt;|fim_hole|\u0026gt; ùëìsuf \u0026lt;|fim_end|\u0026gt; ùëìmiddle \u0026lt;|eos_token|\u0026gt;.\nadopted for 0.1% of data, generally used for overfitting or limiting the model from using the same general method.\nDo vibe check once in a while commands\nchange num_proc in process.py\npython process.py \u0026ndash;tokenizer_path /model/tokenizer\ntraining run torchrun \u0026ndash;standalone \u0026ndash;nproc_per_node=2 pretrain.py\n","permalink":"https://cohlem.github.io/sub-notes/building-lillm/","summary":"Pre-training Document packing while pretraining, different documents could be packed inside a sequence. For instance, a model with context_length 1024 can have 256 tokens from one doc and rest from the other. Demilited by EOS token.\nThe samples may contaminate the attention, for which cross sample attention masking is used. But, it isn\u0026rsquo;t used by DeepSeek v3, lets not use it.\nwhile packing documents. we simply pack them as they appear in order and then add EOS token (used by GPT-2,3).","title":"Notes-while-building-lilLM"},{"content":"torch.stack(tensors, dim) stacks the tensors across dim\n#usage # data has to be tensor torch.stack([data[i:i+some_number] for i in range(10)]) torch.from_numpy(numpy_array) shares the memory with the numpy_array but is tensor type\na = np.array([1,2,3]) b = torch.tensor(a) # creates copy c = torch.from_numpy(a) # shares memory a[0] = 11 c # outputs: tensor([11, 2, 3]) torch.flatten(input, start,end=-1) flattens the input from dim start to end (-1 by default)\nt = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]) torch.flatten(t) torch.flatten(t, start_dim=1) # (2,2,2) --\u0026gt; (2,2*2) [5, 6, 7, 8]]) torch.stack and torch.cat((tensors), dim) torch.stack stacks tensors along new dim, whereas torch.cat concatenates along that specific dim.\nexample:\na = torch.randn(2,5,8,32) b = torch.randn(2,1,8,32) torch.cat((a,b), dim=1).shape #outputs : torch.Size([2, 6, 8, 32]) a = torch.randn(3,5,8,32) b = torch.randn(3,5,8,32) torch.stack((a,b), dim=1).shape #outputs: torch.Size([3, 2, 5, 8, 32]) For the past 2 years I\u0026rsquo;ve been involved in training and experimenting machine learning systems, mostly using third party packages such as sklearn, huggingface and so on. Sometimes the experiments become too specific and the abstraction provided by these packages become a bottleneck for the performance optimization. My research goal is to understand these bottlenecks in deep and write my own optimized code for hardware-specific optimization which enables resource efficient training or inference.\n","permalink":"https://cohlem.github.io/sub-notes/pytorch/","summary":"torch.stack(tensors, dim) stacks the tensors across dim\n#usage # data has to be tensor torch.stack([data[i:i+some_number] for i in range(10)]) torch.from_numpy(numpy_array) shares the memory with the numpy_array but is tensor type\na = np.array([1,2,3]) b = torch.tensor(a) # creates copy c = torch.from_numpy(a) # shares memory a[0] = 11 c # outputs: tensor([11, 2, 3]) torch.flatten(input, start,end=-1) flattens the input from dim start to end (-1 by default)\nt = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]) torch.","title":"Pytorch Commands I forget time to time/ commands that are essential"},{"content":"Unicode Character encoding standard aims to incorporate all the available digital characters Each character in Unicode has a unique 4 to 6-digit hexadecimal number. For Example, the letter \u0026lsquo;A\u0026rsquo; has the code 0041, represented as U+0041. compatible with ASCII first 128 characters in Unicode directly correspond to the characters represented in the 7-bit ASCII table Unicode Transformation Format (UTF-8) uses 1-4 bytes to represent each character can encode all the unicode code points backward compatible with ASCII Example: (1 byte) The character \u0026#39;A\u0026#39; (U+0041) is encoded as¬†`01000001`¬†(0x41 in hexadecimal). (2 byte) The character \u0026#39;¬¢\u0026#39; (U+00A2) is encoded as¬†`11000010 10100010`¬†(0xC2 0xA2 in hexadecimal). (3 byte) The character \u0026#39;‚Ç¨\u0026#39; (U+20AC) is encoded as¬†`11100010 10000010 10101100`¬†(0xE2 0x82 0xAC in hexadecimal). (4 byte) The character \u0026#39;†úé\u0026#39; (U+2070E) is encoded as¬†`11110000 10100000 10011100 10001110`(0xF0 0xA0 0x9C 0x8E in hexadecimal). let\u0026rsquo;s understand some difference between unicode and utf-8 character \u0026lsquo;‚Ç¨\u0026rsquo; has\nunicode code point (hex): U+20AC unicode code point (decimal): 8364\nSo there is a single number (decimal) that represents characters in unicode\nbut, the same character in utf-8 is represented as\nBinary: 11100010 10000010 10101100\nHexadecimal:¬†0xE2 0x82 0xAC\nDecimal:¬†226, 130, 172\nWhy ? utf-8 is a standard that stores characters in 1-4 bytes as described above.\nsimilarly in python, we can get it\u0026rsquo;s hex values by '‚Ç¨'.encode('utf-8') and converting it to list gives us it\u0026rsquo;s list of decimal values and doing ord(\u0026lsquo;‚Ç¨\u0026rsquo;) gives us it\u0026rsquo;s unicode code point in decimal`\nBuild vocabulary We build our vocabulary by gathering chunks of bytes that appear together most of the times,\nSuppose the data to be encoded is\naaabdaaabac\nthe byte pair \u0026ldquo;aa\u0026rdquo; occurs most often, so it will be replaced by a byte that is not used in the data, such as \u0026ldquo;Z\u0026rdquo;. Now there is the following data and replacement table:\nZabdZabac Z=aa Then the process is repeated with byte pair \u0026ldquo;ab\u0026rdquo;, replacing it with \u0026ldquo;Y\u0026rdquo;:\nZYdZYac Y=ab Z=aa again, the pair ZY occurs twice so our data and replacement table becomes.\nXdXac X=ZY Y=ab Z=aa We now write code to implement this same functionality.\nThe function below constructs a dictionary that keeps track of frequency of bytes that come together.\ndef get_stats(text): freq = dict() for t1,t2 in zip(text, text[1:]): freq[(t1,t2)] = freq.get((t1,t2),0) + 1 return freq This merge function is now used to merge the two bytes into one. similar to Z=aa in the example above.\ndef merge(pair, ids, idx): n_ids = [] i=0 while i \u0026lt; len(ids): if i\u0026lt;len(ids) -1 and ids[i] == pair[0] and ids[i+1] == pair[1]: n_ids.append(idx) i+=2 else: n_ids.append(ids[i]) i+=1 return n_ids We do this iteratively for total_merges times,\nmax_vocab = 356 total_merges = max_vocab - 256 merge_dict = {} for i in range(total_merges): stats = get_stats(ids) pair = max(stats, key=stats.get) idx = 256+i ids = merge(pair, text_utf8, idx) merge_dict[pair] = idx Encode lets encode our text into our tokens using our merge_dict which keeps track of all the possible combination of characters.\ndef encode(text): # given a string, return list of integers (the tokens) tokens = list(text.encode(\u0026#34;utf-8\u0026#34;)) while len(tokens) \u0026gt;= 2: stats = get_stats(tokens) pair = min(stats, key=lambda p: merge_dict.get(p, float(\u0026#34;inf\u0026#34;))) print(pair) if pair not in merges: break # nothing else can be merged idx = merge_dict[pair] tokens = merge(tokens, pair, idx) return tokens here we need to be careful about how we encode the tokens, i.e for instance the new 256 index in our merge_dict should be encoded first because later index for ex: 352 could be combination of 256 and 108. So we have to maintain this order.\nto do that we first get all the token combination available to us in our dataset using stats = get_stats(tokens), and this line of code pair = min(stats, key=lambda p: merges.get(p, float(\u0026quot;inf\u0026quot;))) finds the pair with the lowest key (lets say 101,32 -\u0026gt; 256) and then merges those 101,32 tokens to be 256 and the process is continued until there are no pairs that can be combined using our merge_dict mapping.\nDecode vocab = {idx : bytes([idx]) for idx in range(256)} for (p1,p2),idx in merge_dict.items(): vocab[idx] = vocab[p1] + vocab[p2] The code above maps index to its corresponding byte in utf-8, and the loop combines the byte information corresponding to their indexs, for instance lets say 256 is combo of 101,32, the bytes of 101 and 32 will be combined. lets say 352 is now a combo of 256 and 32, their byte information will be combined, which will be easier to decode the information in the code below.\ndef decode(ids): tokens = b\u0026#34;\u0026#34;.join(vocab[idx] for idx in ids) return tokens.decode(\u0026#39;utf-8\u0026#39;, errors = \u0026#34;replace\u0026#34;) the first line in the function above maps their index to byte information, and then those utf-8 bytes will be decoded to their corresponding characters in utf-8.\nThis is the building block of tokenizer, everything that comes next is a more complex and efficient version of the tokenizer.\nOpenai\u0026rsquo;s BPE tokenizer Let\u0026rsquo;s first understand the problem our current implementation of tokenizer has.\nIt will tokenize the whole sequence. i.e lets say we have a sentence\nI\u0026rsquo;ve chosen a bit funky sentence here, just for the purpose of explanation.\nyou are 52 years old, u are good too, you've achieved so much\nif you look closely, this sequence of characters u are appear twice and will have their own mapping in the vocabulary.\nso for token u are it will have one index in our vocabulary, we can see it combines characters from two separate words, which we would like to minimize, and we would like to separate out these types of tokens 've because they generally go along with other words as well. For this purpose, we process our initial text through this regex.\nre.compile(r\u0026#34;\u0026#34;\u0026#34;\u0026#39;s|\u0026#39;t|\u0026#39;re|\u0026#39;ve|\u0026#39;m|\u0026#39;ll|\u0026#39;d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\u0026#34;\u0026#34;\u0026#34;) as you can see these tokens s|'t|'re|'ve|'m|'ll|'d| are separated from text, and we separate out words and numbers as well\nimport regex as re samplere = re.compile(r\u0026#34;\u0026#34;\u0026#34;\u0026#39;s|\u0026#39;t|\u0026#39;re|\u0026#39;ve|\u0026#39;m|\u0026#39;ll|\u0026#39;d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\u0026#34;\u0026#34;\u0026#34;) samplere.findall(\u0026#34; you\u0026#39;ve done soo much 1245234 \u0026#34;) The sentence is processed in this way.\n[\u0026#39; you\u0026#39;, \u0026#34;\u0026#39;ve\u0026#34;, \u0026#39; done\u0026#39;, \u0026#39; soo\u0026#39;, \u0026#39; much\u0026#39;, \u0026#39; 1245234\u0026#39;, \u0026#39; \u0026#39;] and then we train our tokenizer on each elements that we get from this.\nComplete code for tokenizer class Tokenizer: def __init__(self): self.pattern = None self.merges = dict() self.vocab = None def get_stats(self,ids, freq=None): freq = dict() if freq is None else freq for t1,t2 in zip(ids, ids[1:]): freq[(t1,t2)] = freq.get((t1,t2),0) + 1 return freq def merge(self, ids, pair, idx): n_ids = [] i=0 while i \u0026lt; len(ids): if i\u0026lt;len(ids) -1 and ids[i] == pair[0] and ids[i+1] == pair[1]: n_ids.append(idx) i+=2 else: n_ids.append(ids[i]) i+=1 return n_ids GPT4Pattern = r\u0026#34;\u0026#34;\u0026#34;\u0026#39;(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\u0026#34;\u0026#34;\u0026#34; class SampleTokenizer(Tokenizer): def __init__(self,max_vocab, pattern=None): super().__init__() self.pattern = re.compile(GPT4Pattern if pattern is None else pattern) self.max_vocab = max_vocab self.total_merges = max_vocab - 256 def train(self, x): \u0026#34;\u0026#34;\u0026#34;This objective of this function is to build the merges dictionary mapping and build vocab\u0026#34;\u0026#34;\u0026#34; chunks = self.pattern.findall(x) ids = [list(ch.encode(\u0026#39;utf-8\u0026#39;)) for ch in chunks] for p in range(self.total_merges): freq = dict() idx = 256 + p for item in ids: self.get_stats(item, freq) pair = max(freq, key=freq.get) ids = [self.merge(i,pair, idx) for i in ids] self.merges[pair] = idx self._build_vocab() def _build_vocab(self): self.vocab = {idx : bytes([idx]) for idx in range(256)} for (p1,p2),idx in self.merges.items(): self.vocab[idx] = self.vocab[p1] + self.vocab[p2] def encode(self, x): # given a string, return list of integers (the tokens) tokens = list(x.encode(\u0026#34;utf-8\u0026#34;)) while len(tokens) \u0026gt;= 2: stats = self.get_stats(tokens) pair = min(stats, key=lambda p: self.merges.get(p, float(\u0026#34;inf\u0026#34;))) if pair not in self.merges: break # nothing else can be merged idx = self.merges[pair] tokens = self.merge(tokens, pair, idx) return tokens def decode(self,ids): tokens = b\u0026#34;\u0026#34;.join(self.vocab[idx] for idx in ids) return tokens.decode(\u0026#39;utf-8\u0026#39;, errors = \u0026#34;replace\u0026#34;) st = SampleTokenizer(356) st.train(text) a = st.encode(\u0026#34;hey how are you doing 124\u0026#34;) st.decode(a) # outputs: \u0026#39;hey how are you doing 124\u0026#39; ","permalink":"https://cohlem.github.io/sub-notes/tokenization/","summary":"Unicode Character encoding standard aims to incorporate all the available digital characters Each character in Unicode has a unique 4 to 6-digit hexadecimal number. For Example, the letter \u0026lsquo;A\u0026rsquo; has the code 0041, represented as U+0041. compatible with ASCII first 128 characters in Unicode directly correspond to the characters represented in the 7-bit ASCII table Unicode Transformation Format (UTF-8) uses 1-4 bytes to represent each character can encode all the unicode code points backward compatible with ASCII Example: (1 byte) The character \u0026#39;A\u0026#39; (U+0041) is encoded as¬†`01000001`¬†(0x41 in hexadecimal).","title":"Tokenization"},{"content":"Papers that I\u0026rsquo;ve read with their respective notes.\nLLaMA: Open and Efficient Foundation Language Models Trained on 1.4T tokens. Wikipedia and Books domain trained for 2 epochs (maybe because its cleaner, smaller, offers coherent long sequences) use manual backprop for training efficiency i.e save checkpoints of activations that take longer to compute (linear layers) and use them during backprop and generate others such as (ReLu) on the fly. SmolLM2 including specific data eg. math doesn\u0026rsquo;t only do well in math, but also seems to improve reasoning.\nrather than training on one specific dataset, training on mixture of datasets yields better results, for instance, 60-40 mixture of FineWeb-Edu and DCLM yielded almost similar performance to only training on FineWeb-Edu\ndecontamination of curated dataset is generally done, using some bi-gram matching using the eval dataset.\nthey do a multi-stage training approach rather than fixed-data mixture.\nLR decay\nWarmup Phase (Steps 0‚Äì2,000):\nLearning rate increases linearly from near 0 to¬†5.0√ó10‚àí45.0√ó10‚àí4. Stable Phase (Steps 2,000‚ÄìN):\nLearning rate remains constant at¬†5.0√ó10‚àí45.0√ó10‚àí4. Decay Phase (Last 10% of Steps):\nLearning rate decreases linearly from¬†5.0√ó10‚àí45.0√ó10‚àí4¬†to 0. had loss spikes during stage 3, which remained persistent even after rewinding the trianing, and changing the data that caused the spike. The cause of spike remains undetermined, however the eval metrics recovered in the end.\nThey include high quality math data in the end, and decay the to 0\nThey expand the context length from 2k to 8k before the final 75 billion tokens of training and the mixture was adjusted to include 40% long-context documents\nthey curate their own instruction dataset named SmolTalk, because of low performance after training on previously available dataset i.e MagPie-Pro and OpenHermes2.5.\nFilter high conversational dataset and deduplicate using gte-large embedding models.\nin short they do a lot of decontamination (using bi-gram overlaps), deduplication, filtering,\nFor smaller models during sft, they filter smoltalk dataset (e.g., function calling) and hard examples from MagPie-Ultra to better align with the models‚Äô capacity and do DPO on UltraFeedback dataset.\nTOREAD: Training Compute-Optimal Large Language Models https://lifearchitect.ai/chinchilla/#deepmind\n","permalink":"https://cohlem.github.io/sub-notes/paper-summaries/","summary":"Papers that I\u0026rsquo;ve read with their respective notes.\nLLaMA: Open and Efficient Foundation Language Models Trained on 1.4T tokens. Wikipedia and Books domain trained for 2 epochs (maybe because its cleaner, smaller, offers coherent long sequences) use manual backprop for training efficiency i.e save checkpoints of activations that take longer to compute (linear layers) and use them during backprop and generate others such as (ReLu) on the fly. SmolLM2 including specific data eg.","title":"Papers Summaries"},{"content":"KV Cache KV cache visual operation In the note blow, I first describe how inferencing is done if we simply do operation without KV cache and then describe how KV cache helps removing redundant operations.\nWe don\u0026rsquo;t make use of KV cache while training because we already have data filled for each sequence length, we don\u0026rsquo;t need to calculate loss one by one, instead we do it in batches, whereas while inferencing we do it generally for 1 batch with some sequences and then we keep on appending next-predicted token to that sequence one by one. To understand better look at the notes below.\nMemory needed for storing KV cache let\u0026rsquo;s calculate total memory needed for storing KV cache\nbatch_size = 1 (for inferencing) d_model = 4096 num_of_kv_heads = 32 head_dim = d_model/num_of_kv_heads = 128 seq_len = 10000 precision (fp16) = 2 bytes 2 (for k and v separately) x precision x head_dim x num_of_kv_heads x d_model x seq_len x batch_size = 5,24,28,80,000 bytes close to 5GB lets say we have 7B parameter model, 7x10^9 x2 (bytes) = 14x10^9 bytes = 14GB\nwe need almost 1/3 total memory for inferencing.\nLet\u0026rsquo;s explore the code for using KV cache in Llama models.\nPlease note I\u0026rsquo;ve modified some part of the original Llama code below to just explain the case of KV cache here.\n@dataclass class ModelArgs: dim: int = 4096 n_layers: int = 32 n_heads: int = 32 n_kv_heads: int = 16 # modified to explain GQA vocab_size: int = -1 # defined later by tokenizer multiple_of: int = 256 # make SwiGLU hidden layer size multiple of large power of 2 ffn_dim_multiplier: Optional[float] = None norm_eps: float = 1e-5 max_batch_size: int = 32 max_seq_len: int = 2048 Attention Architecture Code you might be familiar with the code below if you\u0026rsquo;ve implemented attention mechanism on your own (more explanation below)\nclass Attention(nn.Module): \u0026#34;\u0026#34;\u0026#34;Multi-head attention module.\u0026#34;\u0026#34;\u0026#34; def __init__(self, args: ModelArgs): \u0026#34;\u0026#34;\u0026#34; Initialize the Attention module. Args: args (ModelArgs): Model configuration parameters. Attributes: n_kv_heads (int): Number of key and value heads. n_local_heads (int): Number of local query heads. n_local_kv_heads (int): Number of local key and value heads. n_rep (int): Number of repetitions for local heads. head_dim (int): Dimension size of each attention head. wq (ColumnParallelLinear): Linear transformation for queries. wk (ColumnParallelLinear): Linear transformation for keys. wv (ColumnParallelLinear): Linear transformation for values. wo (RowParallelLinear): Linear transformation for output. cache_k (torch.Tensor): Cached keys for attention. cache_v (torch.Tensor): Cached values for attention. \u0026#34;\u0026#34;\u0026#34; super().__init__() self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads # model_parallel_size = fs_init.get_model_parallel_world_size() self.n_local_heads = args.n_heads self.n_local_kv_heads = self.n_kv_heads self.n_rep = self.n_local_heads // self.n_local_kv_heads self.head_dim = args.dim // args.n_heads self.wq = nn.Linear( args.dim, args.n_heads * self.head_dim, bias=False, ) self.wk = nn.Linear( args.dim, self.n_kv_heads * self.head_dim, bias=False, ) self.wv = nn.Linear( args.dim, self.n_kv_heads * self.head_dim, bias=False, ) self.wo = nn.Linear( args.n_heads * self.head_dim, args.dim, bias=False, ) self.cache_k = torch.zeros( ( args.max_batch_size, args.max_seq_len, self.n_local_kv_heads, self.head_dim, ) ) self.cache_v = torch.zeros( ( args.max_batch_size, args.max_seq_len, self.n_local_kv_heads, self.head_dim, ) ) def forward( self, x: torch.Tensor, start_pos: int, # freqs_cis: torch.Tensor, mask: Optional[torch.Tensor], ): \u0026#34;\u0026#34;\u0026#34; Forward pass of the attention module. Args: x (torch.Tensor): Input tensor. start_pos (int): Starting position for caching. freqs_cis (torch.Tensor): Precomputed frequency tensor. mask (torch.Tensor, optional): Attention mask tensor. Returns: torch.Tensor: Output tensor after attention. \u0026#34;\u0026#34;\u0026#34; bsz, seqlen, _ = x.shape xq, xk, xv = self.wq(x), self.wk(x), self.wv(x) xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim) xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim) xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim) # xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis) self.cache_k = self.cache_k.to(xq) self.cache_v = self.cache_v.to(xq) self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv keys = self.cache_k[:bsz, : start_pos + seqlen] values = self.cache_v[:bsz, : start_pos + seqlen] # Grouped Query Attention # for production use repeat_kv function, it\u0026#39;s memory efficient ## Repeat the key, values heads, repeats values at dim =2 self.n_rep times, now keys and values size match query size. keys = torch.repeat_interleave(keys, dim=2, repeats=self.n_rep) # (bs, cache_len + seqlen, n_local_heads, head_dim) values= torch.repeat_interleave(values, dim=2, repeats=self.n_rep) # (bs, cache_len + seqlen, n_local_heads, head_dim) xq = xq.transpose(1, 2) # (bs, n_local_heads, seqlen, head_dim) keys = keys.transpose(1, 2) # (bs, n_local_heads, cache_len + seqlen, head_dim) values = values.transpose(1, 2) # (bs, n_local_heads, cache_len + seqlen, head_dim) print(xq.shape) print(keys.transpose(2,3).shape) scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim) if mask is not None: scores = scores + mask # (bs, n_local_heads, seqlen, cache_len + seqlen) scores = F.softmax(scores.float(), dim=-1).type_as(xq) output = torch.matmul(scores, values) # (bs, n_local_heads, seqlen, head_dim) output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1) return self.wo(output) Inferencing code This code generates embedding for next tokens by passing existing sequence of tokens to our attention layer (not FFNN) and append it to our existing sequence of tokens and pass it again to the attention layer and do it to generate it til 10 tokens.\nimport torch import torch.nn as nn import math # Define a simple ModelArgs class for testing # Initialize the Attention module args = ModelArgs() attn = Attention(args) # Input sentence and tokenization sentence = \u0026#39;this is awesome\u0026#39; tokens = sentence.split(\u0026#39; \u0026#39;) # Hyperparameters B = 1 # Batch size T = len(tokens) # Sequence length C = args.dim # Feature dimension # Initialize input tensor with random values (for demonstration) x = torch.randn(B, args.max_seq_len, C) # Shape: (batch_size, max_seq_len, feature_dim) # Inference loop start_pos = 0 for cur_pos in range(T, 10): # Generate tokens up to max_seq_len # Forward pass through the attention module out = attn(x[:, start_pos:cur_pos], start_pos, None) # Shape: (batch_size, cur_pos - start_pos, feature_dim) # Update the input tensor with the output for the next position x[:, cur_pos] = out[:, -1, :] # Take the last token\u0026#39;s output and append it to the sequence # Update start_pos for the next iteration start_pos = cur_pos # Final output print(\u0026#34;Final output tensor:\u0026#34;) print(x) let\u0026rsquo;s break down the whole code one by one\n# Initialize the Attention module args = ModelArgs() attn = Attention(args) # Input sentence and tokenization sentence = \u0026#39;this is a\u0026#39; tokens = sentence.split(\u0026#39; \u0026#39;) # Hyperparameters B = 1 # Batch size T = len(tokens) # Sequence length C = args.dim # Feature dimension # Initialize input tensor with random values (for demonstration) x = torch.randn(B, args.max_seq_len, C) # Shape: (batch_size, max_seq_len, feature_dim) this is straightforward, we have initial sentence that we pass it to the model i.e \u0026ldquo;this is awesome\u0026rdquo;, construct a random input matrix \u0026ldquo;x\u0026rdquo; of (batch_size, max_seq_len, feature_dim). In real, these random input matrix is the matrix full of input embeddings.\nstart_pos = 0 the is the starting point for caching, since we haven\u0026rsquo;t cached anything yet, we start from the initial position i.e 0 for token \u0026ldquo;this\u0026rdquo;\nfor cur_pos in range(T, 10): # Generate tokens up to max_seq_len # Forward pass through the attention module out = attn(x[:, start_pos:cur_pos], start_pos, None) # Shape: (batch_size, cur_pos - start_pos, feature_dim) # Update the input tensor with the output for the next position x[:, cur_pos] = out[:, -1, :] # Take the last token\u0026#39;s output and append it to the sequence # Update start_pos for the next iteration start_pos = cur_pos we iterate from T,10 because we already have tokens til T and we want to generate 10-T tokens.\nwe pass the sequence from start_pos:cur_pos to our attn architecture.\nfirst this will be x[:, 0:3] (which is the initial tokens \u0026ldquo;this is a\u0026rdquo;, because we first need to calculate the attention for these initial tokens and cache them first.\nlet\u0026rsquo;s directly come down to this part of code, because all other are usual code for attention without caching.\nself.cache_k[:bsz, start_pos : start_pos + seqlen] = xk self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv keys = self.cache_k[:bsz, : start_pos + seqlen] values = self.cache_v[:bsz, : start_pos + seqlen] as you can see we first cache xk to the positions [:bsz, start_pos : start_pos + seqlen]\nwhich is basically caching this index of our input [:1, 0:3] which is basically the initially tokens (\u0026rsquo;this is a')\nand then we pluck out the same tokens from our cached keys and cached values\nkeys = self.cache_k[:bsz, : start_pos + seqlen]\nthis plucking out will make sense in the next run.\nnow rest of the code in the attn class is executed and we get output same as the size of query i.e which is prediction for these positions [:bsz, start_pos : start_pos + seqlen]\nand then we pluck out the last one, because its want we need and add it in the end of our input x[:, cur_pos] = out[:, -1, :].\nnow lets say the predicted token was \u0026ldquo;good\u0026rdquo;, we now have sequence \u0026ldquo;this is a good\u0026rdquo; and start_pos=3, and now in the next iteration cur_pos=4, we pass this input i.e (x[:, 3:4], 3, None) to our attention\nas you can see this is simply the prediction from earlier iteration and this is what we pass, because we only need the embedding for this token \u0026ldquo;good\u0026rdquo;.\nand then we only this new token in the code\nself.cache_k[:bsz, start_pos : start_pos + seqlen] = xk self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv self.cache_k[:bsz, 3:3 + 1], as you can see we are just appending this new token to the preivous cache to be used in the later iteration.\nnow we pluck out this cache\nkeys = self.cache_k[:bsz, : start_pos + seqlen] values = self.cache_v[:bsz, : start_pos + seqlen] i.e keys = self.cache_k[:bsz, :3+1] which is the key and values cache til that token and simply calculate the attention scores and apply them, and this process goes on until the required sequence is generated (in our case 10)\nThis is all we need to know about KV cache.\nGrouped Query Attention Explanation As we know the main bottleneck while training and inferencing is not the amount of operations that our GPU can perform but rather the amount of data our GPU can move between tensor cores and the GPU memory. This is what GQA tries to solve, it tries to achieve balance between the accuracy of Multi-Head Attention (it performs better than these attention variants) and speed of attention calculation. The picture below accurately explains Multi-Head Attention (MHA), Multi-Query Attention (MQA) and Grouped Query Attention (GQA)\nThe main difference between them is:\nMHA : Q,K,V are divided into equal number of heads MQA: Only Q is divided into different heads, whereas K,V remain the same. However, the resulting number of attention heads are the same as MHA. (K,V are same, we don\u0026rsquo;t have to move data back and forth, this is where it helps in achieving performance gains.) GQA: Query is divided into total number of heads but mainly in groups, and K, V have different number of heads mainly referred to as kv_heads. As shown in the figure, similar group of query interact with their respective heads. Code for GQA keys = torch.repeat_interleave(keys, dim=2, repeats=self.n_rep) # (bs, cache_len + seqlen, n_local_heads, head_dim) values= torch.repeat_interleave(values, dim=2, repeats=self.n_rep) # The way this code works is just by duplicating the keys and values across the dim=2, self.n_rep number of times. This self.n_rep is obtained by dividing self.n_local_heads by self.n_local_kv_heads.\nFor instance, lets say our keys were (2,2,2,4) ( # (bs, cache_len + seqlen, n_local_kv_heads, head_dim))\nThe dimension across n_local_kv_heads will be repeated self.n_rep times,\na = torch.randn(2,2,2,4) # B,T,n_kv_head, head_dim a tensor([[[[ 0.6406, -1.2496, 0.9831, -0.3773], [ 1.0520, 0.5683, 0.6138, 0.0082]], [[-0.6792, 1.0518, 0.6339, 0.9386], [-0.0693, 0.8445, 1.8666, 1.6446]]], [[[-0.5852, -1.5809, -0.3186, 1.2536], [-0.9714, 0.4342, -1.0229, 0.1140]], [[-0.4645, 0.6589, -0.6345, 0.9500], [ 0.3443, -0.7342, -0.0163, 0.3242]]]]) torch.repeat_interleave(a, dim=2, repeats=2) tensor([[[[ 0.6406, -1.2496, 0.9831, -0.3773], [ 0.6406, -1.2496, 0.9831, -0.3773], [ 1.0520, 0.5683, 0.6138, 0.0082], [ 1.0520, 0.5683, 0.6138, 0.0082]], [[-0.6792, 1.0518, 0.6339, 0.9386], [-0.6792, 1.0518, 0.6339, 0.9386], [-0.0693, 0.8445, 1.8666, 1.6446], [-0.0693, 0.8445, 1.8666, 1.6446]]], [[[-0.5852, -1.5809, -0.3186, 1.2536], [-0.5852, -1.5809, -0.3186, 1.2536], [-0.9714, 0.4342, -1.0229, 0.1140], [-0.9714, 0.4342, -1.0229, 0.1140]], [[-0.4645, 0.6589, -0.6345, 0.9500], [-0.4645, 0.6589, -0.6345, 0.9500], [ 0.3443, -0.7342, -0.0163, 0.3242], [ 0.3443, -0.7342, -0.0163, 0.3242]]]]) you can see how the values are copied one by one 2 times for this scenario.\nBUT, this simply copies the numbers twice, there\u0026rsquo;s another way that\u0026rsquo;s used in Llama code\ndef repeat_kv(x: torch.Tensor, n_rep: int) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34;torch.repeat_interleave(x, dim=2, repeats=n_rep)\u0026#34;\u0026#34;\u0026#34; bs, slen, n_kv_heads, head_dim = x.shape if n_rep == 1: return x return ( x[:, :, :, None, :] .expand(bs, slen, n_kv_heads, n_rep, head_dim) .reshape(bs, slen, n_kv_heads * n_rep, head_dim) ) It performs the same operation as torch.repeat_interleave, but the in a more memory efficient way.\nx[:, :, :, None, :] adding None will add one extra dimension to our vector. its shape will be (2,2,2,1,4)\n.expand(bs, slen, n_kv_heads, n_rep, head_dim) will expand(repeat) the the singleton dimension i.e our dimension 3 to n_rep, it will repeat n_rep times but not by copying the same elements but by creating a new view for that dimension which points to the same old memory location, and then we reshape it to (bs, slen, n_kv_heads * n_rep, head_dim).\nBy not copying and simply creating a new view, it saves memory.\n","permalink":"https://cohlem.github.io/sub-notes/kv-cache-gqa/","summary":"KV Cache KV cache visual operation In the note blow, I first describe how inferencing is done if we simply do operation without KV cache and then describe how KV cache helps removing redundant operations.\nWe don\u0026rsquo;t make use of KV cache while training because we already have data filled for each sequence length, we don\u0026rsquo;t need to calculate loss one by one, instead we do it in batches, whereas while inferencing we do it generally for 1 batch with some sequences and then we keep on appending next-predicted token to that sequence one by one.","title":"KV cache and Grouped Query Attention"},{"content":"Recap of LayerNorm let\u0026rsquo;s first recap by understanding why LayerNorm was used:\nWe needed to balance the distribution of inputs (internal covariance shift) i.e we want inputs to be roughly gaussian (mean 0, std 1), it not maintained it would result in zeroing out the gradients. output of some blocks (transformer block) may produce large values or very small values that would result in either exploding or vanishing gradient problem, in order to have stable training, we needed to have stable range for those outputs. In this paper, the authors raise concern about LayerNorm.\nit\u0026rsquo;s computationally expensive and they claim that re-centering (calculating the x - mean) has little impact for stabilization. they\ncompletely get rid of mean statistic now there is less overhead because we don\u0026rsquo;t have to calculate one extra statistic. they also provide that RMSNorm is invariant (does not change) to inputs or weights matrices.\nwhich indicates that the change in scale of input of weights doesn\u0026rsquo;t affect the RMSNorm.\n","permalink":"https://cohlem.github.io/sub-notes/rmsnorm/","summary":"Recap of LayerNorm let\u0026rsquo;s first recap by understanding why LayerNorm was used:\nWe needed to balance the distribution of inputs (internal covariance shift) i.e we want inputs to be roughly gaussian (mean 0, std 1), it not maintained it would result in zeroing out the gradients. output of some blocks (transformer block) may produce large values or very small values that would result in either exploding or vanishing gradient problem, in order to have stable training, we needed to have stable range for those outputs.","title":"RMSNorm"},{"content":"Recap of Absolute PE We previously used absolute positional embedding in our GPT-2 model.\nDisadvantages No notion of relative information between tokens doesn\u0026rsquo;t work for sequences larger than context length the model is trained with, because we run out of token embeddings for tokens that come at sequence larger than the context length. RoPE pre-requisites This is how we rotate a point by an angel theta in a two dimensional space and this is all we need in RoPE.\nIn RoPE, the notion of providing a position to a token is by rotating it by some angle. For instance, if we have a token \u0026ldquo;this\u0026rdquo; with an embedding of 2 i.e [0.2782, 1.5109] and lets say it occurs in our sequence at first\n[this,is,awesome] The idea is to rotate token\u0026rsquo;s embedding according to the position it occurs in our sequence.\nFor instance, if this occurs in 2nd position it\u0026rsquo;s original embedding ([0.2782, 1.5109]) will be rotated by some angle m x theta, where m is the position and theta is some angle. You can see how we\u0026rsquo;re incorporating information about absolute position of tokens (m) while rotating it.\nbut the embedding is usually bigger than what we\u0026rsquo;ve assumed here, it\u0026rsquo;s basically 256,512,768 depending on what we choose. Let\u0026rsquo;s say we have embedding dimension of 4 for the sake of simplicity. The idea is to first divide embedding dimensions into pairs and apply rotation to each pairs.\nd_model = 4 sentence = \u0026#39;this is awesome\u0026#39; x = [i for i in range(len(sentence.split(\u0026#39; \u0026#39;)))] Q = torch.randn(len(sequence), d_model) Q output\n[[ 0.2782, 1.5109, 0.1739, -0.7098], [ 0.3792, -0.1098, 0.3707, -0.4049], [ 0.1652, 0.5787, 0.4085, -0.7005]] so embedding for our token \u0026ldquo;this\u0026rdquo; corresponds to\n[ 0.2782, 1.5109, 0.1739, -0.7098] it is divided into two i.e i values i.e i = 0 will correspond to [ 0.2782, 1.5109] and i=1 will correspond to[0.1739, -0.7098] so, the i values ranges from 0 to d_model/2 - 1, where d_model is the embedding dimension and and we can apply rotation to each of these separately and combine them together into one single embedding.\nthe angle by which we rotate a pair depends on m and theta, as m is simply the position and theta is generally this\n10000 ** (-2 * i / d_model) #where i corresponds to each pair as described above. let\u0026rsquo;s see how embedding of our token \u0026ldquo;this\u0026rdquo; is rotated if it occurs at different positions (i.e different m) in this case i\u0026rsquo;m assuming i=0 (only considering the first pair)\nWe construct our rotation matrix like this\ndef rotation_matrix(m, i, d_model): \u0026#34;\u0026#34;\u0026#34; Compute the 2x2 rotation matrix for RoPE. Args: m (int or torch.Tensor): Position (token index). i (int or torch.Tensor): Dimension index. d_model (int): Embedding dimension. Returns: torch.Tensor: 2x2 rotation matrix. \u0026#34;\u0026#34;\u0026#34; # Compute theta_i thetai = 10000 ** (-2 * i / d_model) print(thetai) # Ensure m is a tensor m = torch.tensor(m, dtype=torch.float32) # Compute cos(m * theta_i) and sin(m * theta_i) angle = m * thetai cos = torch.cos(angle) sin = torch.sin(angle) # Construct the 2x2 rotation matrix rotation_matrix = torch.tensor([ [cos, -sin], [sin, cos] ]) return rotation_matrix and apply this rotation for different m values(different positions) i.e 1,2,3,4,5\n# Input vector inp = Q[0,:2].detach().numpy() # [ 0.2782, 1.5109] # Create a figure and axis plt.figure() # Draw the coordinate axes plt.axhline(0, color=\u0026#34;black\u0026#34;, linewidth=1) # X-axis plt.axvline(0, color=\u0026#34;black\u0026#34;, linewidth=1) # Y-axis # Set axis limits plt.xlim(-2, 2) plt.ylim(-2, 2) # Plot the original input vector plt.plot([0, inp[0]], [0, inp[1]], color=\u0026#39;blue\u0026#39;, label=\u0026#39;Original\u0026#39;, marker=\u0026#39;o\u0026#39;) # Apply rotation and plot the rotated vectors for m in range(1, 5): # Compute the rotation matrix rot_mat = rotation_matrix(m, 0, d_model) # Apply the rotation to the input vector ans = rot_mat @ inp # Plot the rotated vector print(ans) plt.plot([0, ans[0]], [0, ans[1]], label=f\u0026#39;Rotated (m={m})\u0026#39;, marker=\u0026#39;o\u0026#39;) # Add labels and title plt.xlabel(\u0026#34;X-axis\u0026#34;) plt.ylabel(\u0026#34;Y-axis\u0026#34;) plt.title(\u0026#34;RoPE Rotation Visualization\u0026#34;) # Add a legend plt.legend() # Add a grid plt.grid(True, linestyle=\u0026#34;--\u0026#34;, alpha=0.7) # Display the plot plt.show() you can see how these are rotated by some big angles when m is changed.\nbut this isn\u0026rsquo;t the case for later embedding pairs (i.e i = 300, when d_model = 768).\nThe more we increase our i values the less angle it will rotate with. Lets see one example I\u0026rsquo;m keeping the value of m as a constant i.e 1 here and plotting for different i values for sample input embedding.\ninp = Q[0,:2].detach().numpy() # Create a figure and axis plt.figure() # Draw the coordinate axes plt.axhline(0, color=\u0026#34;black\u0026#34;, linewidth=1) # X-axis plt.axvline(0, color=\u0026#34;black\u0026#34;, linewidth=1) # Y-axis # Set axis limits plt.xlim(-2, 2) plt.ylim(-2, 2) # Plot the original input vector plt.plot([0, inp[0]], [0, inp[1]], color=\u0026#39;blue\u0026#39;, label=\u0026#39;Original\u0026#39;, marker=\u0026#39;o\u0026#39;) #Apply rotation and plot the rotated vectors for i in range(1, 20): # Compute the rotation matrix rot_mat = rotation_matrix(1,i , 40) # Apply the rotation to the input vector ans = rot_mat @ inp # Plot the rotated vector print(ans) plt.plot([0, ans[0]], [0, ans[1]], label=f\u0026#39;Rotated i={i} (m={0})\u0026#39;, marker=\u0026#39;o\u0026#39;) # Add labels and title plt.xlabel(\u0026#34;X-axis\u0026#34;) plt.ylabel(\u0026#34;Y-axis\u0026#34;) plt.title(\u0026#34;RoPE Rotation Visualization\u0026#34;) # Add a legend plt.legend() # Add a grid plt.grid(True, linestyle=\u0026#34;--\u0026#34;, alpha=0.7) # Display the plot plt.show() as you can see the angle between them becomes smaller and smaller when I is increased.\nNOTE: In the figure above our original plot (dark blue) lies in the same pos as the plot covered by i=19.\nI just wanted to show how angles change depending on m and i values.\nNow that we have basic intuition, lets implement it.\nrotating a embedding of dimension 2 is simple, but how do we do it for more than 2 dimension in just one operation?\nhere\u0026rsquo;s how. we construct matrix like this where the diagonal is rotation matrix (same matrix that we used above) and all the other elements are sparse i.e 0 and then multiply this big rotation matrix with our embedding of size d.\nBut the authors of this paper have derived a simple formula for doing the same operation but without the big matrix, because it would be computationally expensive where most of the elements are sparse.\nhere\u0026rsquo;s the simplified formula. lets apply this simplified version to our token \u0026ldquo;this\u0026rdquo; where it\u0026rsquo;s embedding is [ 0.2782, 1.5109, 0.1739, -0.7098]\nNOTE: this code below only applies the rotation to token \u0026ldquo;this\u0026rdquo; only for simplicity. m=0 because it occurs at position 0.\nIf you take a closer look the code below, it matches the formula shown above.\nm = 0 cos_dim = [math.cos(m*(10000**(-2*i/d_model))) for i in range(d_model//2) for _ in range(2)] sin_dim = [math.sin(m*(10000**(-2*i/d_model))) for i in range(d_model//2) for _ in range(2)] QT = torch.empty_like(Q[0]) QT[1::2] = Q[0][::2] QT[::2] = -Q[0][1::2] simplified_ans = Q[0]*torch.tensor(cos_dim) + QT*torch.tensor(sin_dim) # using m This code below applies the rotation to all 3 tokens in our sequence for all i values.\nthetai = 1.0 / (10000 ** (2*torch.arange(0, d_model//2).float() / d_model)) m = torch.arange(Q.shape[0], dtype=torch.float) sinusoid = m.unsqueeze(1) * thetai.unsqueeze(0) # Outer product freq = sinusoid.repeat_interleave(2,dim=1) cos= torch.cos(freq) sin = torch.sin(freq) Q_new = torch.empty_like(Q) Q_new[:,::2] = -Q[:,1::2] Q_new[:,1::2] = Q[:,::2] ans = Q*cos + Q_new*sin all these code were for matrix rotation. But there\u0026rsquo;s more to it.\nllama models don\u0026rsquo;t use these type of operation, because there seems to be too much tensor manipulation is a bit computationally expensive. There seems to be an elegant way of doing 2D rotation, and that is by using complex numbers.\nBefore going into the implementation, please read these paper notes.\nComplex plane is simply representing real numbers along the x-axis and imaginary numbers along y (i.e imaginary plane is perpendicular to the real plane)\nafter completely understanding the notes, now we you may have the notion that we can represent our embedding in complex numbers and then multiply by cos(theta) +i.sin(theta) to rotate it by theta.\nwhat we do now in the code below is represent our input in complex form, and represent the rotation angles in polar form and then multiply together to get the output.\ndef precompute_freq_cis(dim, end, theta = 10000): thetai = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)) m = torch.arange(end, device=thetai.device) mthetai = torch.outer(m,thetai) # m x thetai return torch.polar(torch.ones_like(mthetai),mthetai) # computes r * e^mthetai def apply_rotary_pe(xq, xk, cis): def unite_shape(pos_cis, x): ndim = x.ndim assert 0 \u0026lt;= 1 \u0026lt; ndim assert pos_cis.shape == (x.shape[1], x.shape[-1]) shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)] return pos_cis.view(*shape) xq_ = torch.view_as_complex(xq.view(*xq.shape[:-1],-1,2)) # converting to complex form xk_ = torch.view_as_complex(xk.view(*xq.shape[:-1],-1,2)) # converting to complex form cis = unite_shape(cis,xq_) xq_out = torch.view_as_real(xq_ * cis).flatten(3) xk_out = torch.view_as_real(xk_ * cis).flatten(3) return xq_out.type_as(xq), xk_out.type_as(xk) freq_cis = precompute_freq_cis(4,3) xq = torch.randn(5,3,2,4) xk = torch.randn(5,3,2,4) xq_out, xk_out = apply_rotary_pe(xq,xk,freq_cis) This is how it\u0026rsquo;s originally implemented in llama models as you can see here\nhttps://github.com/meta-llama/llama/blob/8fac8befd776bc03242fe7bc2236cdb41b6c609c/llama/model.py#L132\nReferences https://arxiv.org/pdf/2104.09864 RoPE Complex Numbers Fundamentals llama code implementation ","permalink":"https://cohlem.github.io/sub-notes/rope/","summary":"Recap of Absolute PE We previously used absolute positional embedding in our GPT-2 model.\nDisadvantages No notion of relative information between tokens doesn\u0026rsquo;t work for sequences larger than context length the model is trained with, because we run out of token embeddings for tokens that come at sequence larger than the context length. RoPE pre-requisites This is how we rotate a point by an angel theta in a two dimensional space and this is all we need in RoPE.","title":"RoPE"},{"content":"Watch list Mirror (1975) Saltburn Wind River Hell or High Water Recently Watched (my rating /10) Gladiator 2 (6/10) Braveheart () Personal Favorites (unranked) Stalker (1979) Parasite Mother (dir. Bong Joon ho) Memories of murder Dune 1 and 2 Snowpiercer Gravity Arrival Incendis Prisoners \u0026hellip;.. I\u0026rsquo;ll add some when I remember, by now I\u0026rsquo;ve almost forgotten half of the movies I\u0026rsquo;ve watched over the past years. Series (ranked) I don\u0026rsquo;t follow series a lot but these are some that I\u0026rsquo;ve enjoyed watching.\nBreaking Bad Game of Thrones True Detective House of the Dragon ","permalink":"https://cohlem.github.io/random/movies/","summary":"Watch list Mirror (1975) Saltburn Wind River Hell or High Water Recently Watched (my rating /10) Gladiator 2 (6/10) Braveheart () Personal Favorites (unranked) Stalker (1979) Parasite Mother (dir. Bong Joon ho) Memories of murder Dune 1 and 2 Snowpiercer Gravity Arrival Incendis Prisoners \u0026hellip;.. I\u0026rsquo;ll add some when I remember, by now I\u0026rsquo;ve almost forgotten half of the movies I\u0026rsquo;ve watched over the past years. Series (ranked) I don\u0026rsquo;t follow series a lot but these are some that I\u0026rsquo;ve enjoyed watching.","title":"Movies"},{"content":"GPU physcial structure let\u0026rsquo;s first understand the structure of GPU.\nInside a GPU it has a chip named GA102 (depends on architecture, this is for ampere architecture) built from 28.3million transistors (semiconductor device that can¬†switch¬†or¬†amplify¬†electrical signals) and majority area covered by processing cores. processing core is divide into seven Graphics processing clusters (GPCs)\namong each GPC there are 12 Streaming Multiprocessors. Inside each SM there are 4 warps and 1 Raytracing core inside a warp there are 32 Cudas and 1 Tensor Core.\nAltogether there are\n10752 CUDA Cores 336 Tensor Cores 84 Ray Tracing Cores Each cores have different function.\nCuda Cores cuda core is like a basic calculator with multiplication and addition operations.\nMostly used for processing video frames. perform operations like A x B + C called fused multiply and add (FMA) half of the cuda cores perform FMA on 32-bit floating point numbers other half perform 32-bit integer FMA. other sections perform bit shifting and bit masking as well as collecting and queuing incoming operands, and accumulating output results. So it can be though of like a calculator. performs one 1 multiply and 1 add operation per 1 cycle so altogether 2 operations x 10752 cuda cores x 1.7 GhZ clock speed(10^9 cycles per second) = 35.6 trillion calculations per second. Ray tracing Cores used for executing ray tracing algorithms Depending on the amount of streaming multiprocessors that are damaged during manufacturing they are categoriezed and sold at different prices, for instance RTX 3090 ti has full 10752 cuda where as 3090 might have some damaged SMs. These cards might have different clock speed too.\nGraphics Memory GDDR6X SDRAM these 24GBs of GDDR6X surround the GPU chip In order to run the operations of GPU chip\nThey must be loaded from SSD to these graphics memory. They have certain bandwidth i.e the amount of data they can transfer per second to the GPU chip. They have 1.15 Terbytes/sec bandwidth. Similar to HBM memory in AI chips, would look something like this How are operations executed? Each instruction is executed by a thread (which is matched to a single cuda core) and these threads are combined into 32 cores called warp, 4 warps are combined to form thread blocks that are operated by Streaming Multiprocessor (SM). Similarly SM is combined together. All these are operated by Gigathread Engine If processor runs at 1Ghz, it can run 10^9 cycles per second, assuming 1cycle = 1 basic operation it can execute 10^9 operations.\nits basically like a unit of time, 10^9 cycles per 1 second means, one cycle takes 10^-9 seconds to run. Different operations take varying amount of cycles (i.e latency of the operation) to perform. - Global memory access (up to 80GB): ~380 cycles - L2 cache: ~200 cycles - L1 cache or Shared memory access (up to 128 kb per Streaming Multiprocessor): ~34 cycles - Fused multiplication and addition, a*b+c (FFMA): 4 cycles - Tensor Core matrix multiply: 1 cycle Tensor Cores (Most important) These cores are used for matrix multiplication and matrix additions.\npredominantly used in deep learning calculation. First lets start by understanding the precision formats that\u0026rsquo;s used.\n1. FP16 (Half-Precision Floating-Point) Full Name: Half-precision floating-point.\nSize: 16 bits (2 bytes).\nBit Allocation:\n1 bit for the sign.\n5 bits for the exponent.\n10 bits for the mantissa.\nPrecision: About 3 decimal digits, 2^10 = 1024, taking log10(1024) = 3 decimal digits.\n2. FP32 (Single-Precision Floating-Point) Full Name: Single-precision floating-point.\nSize: 32 bits (4 bytes).\nBit Allocation:\n1 bit for the sign.\n8 bits for the exponent.\n23 bits for the mantissa.\nPrecision: About 7 decimal digits.\nThe operation performed by Tensor Core is something like this.\n(picture) It performs 64 FMA operations per clock.\nUnderstanding matrix multiplication in tensor cores. Task:\nMatrix multiply A and B with each size 32 x 32\nlets say our tensor cores process 4x4 matrix multiplications per 1 cycle. The step to multiply A and B are\ndivide A and B into tiles of 4x4 matrices i.e 64 4x4 tile for each matrix load the tiles into shared memory (164KB) from global memory takes about 200 cycles. load the tiles into tensor core registers for operation from shared memory, take about 34 cycles. perform matrix multiplication on eight tensor cores all in parallel, take 1 cycle and then accumulate the values. total cycle 200+34 + 1 = 235 cycles. Memory Bandwidth we have seen that Tensor Cores are very fast. So fast, in fact, that they are idle most of the time as they are waiting for memory to arrive from global memory. For example, during GPT-3-sized training, which uses huge matrices ‚Äî the larger, the better for Tensor Cores ‚Äî we have a Tensor Core TFLOPS utilization of about 45-65%, meaning that even for the large neural networks about 50% of the time, Tensor Cores are idle.\nNvidia Ampere Architecture 108 streaming multiprocessors (SMs) 432 Tensor Cores 6,912 FP32 Cuda Cores 3,456 FP64 cuda cores (they share the same physical hardware, different in a sense that each require different clock speed and precision ) More details in the picture below.\nThese TFLOPS are calculated using this formula\nNumber of cores for that precision (FP64) x clock speed x Operations per clock (generally 1FMA or 2 Operations)\nTensor Float 32 (TF32):\n156 TFLOPS | 312 TFLOPS*: TF32 is a mixed-precision format optimized for AI workloads. It provides a balance between FP16 and FP32 precision. BFLOAT16:\n312 TFLOPS | 624 TFLOPS*: BFLOAT16 is a 16-bit floating-point format used in deep learning, offering a good trade-off between range and precision. FP16 Tensor Core:\n312 TFLOPS | 624 TFLOPS*: FP16 is used for deep learning training and inference, providing high throughput for lower-precision computations. INT8 Tensor Core:\n624 TOPS | 1248 TOPS*: INT8 is used for inference tasks, where lower precision is acceptable, and high throughput is critical. This figure shows the range and precision of each of these.\nPreferred:\nTF32 for training and mostly FP16 and BF16 for inference. How numbers are stored (example of fp16) lets store -8764.781267¬†in FP16 format\nfirst convert int and frac part to binary, we get\n(8764.781267)base10‚âà(10001000111100.110012)base2\nNormalize the binary number to the form¬†1.mantissa x 2^(exponent)\n10001000111100.11001‚Äã=1.000100011110011001‚Äã√ó2^13\nThe exponent is biased by¬†15¬†in FP16: i.e Exponent¬†Bits=Actual¬†Exponent+15=13+15=28\n(28)base10‚Äã=(11100)base2\nThe mantissa is the fractional part of the normalized number, truncated to¬†10 bits:\n(1.000100011110011001)base2‚Üí(00010001111.000100011110011001)base2‚Äã‚Üí(0001000111)base2\nThe number is negative, so the sign bit is:\nSign¬†Bit=1Sign¬†Bit=1\nCombine the sign bit, exponent bits, and mantissa bits:\nSign Bit:¬†1\nExponent Bits:¬†11100\nMantissa Bits:¬†0001000111\nThe FP16 representation is:\n11110000010001111111000001000111\n‚Äã\nTensor cores are optimized for matrix multiplications so it can peform more operations per clock rather than just 64 FMA per clock.\nSparsity Matrix contains large number of zeros in it, by using a fine-grained pruning algorithm to compress (essentially removing) small and zero-value matrices, the GPU saves computing resources, power, memory and bandwidth.\nForm Factor They define how the GPU is physically integrated into a system and how it connects to other components like the CPU and memory.\nPCIe¬†is a standard interface used to connect GPUs, SSDs, network cards, and other peripherals to a computer‚Äôs motherboard SXM are not standalone cards and is used in data centers. Bandwidth PCIe Gen4: 64 GB/s (x16) NVLink: 600 GB/s (per GPU pair) References How do Graphics Cards Work? Exploring GPU Architecture NVIDIA A100 TENSOR CORE GPU ","permalink":"https://cohlem.github.io/sub-notes/gpus/","summary":"GPU physcial structure let\u0026rsquo;s first understand the structure of GPU.\nInside a GPU it has a chip named GA102 (depends on architecture, this is for ampere architecture) built from 28.3million transistors (semiconductor device that can¬†switch¬†or¬†amplify¬†electrical signals) and majority area covered by processing cores. processing core is divide into seven Graphics processing clusters (GPCs)\namong each GPC there are 12 Streaming Multiprocessors. Inside each SM there are 4 warps and 1 Raytracing core inside a warp there are 32 Cudas and 1 Tensor Core.","title":"GPUs"},{"content":"When we have enough resources we would want to train our neural networks in parallel, the way to do this is to train our NN with different data (different batches of data) in each GPU in parallel. For instance, if we have 8X A100 we run 8 different batches of data on each A100 GPU.\nThe way to do this in pytorch is to use DDP (take a look into their docs)\nThe important thing to be careful about is that when we train our NN in different GPUs, each GPU calculates gradient and that gradient is averaged among all the all the gradients calculated from each GPUs and then deposited on each of the gpu and then we do the descent step.\nWhy do we do this ?\nSo that our weights become consistent and mathematically equivalent to when we train on 8x the batch size but on same GPU.\nAt first I was confused how would this be equivalent to the gradients when we trained it on single GPU but with 8x the batch size.\nHere\u0026rsquo;s a simple mathematical formula.\nFIGURE1 But lets look at our manual backprogpagation which will help us understand better.\nForward pass n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 64 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) # Layer 1 W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) b1 = torch.randn(n_hidden, generator=g) * 0.1 # using b1 just for fun, it\u0026#39;s useless because of BN # Layer 2 W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1 b2 = torch.randn(vocab_size, generator=g) * 0.1 # BatchNorm parameters bngain = torch.randn((1, n_hidden))*0.1 + 1.0 bnbias = torch.randn((1, n_hidden))*0.1 # Note: I am initializating many of these parameters in non-standard ways # because sometimes initializating with e.g. all zeros could mask an incorrect # implementation of the backward pass. parameters = [C, W1, b1, W2, b2, bngain, bnbias] print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters: p.requires_grad = True batch_size = 32 n = batch_size # a shorter variable also, for convenience # construct a minibatch ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y # forward pass, \u0026#34;chunkated\u0026#34; into smaller steps that are possible to backward one at a time emb = C[Xb] # embed the characters into vectors embcat = emb.view(emb.shape[0], -1) # concatenate the vectors # Linear layer 1 hprebn = embcat @ W1 + b1 # hidden layer pre-activation # BatchNorm layer bnmeani = 1/n*hprebn.sum(0, keepdim=True) bndiff = hprebn - bnmeani bndiff2 = bndiff**2 bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel\u0026#39;s correction (dividing by n-1, not n) bnvar_inv = (bnvar + 1e-5)**-0.5 bnraw = bndiff * bnvar_inv hpreact = bngain * bnraw + bnbias # Non-linearity h = torch.tanh(hpreact) # hidden layer # Linear layer 2 logits = h @ W2 + b2 # output layer # cross entropy loss (same as F.cross_entropy(logits, Yb)) logit_maxes = logits.max(1, keepdim=True).values norm_logits = logits - logit_maxes # subtract max for numerical stability counts = norm_logits.exp() counts_sum = counts.sum(1, keepdims=True) counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can\u0026#39;t get backprop to be bit exact... probs = counts * counts_sum_inv logprobs = probs.log() loss = -logprobs[range(n), Yb].mean() # PyTorch backward pass for p in parameters: p.grad = None for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way norm_logits, logit_maxes, logits, h, hpreact, bnraw, bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani, embcat, emb]: t.retain_grad() loss.backward() loss Backprop # Exercise 1: backprop through the whole thing manually, # backpropagating through exactly all of the variables # as they are defined in the forward pass above, one by one # ----------------- # YOUR CODE HERE :) dlogprobs = torch.zeros_like(logprobs) dlogprobs[range(n), Yb] = -1.0*(1/logprobs.shape[0]) # 1 \u0026lt;=== look here dprobs = (1/probs)*dlogprobs # 2 dcounts_sum_inv = (dprobs*counts).sum(1, keepdim = True) dcounts = dprobs * counts_sum_inv dcounts_sum = -1.0*((counts_sum)**(-2.0))*dcounts_sum_inv dcounts += torch.ones_like(counts_sum)*dcounts_sum dnorm_logits = norm_logits.exp()*dcounts dlogit_maxes = (-1.0*dnorm_logits).sum(1,keepdim=True) dlogits = (1.0*dnorm_logits) dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1])*dlogit_maxes db2 = (dlogits*torch.ones_like(logits)).sum(0) dh = dlogits @ W2.T dW2 = h.T @ dlogits dhpreact = dh*(1-h**(2)) dbnbias = (dhpreact*torch.ones_like(bnraw)).sum(0, keepdim= True) dbngain = (dhpreact*bnraw*torch.ones_like(bnraw)).sum(0, keepdim=True) dbnraw = dhpreact*bngain*torch.ones_like(bnraw) dbnvar_inv = (dbnraw* (torch.ones_like(bndiff) * bndiff)).sum(0, keepdim=True) dbndiff = (dbnraw* (torch.ones_like(bndiff) * bnvar_inv)) dbnvar = dbnvar_inv* (-0.5)*(((bnvar + 1e-5))**(-1.5)) dbndiff2 = (1.0/(n-1) )*torch.ones_like(bndiff2) * dbnvar dbndiff += dbndiff2*2*(bndiff) dhprebn = dbndiff*1.0 dbnmeani = (torch.ones_like(hprebn)*-1.0*dbndiff).sum(0, keepdim = True) dhprebn += torch.ones_like(hprebn)*(1/n)*dbnmeani db1 = (torch.ones_like(dhprebn)*dhprebn).sum(0) dembcat = dhprebn @ W1.T dW1 = embcat.T @ dhprebn demb = dembcat.view(emb.shape[0],emb.shape[1],emb.shape[2]) dC = torch.zeros_like(C) for i in range(Xb.shape[0]): for j in range(Xb.shape[1]): dC[Xb[i,j]] += demb[i,j] # print(demb[i,j].shape) # ----------------- cmp(\u0026#39;logprobs\u0026#39;, dlogprobs, logprobs) cmp(\u0026#39;probs\u0026#39;, dprobs, probs) cmp(\u0026#39;counts_sum_inv\u0026#39;, dcounts_sum_inv, counts_sum_inv) cmp(\u0026#39;counts_sum\u0026#39;, dcounts_sum, counts_sum) cmp(\u0026#39;counts\u0026#39;, dcounts, counts) cmp(\u0026#39;norm_logits\u0026#39;, dnorm_logits, norm_logits) cmp(\u0026#39;logit_maxes\u0026#39;, dlogit_maxes, logit_maxes) cmp(\u0026#39;logits\u0026#39;, dlogits, logits) cmp(\u0026#39;h\u0026#39;, dh, h) cmp(\u0026#39;W2\u0026#39;, dW2, W2) cmp(\u0026#39;b2\u0026#39;, db2, b2) cmp(\u0026#39;hpreact\u0026#39;, dhpreact, hpreact) cmp(\u0026#39;bngain\u0026#39;, dbngain, bngain) cmp(\u0026#39;bnbias\u0026#39;, dbnbias, bnbias) cmp(\u0026#39;bnraw\u0026#39;, dbnraw, bnraw) cmp(\u0026#39;bnvar_inv\u0026#39;, dbnvar_inv, bnvar_inv) cmp(\u0026#39;bnvar\u0026#39;, dbnvar, bnvar) cmp(\u0026#39;bndiff2\u0026#39;, dbndiff2, bndiff2) cmp(\u0026#39;bndiff\u0026#39;, dbndiff, bndiff) cmp(\u0026#39;bnmeani\u0026#39;, dbnmeani, bnmeani) cmp(\u0026#39;hprebn\u0026#39;, dhprebn, hprebn) cmp(\u0026#39;embcat\u0026#39;, dembcat, embcat) cmp(\u0026#39;W1\u0026#39;, dW1, W1) cmp(\u0026#39;b1\u0026#39;, db1, b1) cmp(\u0026#39;emb\u0026#39;, demb, emb) cmp(\u0026#39;C\u0026#39;, dC, C) lets look at our last line of code in forward pass\nloss = -logprobs[range(n), Yb].mean() what we do is simply the cross entropy i.e normalize using softmax function and pluck out the log probability from the output token\u0026rsquo;s index.\nand calculate it\u0026rsquo;s derivative here in second line of code in the backward pass like this.\ndlogprobs[range(n), Yb] = -1.0*(1/logprobs.shape[0]) # 1 \u0026lt;=== look here what we\u0026rsquo;re doing is simply calculating the average over all the batches and that average is deposited in each element in [range(n), Yb] .\nWhy do we do the average? dL/dlogprobs = 1 (which comes from dL/dL) x d(dlogprobs[range(n), Yb])/dlogprobs\nsince the elements in this range [range(n), Yb]) is averaged, each element will get (1/total_number) x itself\nso the local derivative of (1/total_number) x iteself w.r.t itself is 1/total_number which will be now deposited into the elements of dlogprobs[range(n), Yb]\nlet\u0026rsquo;s stop right here and this matches our first equation in FIGURE1, and if we do this operation for 8 times more batches on single GPU we get the second equation in FIGURE1. If we look at this code, we can see that we are only increasing the n value here.\ndlogprobs[range(8*n), Yb] = -1.0*(1/logprobs.shape[0]) # 1 \u0026lt;=== look here So from this it\u0026rsquo;s safe to say that training on multiple GPUs and averaging their gradients is same as training on single GPU with 8 times more batches.\n","permalink":"https://cohlem.github.io/sub-notes/ddp/","summary":"When we have enough resources we would want to train our neural networks in parallel, the way to do this is to train our NN with different data (different batches of data) in each GPU in parallel. For instance, if we have 8X A100 we run 8 different batches of data on each A100 GPU.\nThe way to do this in pytorch is to use DDP (take a look into their docs)","title":"DDP and gradient sync"},{"content":"Training Neural Networks Karpathy\u0026rsquo;s advice while training NN\nDeep Learning Concepts Contains simple explanation for DL concepts\nHow to scale your LLM (Must read) https://jax-ml.github.io/scaling-book/\n","permalink":"https://cohlem.github.io/notes/essential-blogs/","summary":"Training Neural Networks Karpathy\u0026rsquo;s advice while training NN\nDeep Learning Concepts Contains simple explanation for DL concepts\nHow to scale your LLM (Must read) https://jax-ml.github.io/scaling-book/","title":"Essential blogs"},{"content":"Gradient Accumulation When we want to train a neural network with some predefined set of tokens, but don\u0026rsquo;t have enough GPU resources, what do we do?\nGradient Accumulation We simply accumulate the gradients. For instance, in order to reproduce GPT-2 124B, we need to train the model with 0.5 Million tokens in a single run with 1024 context length, we would need 0.5e6/ 1024 = 488 batches i.e B,T = (488,1024) to calculate the gradients and update them.\nWe don\u0026rsquo;t have the enough resources, to fit those batches in our GPU, now what we do is divide that 488 into multiple batches and then do the forward pass and calculate gradients and accumulate (+) the gradients but we don\u0026rsquo;t update the gradients until we reach the end of the the desired batch size. After that, we update the parameters once.\n","permalink":"https://cohlem.github.io/sub-notes/gradient-accumulation/","summary":"Gradient Accumulation When we want to train a neural network with some predefined set of tokens, but don\u0026rsquo;t have enough GPU resources, what do we do?\nGradient Accumulation We simply accumulate the gradients. For instance, in order to reproduce GPT-2 124B, we need to train the model with 0.5 Million tokens in a single run with 1024 context length, we would need 0.5e6/ 1024 = 488 batches i.e B,T = (488,1024) to calculate the gradients and update them.","title":"Gradient Accumulation"},{"content":"Precision The more the precision point the less operation (TFLOPS) is performed.\nFP64 used for scientific research purposes, where precision is a must. TF32 and BFLOAT16 are mostly used in NN training. INT8 is used for inference. Picture below shows specifications of A100 GPU.\nUsing these precision points may have some difference in code. See pytorch\u0026rsquo;s docs\ntorch.compile It works in a similar fashion like the GCC compiler. It works by reducing overheads introduced by the python interpreter and optimizing the GPU read and writes.\nFor instance\ndef gelu(x): \u0026#34;\u0026#34;\u0026#34; Applies the GELU activation function to the input. \u0026#34;\u0026#34;\u0026#34; return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) First this operation resides in GPU\u0026rsquo;s HBM memory, and this part of calculation \u0026ldquo;torch.pow(x, 3)\u0026rdquo; is passed to GPU and it performs the operations, one by one, the instructions are sent from HBM to GPU cores and transferred back to HBM one by one. But torch.compiles evaluates that the code is simply operation on input x and some +,* and transfers the code to GPU once and does all the operation and send it back to HBM, in this way it optimizes the training process.\nFlash attention It is somewhat similar to torch.compile\u0026rsquo;s process but torch.compile itself cannot comprehend our code(shown below) to perform the optimization.\naw = (Q @ torch.transpose(K, -2,-1)) # for matmul dim of q should be B,T,C and k should be B,C,T aw = aw/(K.shape[-1] **0.5) mask = self.tril[:,:,:T,:T] == 0 # generate mask aw = aw.masked_fill_(mask, float(\u0026#39;-inf\u0026#39;)) # apply mask i.e fill true values with -inf aw = torch.softmax(aw,dim=-1) # -inf values are converted to 0 and then each row is normalized cv = aw @ V # context vector We have to call torch.nn.functional.scaled_dot_product_attention combined with torch.compile to use FlashAttention in our code.\nRemove ugly numbers. Always include numbers in our code that have powers of 2 in it.\nfor instance 16,32,64 work best.\nImprovements\nfor instance, while training GPT-2 our vocab_size is 50257\nif we factorize it has divisors\n1 | 29 | 1733 50257 None of it have powers of 2, so the GPU performs operation on that matrix by truncating til the last powers of 2 and then doing the operation on the remaining parts, which is inefficient. We can simply increase that number to be a closed number that has powers of 2 such as 50304 = 2^7 √ó 3 x 131 which has high number of power of 2.\nWe can simply increase the training speed by making our numbers in our code have more powers of 2.\n","permalink":"https://cohlem.github.io/sub-notes/training-speed-optimization/","summary":"Precision The more the precision point the less operation (TFLOPS) is performed.\nFP64 used for scientific research purposes, where precision is a must. TF32 and BFLOAT16 are mostly used in NN training. INT8 is used for inference. Picture below shows specifications of A100 GPU.\nUsing these precision points may have some difference in code. See pytorch\u0026rsquo;s docs\ntorch.compile It works in a similar fashion like the GCC compiler. It works by reducing overheads introduced by the python interpreter and optimizing the GPU read and writes.","title":"Training Speed Optimization"},{"content":"Skip connections are simply skipping the layers by adding the identity of input to it\u0026rsquo;s output as shown in the figure below.\nWhy add the identity of input x to the output ? We calculate the gradients of parameters using chain rule, as shown in figure above. For deeper layers the gradient start to become close to 0 and the gradient stops propagating, which is a vanishing gradient problem in a deep neural networks.\nWhen we add the identity of input to it\u0026rsquo;s output like this hl+1‚Äã=F(hl‚Äã)+hl‚Äã\nand when we backpropagate, we get this type of equation. So even when d(F(h1))/dh1 becomes close to 0 the previous gradient dL/dhl+1 is propagated as it is multiplied with 1 which is the outcome of adding the identity of input to its output.\nEventually, skip connections stop vanishing gradient problem, and the other thing they help with is that, when going to a residual block(could be attention block or feedforward block) neural network may loose previous information when going through that transformation, so adding identity of input to its output will take into consideration the new learned features + the previous features.\n","permalink":"https://cohlem.github.io/sub-notes/skip-connections/","summary":"Skip connections are simply skipping the layers by adding the identity of input to it\u0026rsquo;s output as shown in the figure below.\nWhy add the identity of input x to the output ? We calculate the gradients of parameters using chain rule, as shown in figure above. For deeper layers the gradient start to become close to 0 and the gradient stops propagating, which is a vanishing gradient problem in a deep neural networks.","title":"skip-connections"},{"content":"The simplest algorithm is the gradient descent in which we simply calculate loss over all the training data and then update our parameters, but it would be too slow and would consume too much resources. A faster approach is to use SGD where we calculate loss over every single training data and then do the parameter update, but the gradient update could be fuzzy. A more robust approach is to do mini batch SGD.\nThere are different types of optimizers with distinct capabilities.\nSGD with momentum We have one addition in this optimizer i.e velocity term, it accumulates the velocity of previous gradient and move forward with that momentum. I\u0026rsquo;ve tried to see how it builds up velocity in the image below. As we can see gradient update depends not only on it\u0026rsquo;s current gradient but also it\u0026rsquo;s previous weights which provide the instance (t+1) with some momentum i.e gradient from previous steps (t, t-1, t-2 and so on). As we move towards higher iterations (t=100000) the effect of initial gradients i.e t=0, t=1 and so on becomes 0 because beta term is raised to the power t=100000, and only the closest gradients are fully taken into considerations. For instance, we can see in the image below how low priority is given to the previous weights i.e 0.729 to W0, and 0.081 to W1 and so on.\nWeakness same learning rate is applied to all the parameters because of which the update to the parameters isn\u0026rsquo;t precise. RMSprop This optimizer tries to solve the problem of SGD with momentum i.e it tunes the learning rate based on it\u0026rsquo;s gradient. It sets the effect of learning rate based on it\u0026rsquo;s v term which is simply the accumulation of previous weights.\nAs you can see in the calculation done in left hand side the gradient accumulation i.e v is small so the effect of learning rate is bigger (we take bigger jumps), we take bigger jumps because we don\u0026rsquo;t want our convergence to be too slow or to stagnate, so we take bigger jumps.\nBut when the gradient accumulation i.e v term is bigger (as shown in right hand side) we take smaller steps, because in those cases if we take a big jump we might miss the global minima, so the effect of learning rate in this case is decreased. Adam This optimizer is simply the combination of both the momentum and RMSprop. It has its own speed determined by momentum and the learning rate adjustment provided by RMSprop.\nThe only modification is the addition of M hat and V hat i.e we scale of M and V, because initially we set the value of M and V to 0. The explanation about why we this is also provided in the image below. ","permalink":"https://cohlem.github.io/sub-notes/optimization-algorithms/","summary":"The simplest algorithm is the gradient descent in which we simply calculate loss over all the training data and then update our parameters, but it would be too slow and would consume too much resources. A faster approach is to use SGD where we calculate loss over every single training data and then do the parameter update, but the gradient update could be fuzzy. A more robust approach is to do mini batch SGD.","title":"Optimization Algorithms (SGD with momentum, RMSProp, Adam)"},{"content":"Main code n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 64 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) # Layer 1 W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) b1 = torch.randn(n_hidden, generator=g) * 0.1 # using b1 just for fun, it\u0026#39;s useless because of BN # Layer 2 W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1 b2 = torch.randn(vocab_size, generator=g) * 0.1 # BatchNorm parameters bngain = torch.randn((1, n_hidden))*0.1 + 1.0 bnbias = torch.randn((1, n_hidden))*0.1 # Note: I am initializating many of these parameters in non-standard ways # because sometimes initializating with e.g. all zeros could mask an incorrect # implementation of the backward pass. parameters = [C, W1, b1, W2, b2, bngain, bnbias] print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters: p.requires_grad = True batch_size = 32 n = batch_size # a shorter variable also, for convenience # construct a minibatch ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y # forward pass, \u0026#34;chunkated\u0026#34; into smaller steps that are possible to backward one at a time emb = C[Xb] # embed the characters into vectors embcat = emb.view(emb.shape[0], -1) # concatenate the vectors # Linear layer 1 hprebn = embcat @ W1 + b1 # hidden layer pre-activation # BatchNorm layer bnmeani = 1/n*hprebn.sum(0, keepdim=True) bndiff = hprebn - bnmeani bndiff2 = bndiff**2 bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel\u0026#39;s correction (dividing by n-1, not n) bnvar_inv = (bnvar + 1e-5)**-0.5 bnraw = bndiff * bnvar_inv hpreact = bngain * bnraw + bnbias # Non-linearity h = torch.tanh(hpreact) # hidden layer # Linear layer 2 logits = h @ W2 + b2 # output layer # cross entropy loss (same as F.cross_entropy(logits, Yb)) logit_maxes = logits.max(1, keepdim=True).values norm_logits = logits - logit_maxes # subtract max for numerical stability counts = norm_logits.exp() counts_sum = counts.sum(1, keepdims=True) counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can\u0026#39;t get backprop to be bit exact... probs = counts * counts_sum_inv logprobs = probs.log() loss = -logprobs[range(n), Yb].mean() # PyTorch backward pass for p in parameters: p.grad = None for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way norm_logits, logit_maxes, logits, h, hpreact, bnraw, bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani, embcat, emb]: t.retain_grad() loss.backward() loss Initially we have this forward pass of a NN, how do we backpropagate through this? We simply call loss.backward() which is an abstraction of pytorch\u0026rsquo;s autograd engine, it\u0026rsquo;ll construct computation graph and calculate gradients for all the nodes under the hood.\nHow can we do it manually?\nhere\u0026rsquo;s how\nManual Backprop # Exercise 1: backprop through the whole thing manually, # backpropagating through exactly all of the variables # as they are defined in the forward pass above, one by one # ----------------- # YOUR CODE HERE :) dlogprobs = torch.zeros_like(logprobs) dlogprobs[range(n), Yb] = -1.0*(1/logprobs.shape[0]) # 1 dprobs = (1/probs)*dlogprobs # 2 dcounts_sum_inv = (dprobs*counts).sum(1, keepdim = True) dcounts = dprobs * counts_sum_inv dcounts_sum = -1.0*((counts_sum)**(-2.0))*dcounts_sum_inv dcounts += torch.ones_like(counts_sum)*dcounts_sum dnorm_logits = norm_logits.exp()*dcounts dlogit_maxes = (-1.0*dnorm_logits).sum(1,keepdim=True) dlogits = (1.0*dnorm_logits) dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1])*dlogit_maxes db2 = (dlogits*torch.ones_like(logits)).sum(0) dh = dlogits @ W2.T dW2 = h.T @ dlogits dhpreact = dh*(1-h**(2)) dbnbias = (dhpreact*torch.ones_like(bnraw)).sum(0, keepdim= True) dbngain = (dhpreact*bnraw*torch.ones_like(bnraw)).sum(0, keepdim=True) dbnraw = dhpreact*bngain*torch.ones_like(bnraw) dbnvar_inv = (dbnraw* (torch.ones_like(bndiff) * bndiff)).sum(0, keepdim=True) dbndiff = (dbnraw* (torch.ones_like(bndiff) * bnvar_inv)) dbnvar = dbnvar_inv* (-0.5)*(((bnvar + 1e-5))**(-1.5)) dbndiff2 = (1.0/(n-1) )*torch.ones_like(bndiff2) * dbnvar dbndiff += dbndiff2*2*(bndiff) dhprebn = dbndiff*1.0 dbnmeani = (torch.ones_like(hprebn)*-1.0*dbndiff).sum(0, keepdim = True) dhprebn += torch.ones_like(hprebn)*(1/n)*dbnmeani db1 = (torch.ones_like(dhprebn)*dhprebn).sum(0) dembcat = dhprebn @ W1.T dW1 = embcat.T @ dhprebn demb = dembcat.view(emb.shape[0],emb.shape[1],emb.shape[2]) dC = torch.zeros_like(C) for i in range(Xb.shape[0]): for j in range(Xb.shape[1]): dC[Xb[i,j]] += demb[i,j] # print(demb[i,j].shape) # ----------------- cmp(\u0026#39;logprobs\u0026#39;, dlogprobs, logprobs) cmp(\u0026#39;probs\u0026#39;, dprobs, probs) cmp(\u0026#39;counts_sum_inv\u0026#39;, dcounts_sum_inv, counts_sum_inv) cmp(\u0026#39;counts_sum\u0026#39;, dcounts_sum, counts_sum) cmp(\u0026#39;counts\u0026#39;, dcounts, counts) cmp(\u0026#39;norm_logits\u0026#39;, dnorm_logits, norm_logits) cmp(\u0026#39;logit_maxes\u0026#39;, dlogit_maxes, logit_maxes) cmp(\u0026#39;logits\u0026#39;, dlogits, logits) cmp(\u0026#39;h\u0026#39;, dh, h) cmp(\u0026#39;W2\u0026#39;, dW2, W2) cmp(\u0026#39;b2\u0026#39;, db2, b2) cmp(\u0026#39;hpreact\u0026#39;, dhpreact, hpreact) cmp(\u0026#39;bngain\u0026#39;, dbngain, bngain) cmp(\u0026#39;bnbias\u0026#39;, dbnbias, bnbias) cmp(\u0026#39;bnraw\u0026#39;, dbnraw, bnraw) cmp(\u0026#39;bnvar_inv\u0026#39;, dbnvar_inv, bnvar_inv) cmp(\u0026#39;bnvar\u0026#39;, dbnvar, bnvar) cmp(\u0026#39;bndiff2\u0026#39;, dbndiff2, bndiff2) cmp(\u0026#39;bndiff\u0026#39;, dbndiff, bndiff) cmp(\u0026#39;bnmeani\u0026#39;, dbnmeani, bnmeani) cmp(\u0026#39;hprebn\u0026#39;, dhprebn, hprebn) cmp(\u0026#39;embcat\u0026#39;, dembcat, embcat) cmp(\u0026#39;W1\u0026#39;, dW1, W1) cmp(\u0026#39;b1\u0026#39;, db1, b1) cmp(\u0026#39;emb\u0026#39;, demb, emb) cmp(\u0026#39;C\u0026#39;, dC, C) Results logprobs | exact: True | approximate: True | maxdiff: 0.0 probs | exact: True | approximate: True | maxdiff: 0.0 counts_sum_inv | exact: True | approximate: True | maxdiff: 0.0 counts_sum | exact: True | approximate: True | maxdiff: 0.0 counts | exact: True | approximate: True | maxdiff: 0.0 norm_logits | exact: True | approximate: True | maxdiff: 0.0 logit_maxes | exact: True | approximate: True | maxdiff: 0.0 logits | exact: True | approximate: True | maxdiff: 0.0 h | exact: True | approximate: True | maxdiff: 0.0 W2 | exact: True | approximate: True | maxdiff: 0.0 b2 | exact: True | approximate: True | maxdiff: 0.0 hpreact | exact: True | approximate: True | maxdiff: 0.0 bngain | exact: True | approximate: True | maxdiff: 0.0 bnbias | exact: True | approximate: True | maxdiff: 0.0 bnraw | exact: True | approximate: True | maxdiff: 0.0 bnvar_inv | exact: True | approximate: True | maxdiff: 0.0 bnvar | exact: True | approximate: True | maxdiff: 0.0 bndiff2 | exact: True | approximate: True | maxdiff: 0.0 bndiff | exact: True | approximate: True | maxdiff: 0.0 bnmeani | exact: True | approximate: True | maxdiff: 0.0 hprebn | exact: True | approximate: True | maxdiff: 0.0 embcat | exact: True | approximate: True | maxdiff: 0.0 W1 | exact: True | approximate: True | maxdiff: 0.0 b1 | exact: True | approximate: True | maxdiff: 0.0 emb | exact: True | approximate: True | maxdiff: 0.0 C | exact: True | approximate: True | maxdiff: 0.0 The result verifies that our gradients matches pytorch\u0026rsquo;s.\nStep-by-step calculation Backpropagating on scalars is pretty straightforward as we did in our first note but when it comes to tensors, we need to make sure every element\u0026rsquo;s gradient in a tensor is calculated precisely.\nlet\u0026rsquo;s understand it line by line.\nloss = -logprobs[range(n), Yb].mean() now we calculate the derivative of loss (L) w.r.t logprobs, (NOTE: d(loss)/d(loss) is 1). here\nwe have: d(L)/dL to find: d(L)/dlogprobs\nd(L)/dlogprobs = d(L)/dL x d(L)/dlogprobs # d(L)/dlogprobs is local gradient d(L)/dlogprobs = 1.0 * d(L)/dlogprobs Now what could be the d(L)/dlogprobs? let\u0026rsquo;s break it down by representing it into a simple matrix. let\u0026rsquo;s say logprobs is a matrix and using indexing [range(n), Yb] we pluck out it\u0026rsquo;s corresponding values and then we average it. let\u0026rsquo;s consider the plucked out values are\na1 , b1 , c1 it\u0026rsquo;s mean would be\n1/3 x a1 + 1/3 x b1 + 1/3 x c1 the derivative of d(L)/da1 = 1/3 , d(L)/db1 = 1/3 , d(L)/dc1 = 1/3 we see a pattern here, derivate of every element is 1/total_no_of_elements.\nso\ndlogprobs = torch.zeros_like(logprobs) # because all the other elements will have 0 gradient as they\u0026#39;ll be considered constant dlogprobs[range(n), Yb] = 1.0 * 1/(-logprobs).shape[0] logprobs = probs.log() to find : dprobs\nwe know d(logx)/dx = 1/x, so its fairly simple.\ndprobs = 1/probs * dlogprobs # don\u0026#39;t forget to add the dlogprobs because its the gradient\u0026#39;s that propagated probs = counts * counts_sum_inv let\u0026rsquo;s find dcounts_sum_inv we need to make sure that the gradient of any tensor should have the same size as that tensor. the shape of counts_sum_inv is (32,1)\ndcounts_sum_inv = (dprobs *torch.ones_like(count) * counts).sum(1, keepdim = True) why do we sum it across rows? this is because in probs = counts * counts_sum_inv,\ncounts has shape (32,27) and counts_sum_inv has (32,1), so first the counts_sum_inv is broadcasted and is made into shape (32,27) by copying the column and then finally is multiplied with counts. There are two operations that take place in order (broadcasting and addition). So, when we backpropagate through this equation the order should be addition and broadcasting. so the dcounts_sum_inv is (dprobs * torch.ones_like(count) * counts), but this is of shape (32,27), as we have seen the columns in counts_sum_inv are broadcasted, which mean one column is used 27 times, so we know that from our first note that when a variable is used more than once it\u0026rsquo;s derivate is added up, so we sum across the rows (sum it 27 times).\ndcounts_sum_inv = (dprobs* torch.ones_like(count) * counts).sum(1, keepdim = True) Similarly, we can now calculate the gradients for tensors that were broadcasted in our forward pass. All the other gradient calculation is relatively straightforward except this equation\nlogits = h @ W2 + b2 # output layer why? because if we go deep into matrix multiplication, we see there are two operations involved i.e multiplication and addition. The formula for calculating gradient for the equation above is derived in the picture below.\nWe come up with a simple equation.\ndh = dlogits @ W2.T dW2 = h.T @ dlogits I believe these were the main gradient calculation steps and gradients for other nodes can be calculated in a similar manner.\nA more detailed code can be found here\n","permalink":"https://cohlem.github.io/sub-notes/manual-backpropagation-on-tensors/","summary":"Main code n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 64 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) # Layer 1 W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) b1 = torch.randn(n_hidden, generator=g) * 0.1 # using b1 just for fun, it\u0026#39;s useless because of BN # Layer 2 W2 = torch.","title":"manual-backpropagation-on-tensors"},{"content":"In deep learning, it\u0026rsquo;s important to visualize a matrix and how it is represented in a dimension space because the operations that we perform on those matrix becomes very much intuitive afterwards.\nVisualizing two dimensional matrix. This has to be the most intuitive visualization.\n[ [12, 63, 10, 42, 70, 31, 34, 8, 34, 5], [10, 97, 100, 39, 64, 25, 86, 22, 31, 25], [28, 44, 82, 61, 70, 94, 22, 88, 89, 56] ] We can simply imagine rows are some examples and columns as those examples\u0026rsquo; features.\nVisualizing three dimensional matrix. let\u0026rsquo;s imagine we have a matrix M with the values\nM = [ [ [1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12] ], [ [13, 14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24] ] ] it is of dimension (2,4,3). How do we visualize it? The key to visualizing a three dimensional matrix is to image it like a two dimensional matrix but only that it\u0026rsquo;s first dimension is stacked behind one after another.\nfor instance, for the same matrix image it like it is (4,3) matrix but two of these are stacked behind one after another.\nlet\u0026rsquo;s understand it using an image.\nas you can see from this size of the view we can mostly see its last two dimensions i.e (4,3).\nLet\u0026rsquo;s take another view. as you can see that 2 examples are stacked behind one another.\nTo visualize it in 3D you can take visit this website https://array-3d-viz.vercel.app\n","permalink":"https://cohlem.github.io/sub-notes/matrix-visualization/","summary":"In deep learning, it\u0026rsquo;s important to visualize a matrix and how it is represented in a dimension space because the operations that we perform on those matrix becomes very much intuitive afterwards.\nVisualizing two dimensional matrix. This has to be the most intuitive visualization.\n[ [12, 63, 10, 42, 70, 31, 34, 8, 34, 5], [10, 97, 100, 39, 64, 25, 86, 22, 31, 25], [28, 44, 82, 61, 70, 94, 22, 88, 89, 56] ] We can simply imagine rows are some examples and columns as those examples\u0026rsquo; features.","title":"Matrix Visualization"},{"content":"source: Building makemore Part 3: Activations \u0026amp; Gradients, BatchNorm\nThings to look out for while training NN Take a look at previous notes to understand this note better\nconsider we have this simple 6 layer NN\n# Linear Layer g = torch.Generator().manual_seed(2147483647) # for reproducibility class Layer: def __init__(self,fan_in, fan_out, bias=False): self.w = torch.randn((fan_in, fan_out),generator = g) / (fan_in)**(0.5) # applying kaiming init self.bias = bias if bias: self.b = torch.zeros(fan_out) def __call__(self, x): y = x @ self.w self.out = y + self.b if self.bias else y return self.out def parameters(self): return [self.w] + [self.b] if self.bias else [self.w] class Tanh: def __call__(self, x): self.out = torch.tanh(x) return self.out def parameters(self): return [] class BatchNormalization1: def __init__(self,nf, eps= 1e-5, mom=0.1): self.bngain = torch.ones(nf) self.bnbias = torch.zeros(nf) self.out = None self.mom = mom self.training = True self.running_mean = torch.ones(nf) self.running_var = torch.zeros(nf) self.eps = eps def __call__(self,x): if self.training: meani = x.mean(0, keepdim = True) vari = x.var(0, keepdim = True) else: meani = self.running_mean vari = self.running_var if self.training: with torch.no_grad(): self.running_mean = (1-self.mom)*self.running_mean + self.mom*meani self.running_var = (1-self.mom)*self.running_var + self.mom*vari self.out = self.bngain *((x - meani)/ torch.sqrt(vari + self.eps)) + self.bnbias return self.out def parameters(self): return [self.bngain, self.bnbias] Structure\nimport torch.nn.functional as F x = torch.randn(32, 30, generator = g) y = torch.tensor([random.randint(0,26) for _ in range(32)] ) # Embedding layer, n_embd = 10 n_vocab = 27 n_dim = 100 batch_size = 32 C = torch.randn((n_vocab,n_embd)) st = [ # x shape = 32, 30 Layer(n_embd*block_size,n_dim), Tanh(), Layer(n_dim, n_dim), Tanh(), Layer(n_dim, n_dim) , Tanh(), Layer(n_dim, n_dim), Tanh(), Layer(n_dim, n_dim), Tanh(), Layer(n_dim, n_vocab),BatchNormalization1(n_vocab) ] with torch.no_grad(): st[-1].bngain *= 0.1 for layer in st[:-1]: if isinstance(layer, Layer): layer.w *= 5/3 parameters = [C] + [p for l in st for p in l.parameters()] for p in parameters: p.requires_grad = True Training Loop\nfor iteration in range(200000): # for iteration in range(2000): idx = torch.randint(0,Xtr.shape[0], (batch_size,)) x_emb = C[Xtr[idx]].view(-1, block_size * n_embd) x = x_emb for idx,item in enumerate(st): # print(idx) x = item(x) loss = F.cross_entropy(x,y) for layer in st: layer.out.retain_grad() for p in parameters: p.grad = None loss.backward() lr = 0.1 if iteration \u0026lt; 150000 else 0.01 for p in parameters: p.data += -lr*p.grad if iteration % 10000 ==0: print(loss.data) # if iteration \u0026gt;= 1000: # break let\u0026rsquo;s look at our activations before initializing weights using kaiming init.\n# these are just part of modified code from the code that\u0026#39;s given above. class Layer: def __init__(self,fan_in, fan_out, bias=False): self.w = torch.randn((fan_in, fan_out),generator = g) # / (fan_in)**(0.5) # commenting our the kaiming init # part of code with torch.no_grad(): st[-1].bngain *= 0.1 for layer in st[:-1]: if isinstance(layer, Layer): layer.w *= 1.0 # setting gains to 1.0 (no gain) Activation plot As you can see almost all the pre activations are saturated, this is because our weight is initialized in such a way that after applying tanh, most of our output values lie in -1 and 1, which will stop gradient propagation.\nNow applying kaiming init with with no gain.\n# these are just part of modified code from the code that\u0026#39;s given above. class Layer: def __init__(self,fan_in, fan_out, bias=False): self.w = torch.randn((fan_in, fan_out),generator = g) / (fan_in)**(0.5) # applying the kaiming init # part of code with torch.no_grad(): st[-1].bngain *= 0.1 for layer in st[:-1]: if isinstance(layer, Layer): layer.w *= 1.0 # setting gains to 1.0 (no gain) The plot is starting to look nicer, because there is less saturation, because now values don\u0026rsquo;t lie in the extreme values of tanh, and gradient will be propagated. But we still have issue, as we can see the standard deviation is decreasing this is because of the property of tanh, i.e it squashes the values, initially (blue plot) the output was decent but in later layers, the distribution is being shrinked that because of the property of tanh.\nnow let\u0026rsquo;s apply kaiming init with gain too, for tanh the gain is 5/3.\nNow the values are being evenly distributed, and the standard deviation is stable (doesn\u0026rsquo;t decrease with iteration).\nWe have to precisely measure the gains to have a stable training. But the introduction of batch normalization changes the case, and we don\u0026rsquo;t have to be that much aware for precisely initializing weights.\nLet\u0026rsquo;s now apply the batch normalization but without kaiming init and see the same plot.\nst = [ # x shape = 32, 30 Layer(n_embd*block_size,n_dim), BatchNormalization1(n_dim), Tanh(), Layer(n_dim, n_dim), BatchNormalization1(n_dim), Tanh(), Layer(n_dim, n_dim), BatchNormalization1(n_dim), Tanh(), Layer(n_dim, n_dim), BatchNormalization1(n_dim), Tanh(), Layer(n_dim, n_dim), BatchNormalization1(n_dim), Tanh(), Layer(n_dim, n_vocab),BatchNormalization1(n_vocab) ] The output values are properly distributed, with very less saturation and a constant standard deviation.\nGradient plot The gradient distribution at each layers would look like this when the pre activations are batch normalized. Gradient to data ratio plot This is what the ratio of gradient (calculated after backprop) to data plot looks like. x-axis represent iterations, y represent the exponents. Ideally, 1e-3 is suitable and that ratio should lie around that line. If the ratio is below that line it means, we need to step up our learning rate, and if it is higher than that line we need to lower our learning rate.\nThe gain that we add during kaiming init has direct correlation with this plot.\nwith torch.no_grad(): # last layer: make less confident layers[-1].gamma *= 0.1 #layers[-1].weight *= 0.1 # all other layers: apply gain for layer in layers[:-1]: if isinstance(layer, Linear): layer.weight *= 0.3 as you can see, when I make gain to 0.3 the ratio significantly varies, i.e ratio for later layers are around 1e-1.5, which mean we would have to lower our learning rate because of this gain change.\nSo the gain significantly affects our learning rate, but it doesn\u0026rsquo;t affect other plots that we plot above, because it\u0026rsquo;s controlled by batch normalization.\nSo we don\u0026rsquo;t get a free pass to assign these gains arbitrarily, because it affects our gradients (as seen from the ratio plot). If we don\u0026rsquo;t worry about these gains, we have to tune these learning rates properly (by increasing or decreasing the learning rate).\nThese data is analyzed throughout the training of NN\nNOTE to myself after any operation look out for how the output\u0026rsquo;s standard deviation changes, we should always maintain the std of 1\nfor instance while doing the dot production attention,\nQ @ K.T\nthe output\u0026rsquo;s std grows by sqrt of last embedding or head dimension, which is the reason why we scale it by the sqrt of that last embedding dimension.\nSimilarly, in skip connections too the addition of x back to the output introduces increase in std, we should scale that down too as i\u0026rsquo;ve mentioned here\n","permalink":"https://cohlem.github.io/sub-notes/diagnostic-tool-while-training-nn/","summary":"source: Building makemore Part 3: Activations \u0026amp; Gradients, BatchNorm\nThings to look out for while training NN Take a look at previous notes to understand this note better\nconsider we have this simple 6 layer NN\n# Linear Layer g = torch.Generator().manual_seed(2147483647) # for reproducibility class Layer: def __init__(self,fan_in, fan_out, bias=False): self.w = torch.randn((fan_in, fan_out),generator = g) / (fan_in)**(0.5) # applying kaiming init self.bias = bias if bias: self.b = torch.","title":"Diagnostic-tool-while-training-nn"},{"content":"As we saw in our previous note how important it is to have the pre-activation values to be roughly gaussian (0 mean, and unit std). We saw how we can initialize our weights that make our pre-activation roughly gaussian by using Kaiming init. But, how do we always maintain our pre-activations to be roughly gaussian?\nAnswer: BatchNormalization\nBenefits\nstable training preserves vanishing gradients BatchNormalization As the name suggests, batches are normalized (across batches), by normalizing across batches we preserve the gaussian property of our pre-activations.\nThis is our initial code\n# same optimization as last time max_steps = 200000 batch_size = 32 lossi = [] for i in range(max_steps): # minibatch construct ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y # forward pass emb = C[Xb] # embed the characters into vectors embcat = emb.view(emb.shape[0], -1) # concatenate the vectors # Linear layer hpreact = (embcat @ W1 + b1) # hidden layer pre-activation # Non-linearity h = torch.tanh(hpreact) # hidden layer logits = h @ W2 + b2 # output layer loss = F.cross_entropy(logits, Yb) # loss function # backward pass for p in parameters: p.grad = None hpreact.retain_grad() logits.retain_grad() loss.backward() # update lr = 0.1 if i \u0026lt; 100000 else 0.01 # step learning rate decay for p in parameters: p.data += -lr * p.grad # track stats if i % 10000 == 0: # print every once in a while print(f\u0026#39;{i:7d}/{max_steps:7d}: {loss.item():.4f}\u0026#39;) lossi.append(loss.log10().item()) We add batch normalization just before the all activation layers. Right now, we only have one linear layer so only one tanh activation is applied, in a big NN, we have to add those batch normalization before applying activations.\nApplying batch normalization is quite simple.\nhpreact = (hpreact - hpreact.mean(0, keepdim = True)) / hpreact.std(0, keepdim =True) but by doing this we are forcing it to lie is a particular space. To add a little bit of entropy (randomness) and let to model learn itself (by backpropagating) where it wants to go, we introduce scaling and shifting. The model will learn itself the direction it wants to go, by back propagating because these scaling and shifting are differentiable.\n# BatchNorm parameters bngain = torch.ones((1, n_hidden)) bnbias = torch.zeros((1, n_hidden)) hpreact = bngain*(hpreact - hpreact.mean(0, keepdim = True)) / hpreact.std(0, keepdim =True) + bnbias by introducing batchNormalization, we are making our pre-activation of one input depend on all the other input, this is because we are subtracting the mean from it and this mean is the depended on all the other inputs.\nNow that we have introduced BatchNormalization, inorder to do inference, we would need the mean and std of the whole dataset, which we can keep track of in running manner, and it is used while doing inference.\n# same optimization as last time max_steps = 200000 batch_size = 32 lossi = [] for i in range(max_steps): # minibatch construct ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y # forward pass emb = C[Xb] # embed the characters into vectors embcat = emb.view(emb.shape[0], -1) # concatenate the vectors # Linear layer hpreact = (embcat @ W1 + b1) # hidden layer pre-activation bnmeani = hpreact.mean(0, keepdim = True) bnstdi = hpreact.std(0, keepdim =True) hpreact = bngain*((hpreact - bnmeani) / bnstdi) + bnbias ## --------------------- keeping track of whole mean and std ------------ with torch.no_grad(): bnmean_running = 0.999*bnmean_running + 0.001*bnmeani bnstd_running = 0.999*bnstd_running + 0.001*bnstdi # ------------------------------------------------------------------------ # Non-linearity h = torch.tanh(hpreact) # hidden layer logits = h @ W2 + b2 # output layer loss = F.cross_entropy(logits, Yb) # loss function # backward pass for p in parameters: p.grad = None hpreact.retain_grad() logits.retain_grad() loss.backward() # update lr = 0.1 if i \u0026lt; 100000 else 0.01 # step learning rate decay for p in parameters: p.data += -lr * p.grad # track stats if i % 10000 == 0: # print every once in a while print(f\u0026#39;{i:7d}/{max_steps:7d}: {loss.item():.4f}\u0026#39;) lossi.append(loss.log10().item()) ","permalink":"https://cohlem.github.io/sub-notes/batchnormalization/","summary":"As we saw in our previous note how important it is to have the pre-activation values to be roughly gaussian (0 mean, and unit std). We saw how we can initialize our weights that make our pre-activation roughly gaussian by using Kaiming init. But, how do we always maintain our pre-activations to be roughly gaussian?\nAnswer: BatchNormalization\nBenefits\nstable training preserves vanishing gradients BatchNormalization As the name suggests, batches are normalized (across batches), by normalizing across batches we preserve the gaussian property of our pre-activations.","title":"BatchNormalization"},{"content":" ","permalink":"https://cohlem.github.io/sub-notes/maximum-likelihood-estimate-as-loss/","summary":" ","title":"Maximum likelihood estimate as loss function"},{"content":"Source: The spelled-out intro to neural networks and backpropagation: building micrograd\nBackpropagation on paper It implements backpropagation for two arithmetic operation (multiplication and addition) which are quite straightforward.\nImplementation is for this equation.\na = Value(2.0, label=\u0026#39;a\u0026#39;) b = Value(-3.0, label=\u0026#39;b\u0026#39;) c = Value(10.0, label=\u0026#39;c\u0026#39;) e = a*b; e.label = \u0026#39;e\u0026#39; d = e + c; d.label = \u0026#39;d\u0026#39; f = Value(-2.0, label=\u0026#39;f\u0026#39;) L = d * f; L.label = \u0026#39;L\u0026#39; L The most important thing to note here is the gradient accumulation step (shown at the bottom-left). If a node takes part two times building up to the final node. The gradient for that node is accumulated. For instance, in the figure node b takes part two times. First, it is involved in equation e = a * b, and another is e = b + m (not in the equation above).\nCode\nclass Value: def __init__(self,data, _children = (), _op = \u0026#39;\u0026#39;, label=None): self.data = data self.label = label self.grad = 0.0 self._prev = set(_children) self._op = _op self._backward = lambda: None def __repr__(self): return f\u0026#34;Value({self.label}, {self.data})\u0026#34; def __add__(self, other): result = Value(self.data + other.data, (self, other), \u0026#39;+\u0026#39;) def _backward(): self.grad = 1.0 * result.grad other.grad = 1.0 * result.grad result._backward = _backward return result def __sub__(self,other): result = Value(self.data - other.data) result._prev = [self, other] result._op = \u0026#39;-\u0026#39; return result def __mul__(self,other): result = Value(self.data * other.data, (self, other), \u0026#39;*\u0026#39;) def _backward(): self.grad = other.data * result.grad other.grad = self.data * result.grad result._backward = _backward return result def backward(self): topo = [] visited = set() self.grad = 1 def build_topo(v): if v not in visited: visited.add(v) for child in v._prev: build_topo(child) topo.append(v) build_topo(self) topo = list(reversed(topo)) print(\u0026#39;gg\u0026#39;, topo) for i in topo: print(i) i._backward() ","permalink":"https://cohlem.github.io/sub-notes/backpropagation-from-scratch/","summary":"Source: The spelled-out intro to neural networks and backpropagation: building micrograd\nBackpropagation on paper It implements backpropagation for two arithmetic operation (multiplication and addition) which are quite straightforward.\nImplementation is for this equation.\na = Value(2.0, label=\u0026#39;a\u0026#39;) b = Value(-3.0, label=\u0026#39;b\u0026#39;) c = Value(10.0, label=\u0026#39;c\u0026#39;) e = a*b; e.label = \u0026#39;e\u0026#39; d = e + c; d.label = \u0026#39;d\u0026#39; f = Value(-2.0, label=\u0026#39;f\u0026#39;) L = d * f; L.label = \u0026#39;L\u0026#39; L The most important thing to note here is the gradient accumulation step (shown at the bottom-left).","title":"Backpropagation from scratch"},{"content":"Backpropagation Backpropagation on scalars from scratch Manual Backpropagation on tensor Loss function Maximum likelihood estimate as loss function Why we add regularization to loss functio√± Optimization Optimization Algorithms (SGD with momentum, RMSProp, Adam) Optimizing loss with weight initialization BatchNormalization RMSNorm Diagnostic tool to look out for while training NN Skip Connections Training Misc Matrix Visualization SwiGLU activation- not mine, but offers best explanation Architecture Implementation GPT implementation MoE RoPE KV Cache and Grouped Query Attention GPU Basic intro to GPU architecture ","permalink":"https://cohlem.github.io/notes/deep-learning-notes/","summary":"Backpropagation Backpropagation on scalars from scratch Manual Backpropagation on tensor Loss function Maximum likelihood estimate as loss function Why we add regularization to loss functio√± Optimization Optimization Algorithms (SGD with momentum, RMSProp, Adam) Optimizing loss with weight initialization BatchNormalization RMSNorm Diagnostic tool to look out for while training NN Skip Connections Training Misc Matrix Visualization SwiGLU activation- not mine, but offers best explanation Architecture Implementation GPT implementation MoE RoPE KV Cache and Grouped Query Attention GPU Basic intro to GPU architecture ","title":"Deep Learning Notes"},{"content":" it penalizes the weights, and prioritizes uniformity in weights. How does it penalize the weights? Now when we do the backprop and gradient descent.\nThe gradient of loss w.r.t some weights become as we can see it penalizes the weight by reducing the weights\u0026rsquo;s value by some higher amount compared to the some minimial weight update when we only used loss function.\nSo overall, the model tries to balance the Loss (L) as well as keep the weights small. This balance prevents the model from relying excessively on any particular weight.\n","permalink":"https://cohlem.github.io/sub-notes/why-we-need-regularization/","summary":"it penalizes the weights, and prioritizes uniformity in weights. How does it penalize the weights? Now when we do the backprop and gradient descent.\nThe gradient of loss w.r.t some weights become as we can see it penalizes the weight by reducing the weights\u0026rsquo;s value by some higher amount compared to the some minimial weight update when we only used loss function.\nSo overall, the model tries to balance the Loss (L) as well as keep the weights small.","title":"Why we add regularization in loss function"},{"content":"","permalink":"https://cohlem.github.io/posts/backpropogation/","summary":"","title":"Backpropagation from scratch"},{"content":"BackStory This is a simple fun little project that I did almost a year ago, At that time I used to see a lot of CodeBullet\u0026rsquo;s videos and wanted to learn the gist behind these evolutionary algorithms, and I can\u0026rsquo;t get into my head if I don\u0026rsquo;t do it from scratch so I wanted to implement this project from scratch. At that time, I wanted to document the learning and the implementation process, I even thought of making a youtube video about this topic but could not complete it, at that time, I had made some video animations about this process but could not complete it because I started doing something else and when I can back it was already too much mess and could not complete the video animations, but I\u0026rsquo;ll include the video animations that I had made earlier.\nSnake using Neural Network and Genetic Algorithm This is a simple demonstration of how neural network can be used along with genetic algorithm to play snake game. Now when I say its a snake game, this is just basically learning how to optimize a function, just like what we do with gradient descent. But that\u0026rsquo;s the interesting part we\u0026rsquo;re not using Gradient Descent, no fancy optimizers, even the initial weights were initialized randomly, I now wish(at the time of writing this blog) that I had used some initialization techniques like Xavier initializations, it would have learned to play so much faster. But even the random initialization works. Let\u0026rsquo;s get to the point.\nWe are playing a snake game let\u0026rsquo;s imagine that has a brain, So the brain here are weights which we initialize using Feed-forward layer and then we use genetic algorithm to evolve that brain(weights) and use this brain(weights) to predict which move to take(up, down, left or right) at every step in the game. So the term Neural Network here only represents the feed-forward layer, we don\u0026rsquo;t use any of the backpropogation.\nThe whole code for this project can be found here: Github\nDemo Video Explanation Following are the explanations that I\u0026rsquo;ve implemented for this project.\nSnake\u0026rsquo;s Eye It is what snake sees at each step while it\u0026rsquo;s playing game. Before performing anything, it has to see and pass it to the brain inorder to process what it has seen. So let\u0026rsquo;s understand what it sees at each step by looking at the video below.\nOur snake sees in 8 different directions(d1 to d8) as shown in the figure above. And at each direction it sees 3 different things Wall, Food, Body(it\u0026rsquo;s own body)\nSnake\u0026rsquo;s Brain Now that we have our snake eyes let\u0026rsquo;s make the brain to process the things that it has seen. We use a simple two layer neural network. The first layer has 8 hidden layers and the second is output layer of 4 units. We only use the feed forward part of neural network and discard all the other backpropagation part. We will later use genetic algorithm to improve our weights.\nThe figure below contains two parts\nSnake\u0026rsquo;s eye(descibed above), and Snake\u0026rsquo;s brain Input to neural network Now that we now our snake sees a body, wall and food. Let\u0026rsquo;s assign values to these metrics.\nSince the neural networks only understand numbers we represent them with a metric, 1 for favorable conditions and 0 for unfavorable.\nMetric for body: One is given when no body is present in that direction and Zero is given when no blocks are present in between Head and Body.\n$body = number of blocks between head and body/(total number of blocks - 1 )$.\nMetric for Wall: One is given when the number of blocks between Head and Wall are maximum and Zero is given when no blocks are present in between Head and Wall.\n$Wall = number of blocks between head and wall/(total number of blocks - 1 )$.\nMetric for Food: It is good for snake to move in the direction of food, so One is given when no blocks are present in between Head and Food and Zero is given when no food is present in that direction.\n$Food = ( total number of blocks - number of blocks between head and wall - 1)/(total number of blocks - 1 )$.\nThe another input to our snake is the it\u0026rsquo;s directions. It should keep. track of where it currently is to make move in another direction.\nValue for Head Direction: This is a simple one-hot encoding for the direction.\nBrain of Snake The brain of our snake is the weights inside our hidden layers and output layers.\nOur first hidden layer will have 8 * 28 weights with 8 bias. Our second layer will have 4 * 8 weights with 4 bias.\nSo, the total weights + bias counts to 268 which is the actual brain of our snake. So our neural network uses that brain to make a prediction in 4 directions. [Up, Right, Down, Left]\nGenetic Algorithm Since we do not use the backpropagation of our neural network to improve weights we will use use Genetic algorithm to improve weights.\nFive phases are considered in a genetic algorithm. You should first read these steps below and then you can come back to the video below.\nInitial brain (Population) We randomly generate the brain of size (1,268). Remember this is the initial phase where we initialize our weigths (brain) for our snake which described above. Later, this shape (1,268) is flattened into the structure of our feed-forward neural network which is described above.\nnp.random.choice(np.arange(-1,1, step =0.001),size = (population_size,weight_size), replace= True)\nweight size is 268 and population_size is the total number of snakes we want to train. I\u0026rsquo;ve trained 500 snakes in each generations. Thing to remember here is, each snake will have different brain. The random function above generates a linear vector which is later converted to matrices of sizes (8,28) for weights, (8,1) for bias and (4,8) for weights and (4,1) for bias using vector_to_matrix() function. The first two matrices is for hidden layer whereas the other two are for output layer.\nFitness function Now that we have 500 different snakes in each generation. We have to differentiate the great Snake from weak. Snake\u0026rsquo;s fitness is based on its score and number of steps it taken to achieve that score, so we created a function by using Steps and Score as variables which will helps the Snake in getting maximum score in less steps. I\u0026rsquo;ve used the following fitness function that evaluates fitness of snakes relative to score and steps taken.\nIf two snakes have the same score then the snake that achieves the score in less number of steps is considered.\nSelection This process selects the best fit snakes from the entire population according to their fitness value described above. It is then used produce a new population of offspring which will be used in a next generation. I\u0026rsquo;ve selected top 50 snakes initially according to their fitness value. The selected snakes will be called parents.\nCrossover We take the parents selected from the selection process to produce a new set of offsprings. We take 50 parents and iterate over (population_size - parent_length) times and use uniform crossover method to produce new offspring from these parents. we later add our 50 parents to the population set. This process will preserve the best fit snakes, even if the crossover and mutation yield bad set of population. The unifrom crossover can be explained likewise.\nMutation This example explains mutating by flipping bits.\n1\t0\t1\t0\t0\t1 0\n‚Üì\n1\t0\t1\t0\t1\t1\t0\nBut in our case we change the value of our snake\u0026rsquo;s brain. Among 268 brain cells we will change 13 of them randomly between -0.5 to 0.5.\nRunning We run the game through many generation(similar to epoch) to evolve our snake\u0026rsquo;s brain by applying the method explained above. We do it until the snake has learned to score desired number of points.\nI know this is shitty writing but still Thank You for reading till the end. ","permalink":"https://cohlem.github.io/posts/using-genetic-algorithm-for-weights-optimization/","summary":"BackStory This is a simple fun little project that I did almost a year ago, At that time I used to see a lot of CodeBullet\u0026rsquo;s videos and wanted to learn the gist behind these evolutionary algorithms, and I can\u0026rsquo;t get into my head if I don\u0026rsquo;t do it from scratch so I wanted to implement this project from scratch. At that time, I wanted to document the learning and the implementation process, I even thought of making a youtube video about this topic but could not complete it, at that time, I had made some video animations about this process but could not complete it because I started doing something else and when I can back it was already too much mess and could not complete the video animations, but I\u0026rsquo;ll include the video animations that I had made earlier.","title":"Using Genetic Algorithm for Weights Optimization"},{"content":"This is my experience and experimentation that I did while building a product for the use case of using LLMs for our own data for question answering. If you\u0026rsquo;re doing something similar, this could be of some help.\nThe most commonly used methods while using LLMs with our own data were typically\nFine-tuning the model with your own data Using Retrieval Augmented Generation (RAG) techniques Fine-tuning the model with your own data This is the initial method and follows the general structure of training a model\nData preparation Train Evaluate For my task, I chose to train OpenAI\u0026rsquo;s davinci base model\nThere were mostly no hyperparameters to tune, as OpenAI takes care of it outside the box. Training the model involved more instruction tuning, instructing the model to act in a similar way, rather than just training the model to save data in its model weights. The effectiveness of instruction tuning mostly depended on the data preparation process. Data preparation involved formatting the data into pairs of \u0026lt;instruction, completion\u0026gt;.\nThis is a crucial step for better outputs and depends on the size of the training data. When the model was trained with a small amount of data, it mostly followed the instructions but had limited knowledge and memory of facts from the dataset. In most cases, while testing the model, it followed the instructions but often produced incorrect answers. When the scale of data was increased, it started to follow both the instructions and retain more knowledge. So, this process was crucial in identifying when training OpenAI\u0026rsquo;s models worked best. If you\u0026rsquo;re using it to train on a small scale of data, this would not yield the desired output, and I would recommend using the other process I employed for small-sized data.\nUsing Retrieval Augmented Generation (RAG) techniques This technique is sometimes referred to as In-context Learning. This is one of the most trending topics since the release of ChatGPT, and you might have heard the phrase \u0026ldquo;chat with your own data.\u0026rdquo; This process is simple yet very effective when you have your own small-scale data, which I used for question answering. The process involves:\nDividing the data into chunks Here, the format of the data doesn\u0026rsquo;t really matter. The only thing we need to take care of is the size of the chunks. The chunk size should always be smaller than the context length of LLMs, providing space for prompt and completion texts. In my case, the context length of gpt-3.5-turbo was 4,096 tokens, and I divided the chunks into token sizes of 1000. I chose a token size of 1000 for and retrieved top-3 chunks to be passed to the LLM.\nConverting the chunks into embeddings This process generates embeddings, which are vector representations of our chunks. I experimented with a couple of embedding models, each having its pros and cons. My recommendations for each of the embedding models are as follows:\nall-MiniLM-L12-v2: Useful when you need fast conversion from chunks to embeddings. It has a relatively small dimension of 384 and does a decent job in converting to embeddings.\nOpenAI\u0026rsquo;s text-embedding-ada-002: Useful when you need to generate highly accurate embeddings. If you are using it in real-time, it would be too slow due to its high dimension size of 1536, and API calling makes it even slower.\nInstructor: Useful when you need the accuracy level of text-embedding-ada-002 and fast conversion from text to embeddings. This model is blazingly fast and would save on cost when you embed lots of data.\nI went with the Instructor-XL model.\nStoring the embeddings Many vector database companies have risen around this use case of storing embeddings, such as Pinecone, Chroma, and many more. The trend is to follow the hype and opt for vector databases, which, in fact, are completely useless. If your embeddings are really big, I would recommend using vector databases; otherwise, for medium to small-scale data, ndarray would do a great job. I decided to just use a numpy array.\nRetrieval Relevant chunks are retrieved based on the similarity between the user\u0026rsquo;s query\u0026rsquo;s embeddings and the chunks\u0026rsquo; embeddings. Metrics like dot product, cosine similarity, are widely adopted, whereas cosine similarity would suffice. After evaluating the similarity, the top-k chunks are retrieved. We can use reranking to improve the quality of relevant chunks, but top-k would suffice. After retrieving, we are ready to feed the retrieved chunks and the user\u0026rsquo;s query to LLM by combining them with prompting.\nPrompting Prompting has been the most important step in getting the desired output. For me, this has turned out to be harder than doing research, writing code, and debugging combined. Writing quality prompts is hard. I experimented with a couple of prompting techniques in this project.\nFew-shot prompting, few-shot combined with Chain of Thought (CoT) prompting, but I couldn\u0026rsquo;t achieve the desired output. I noticed some problems with these techniques for my use case. These prompting techniques caused too many logic jumps, and the desired logic was never analyzed by the LLM. What worked for me was the map-reduce method and double prompting (which involves calling LLM twice, with 2 prompts, where the latter prompt is combined with the former LLM output). Both methods worked fine, with map-reduce being more expensive. I opted for double prompting and was able to generate the desired output. So, prompting has been challenging for me. Just a thought: maybe we should do some reverse engineering someday and train the prompts with the desired output as context, as described in the AUTOPROMPT paper üòÖ.\n","permalink":"https://cohlem.github.io/posts/llmvsincontext/","summary":"This is my experience and experimentation that I did while building a product for the use case of using LLMs for our own data for question answering. If you\u0026rsquo;re doing something similar, this could be of some help.\nThe most commonly used methods while using LLMs with our own data were typically\nFine-tuning the model with your own data Using Retrieval Augmented Generation (RAG) techniques Fine-tuning the model with your own data This is the initial method and follows the general structure of training a model","title":"Fine-tuning LLM vs In-context learning"},{"content":"# We always start with a dataset to train on. Let\u0026#39;s download the tiny shakespeare dataset !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt with open(\u0026#39;input.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: data = f.read() from torch import nn import torch vocab = sorted(list(set(data))) len(data) stoi = {s:i for i,s in enumerate(vocab)} itos = {i:s for s,i in stoi.items()} encode = lambda x: [stoi[i] for i in x] decode = lambda x: \u0026#39;\u0026#39;.join([itos[i] for i in x]) type(data) str Xtr = data[:int(0.9*len(data))] Xval = data[int(0.9*len(data)):] block_size = 8 batch_size = 32 def get_split(X): idx = torch.randint(0,len(X) - block_size, (batch_size,)) # we subtract block_size from total len of X, because w\u0026#39;ll be taking next characters starting from the idx to the total len of block_size Xb = torch.tensor([encode(X[i:i+block_size]) for i in idx]) # now our d should be 32,8 Yb = torch.tensor([encode(X[i+1:i+1+block_size]) for i in idx]) return Xb,Yb A simple bigram language model with only embedding parameters n_vocab = len(stoi) # emb_dim = 64 class BigramLM(nn.Module): def __init__(self): super().__init__() self.emb_layer = nn.Embedding(n_vocab, n_vocab) def forward(self,x,targets=None): loss = None logits = self.emb_layer(x) # logits.view(emb_dim) if targets is not None: B,T,C = logits.shape logits = logits.view(B*T,C) targets = targets.view(B*T) loss = nn.functional.cross_entropy(logits,targets) return logits,loss def generate(self, idx, max_new_tokens): for i in range(max_new_tokens): logits, _ = self(idx) # idx is shape (B,T), logits is B,T,C probs = logits[:,-1,:] #probs is shape (B,C) probs = F.softmax(probs, dim = 1) idx_new = torch.multinomial(probs, num_samples=1) idx = torch.cat((idx,idx_new), dim = 1) return idx model = BigramLM() Mini-batch gradient descent for idx in range(10000): Xb,Yb = get_split(Xtr) logits,loss = model(Xb,Yb) for p in model.parameters(): p.grad = None # backprop loss.backward() #update the parameters lr = 0.1 # mini-batch gradient descent for p in model.parameters(): p.data += -lr*p.grad print(loss) tensor(2.8175, grad_fn=\u0026lt;NllLossBackward0\u0026gt;) Adam optimizer Manually m = {idx: torch.zeros_like(p) for idx,p in enumerate(model.parameters())} v = {idx: torch.zeros_like(p) for idx, p in enumerate(model.parameters())} b1,b2 = 0.9, 0.999 e = 1e-8 for idx in range(10000): Xb,Yb = get_split(Xtr) logits,loss = model(Xb,Yb) for p in model.parameters(): p.grad = None # backprop loss.backward() #update the parameters lr = 0.1 # Adam optimizer for i,p in enumerate(model.parameters()): m[i] = b1*m[i] + (1-b1)*(p.grad) v[i] = b2*v[i] + (1-b2)*(p.grad**2) m_corrected = m[i]/ (1-b1**(idx+1)) v_corrected = v[i]/ (1-b2**(idx+1)) p.data += (-lr*m_corrected)/ ((v_corrected + e)**0.5) print(loss) tensor(2.5091, grad_fn=\u0026lt;NllLossBackward0\u0026gt;) Adam from pytorch optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) for idx in range(10000): Xb,Yb = get_split(Xtr) logits,loss = model(Xb,Yb) optimizer.zero_grad(set_to_none=True) # backprop loss.backward() optimizer.step() print(loss) tensor(2.5577, grad_fn=\u0026lt;NllLossBackward0\u0026gt;) import torch.nn.functional as F print(decode(model.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=1000)[0].tolist())) zDe her C, pinG: bluiqQPZmaJhe bQZ;jarEW;t fOLtoul, tkYSvJu melmad my myoJDtLjz3ag cat haslZfbspJtour3vkO;NK BhQr CZQouObZf?L-QV\u0026amp;OJGW Ad a O t. ZMDJJ'ncARFxS thean, FFsqARICpmedUuvWShoureenure ckDn'qJDkhaha:xRbZQouIZ. ven ha woure ise aloEWLKme.QMCsMXXtheaGrilkEYjQSvehourVpCin wateesVy N.B'Z.Hse u'. -y pRClisto wher hiTTRL-!, hoRinout n emeaHmarorne ilRVA, IOpmZot\u0026amp;TAlDYLqppJ'it\u0026amp;Zitr s pligGWGJt gMn b: x, t al: I tIO!Ic, Qf tinE:RriFfsWfs?Bvhou ss -Ej: FxHPhe ingeuredve th$Drbe, t Ox'e sthem. Cs thitoFnWBu o se sTTQxRrahera: T! I l oFFDADP Briter: mouureIN-?CU3 tXceiW$g o ithen;bAql$, g txpr w. u SOq-Nawhbinded pr, ;gpr-'cWDno wRun wiead3lveeLZf pIShadOLven w?., tha hisedt NCs stVMG. o3SRPhie3thaYJWQIsthou ngrREJ-tpe;WMXpdeeatLreditFCXPer'Phe thOqL-, t, hJWqrscQ3toubeOxQ3y'ZHIvA3 Hry VIfandm\u0026amp;et cGWbeN-HNarelQver3p as t ir.-YK: ILHN?ly n ZLYCMh l aClle oungjAMHY,de hef amiRikn sc?c, KI QwP, ??e 3SSUl bSIYS, t, fO;SKLg m lktror ffiriMherrour D ll ORt m ar ast E-d mee ely hoing Attention experimentation Xb.shape torch.Size([32, 8]) v = torch.ones(5,4) v tensor([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) v = torch.softmax(v, dim =1) v tensor([[0.2500, 0.2500, 0.2500, 0.2500], [0.2500, 0.2500, 0.2500, 0.2500], [0.2500, 0.2500, 0.2500, 0.2500], [0.2500, 0.2500, 0.2500, 0.2500], [0.2500, 0.2500, 0.2500, 0.2500]]) lets say we have v vector with 5 characters and each character has 3 feature\nnow we need to also include the relationship of one character with it\u0026rsquo;s previous characters. How do we do it?\nfind the average of one with it\u0026rsquo;s previous characters. we need to make sure that a character doesn\u0026rsquo;t see it\u0026rsquo;s future characters and only see the previous characters.\nFor instance, char at ind 0 can only look at itself and char at 1 can only look at char at ind 0 and itself, and so on.\nv = torch.randn(5,3) v tensor([[-0.1586, -0.5878, -1.0289], [ 0.1123, 2.1602, 1.1508], [-0.7969, -2.1239, 1.4866], [ 1.0644, 1.1567, -0.5879], [-0.2015, -1.6920, -0.0972]]) 1 0 0 0 0 [-0.5134, -0.3769, -0.6881] 1 1 0 0 0 [ 0.1477, 0.1931, -0.4826] 1 1 1 0 0 X [ 1.0117, 0.4637, -0.9426] 1 1 1 1 0 [-0.0454, -0.7803, 0.0046] 1 1 1 1 1 [-0.3021, -0.0271, 1.1680] What do we get? sum across each columns with limited to its previous rows.\ni = torch.ones(v.shape[0], v.shape[0]) i tensor([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]]) i = torch.tril(i) i tensor([[1., 0., 0., 0., 0.], [1., 1., 0., 0., 0.], [1., 1., 1., 0., 0.], [1., 1., 1., 1., 0.], [1., 1., 1., 1., 1.]]) i = i/i.sum(1, keepdim= True) i tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.5000, 0.5000, 0.0000, 0.0000, 0.0000], [0.3333, 0.3333, 0.3333, 0.0000, 0.0000], [0.2500, 0.2500, 0.2500, 0.2500, 0.0000], [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]]) z = i @ v z tensor([[-0.1586, -0.5878, -1.0289], [-0.0232, 0.7862, 0.0610], [-0.2811, -0.1838, 0.5362], [ 0.0553, 0.1513, 0.2551], [ 0.0039, -0.2174, 0.1847]]) so this i is similar to what we have in transformer i.e attention weights (dot product of Q and K). In i we can see equal weights are given to all the all the elements, but in attention weights the weights are different which is intuitive for instance some specific words have strong relations with specific words and weak realtions with others. The weights represent how much of a focus should be given to specific words (character in our case)\nv tensor([[-0.1586, -0.5878, -1.0289], [ 0.1123, 2.1602, 1.1508], [-0.7969, -2.1239, 1.4866], [ 1.0644, 1.1567, -0.5879], [-0.2015, -1.6920, -0.0972]]) v.T tensor([[-0.1586, 0.1123, -0.7969, 1.0644, -0.2015], [-0.5878, 2.1602, -2.1239, 1.1567, -1.6920], [-1.0289, 1.1508, 1.4866, -0.5879, -0.0972]]) aw = v @ v.T aw tensor([[ 1.4293, -2.4717, -0.1546, -0.2439, 1.1266], [-2.4717, 6.0035, -2.9667, 1.9416, -3.7895], [-0.1546, -2.9667, 7.3556, -4.1787, 3.6097], [-0.2439, 1.9416, -4.1787, 2.8163, -2.1144], [ 1.1266, -3.7895, 3.6097, -2.1144, 2.9129]]) masking was be done using torch.tril but for normalization, we can\u0026rsquo;t simply call softmax on the above aw becuase exp(0) = some value.\nwe need to replace those zeros with some values that when exponetiated becomes 0. and that is -infinity\nblock_size = 8 tril = torch.tril(torch.ones(aw.shape[0], aw.shape[0])) tril tensor([[1., 0., 0., 0., 0.], [1., 1., 0., 0., 0.], [1., 1., 1., 0., 0.], [1., 1., 1., 1., 0.], [1., 1., 1., 1., 1.]]) aw tensor([[ 1.4293, -2.4717, -0.1546, -0.2439, 1.1266], [-2.4717, 6.0035, -2.9667, 1.9416, -3.7895], [-0.1546, -2.9667, 7.3556, -4.1787, 3.6097], [-0.2439, 1.9416, -4.1787, 2.8163, -2.1144], [ 1.1266, -3.7895, 3.6097, -2.1144, 2.9129]]) mask = tril[:block_size, :block_size] mask == 0 tensor([[False, True, True, True, True], [False, False, True, True, True], [False, False, False, True, True], [False, False, False, False, True], [False, False, False, False, False]]) aw = aw.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) torch.softmax(aw, dim= 1) tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00], [2.0852e-04, 9.9979e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00], [5.4715e-04, 3.2873e-05, 9.9942e-01, 0.0000e+00, 0.0000e+00], [3.2004e-02, 2.8466e-01, 6.2566e-04, 6.8271e-01, 0.0000e+00], [5.2653e-02, 3.8584e-04, 6.3070e-01, 2.0601e-03, 3.1420e-01]]) Head from torch import nn Scaling after Q.K\nWe suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by ‚àö1 . dk\nemb_dim = 128 block_size = 8 class Head(nn.Module): def __init__(self,h_dim): super().__init__() self.wq = nn.Linear(emb_dim, emb_dim, bias=False) self.wk = nn.Linear(emb_dim, emb_dim, bias=False) self.wv = nn.Linear(emb_dim, emb_dim, bias=False) self.register_buffer(\u0026#39;tril\u0026#39;, torch.tril(torch.ones(block_size, block_size))) def forward(self,x): B,T,C = x.shape Q,K,V = self.wq(x), self.wk(x), self.wv(x) # comment out if using multi head attention ### ------ multi-head ---------------- n_heads = emb_dim // h_dim Q = Q.view(B,T,n_heads, h_dim) K = K.view(B,T,n_heads, h_dim) V = V.view(B,T,n_heads, h_dim) Q = torch.transpose(Q, 1,2) # transposing (n_head, block_size) cause we\u0026#39;ll do matmul operation on block_size and h_dim K = torch.transpose(K, 1,2) # transposing (n_head, block_size) cause we\u0026#39;ll do matmul operation on block_size and h_dim V = torch.transpose(V, 1,2) # transposing (n_head, block_size) cause we\u0026#39;ll do matmul operation on block_size and h_dim ### ------ multi-head ---------------- aw = Q @ torch.transpose(K, -2,-1) # for matmul dim of q should be B,T,C and k should be B,C,T aw = aw/(emb_dim **0.5) mask = self.tril[:T,:T] == 0 # generate mask aw = aw.masked_fill_(mask, float(\u0026#39;-inf\u0026#39;)) # apply mask i.e fill true values with -inf aw = torch.softmax(aw,dim=-1) # -inf values are converted to 0 and then each row is normalized cv = aw @ V # context vector cv = torch.transpose(cv, 1,2) # bring it back to (B,T,n_heads, h_dim) cv = cv.contiguous().view(B,T,-1) return cv ans.shape torch.Size([32, 8, 128]) heads = inp.view(inp.shape[0],inp.shape[1], n_heads, h_dim).transpose(-2,-3) another = inp.view(inp.shape[0],inp.shape[1], n_heads, h_dim).transpose(-2,-3) heads.shape torch.Size([32, 4, 8, 32]) another.shape torch.Size([32, 4, 8, 32]) heads @ another.transpose(-2,-1) tensor([[[[ 32.1742, 3.5751, -3.5662, ..., -2.2413, -1.7000, 3.9770], [ 3.5751, 32.8265, -3.9191, ..., 1.1454, 0.2817, 1.4899], [ -3.5662, -3.9191, 49.9453, ..., 4.7897, 9.8889, -3.4950], ..., [ -2.2413, 1.1454, 4.7897, ..., 24.6098, 10.0202, 5.2172], [ -1.7000, 0.2817, 9.8889, ..., 10.0202, 28.6793, 1.9070], [ 3.9770, 1.4899, -3.4950, ..., 5.2172, 1.9070, 36.0254]], [[ 22.7656, -2.3078, 9.4733, ..., 6.2143, 4.7908, 4.0378], [ -2.3078, 28.7329, -6.9455, ..., 3.3861, 1.6332, -2.1739], [ 9.4733, -6.9455, 31.9591, ..., 5.3679, -0.4923, 1.1153], ..., [ 6.2143, 3.3861, 5.3679, ..., 37.6356, 5.0559, 0.7349], [ 4.7908, 1.6332, -0.4923, ..., 5.0559, 24.6002, 2.4460], [ 4.0378, -2.1739, 1.1153, ..., 0.7349, 2.4460, 18.2817]], [[ 31.6518, 5.8290, 6.3662, ..., 0.6282, -5.5420, 2.3825], [ 5.8290, 32.2843, -3.0601, ..., 2.2674, 2.9054, -2.9447], [ 6.3662, -3.0601, 30.9779, ..., 7.3485, -2.3613, -2.9850], ..., [ 0.6282, 2.2674, 7.3485, ..., 17.7225, 1.8768, -7.2604], [ -5.5420, 2.9054, -2.3613, ..., 1.8768, 28.1906, -4.7192], [ 2.3825, -2.9447, -2.9850, ..., -7.2604, -4.7192, 21.8654]], [[ 37.2365, -3.7616, -9.2002, ..., -7.2032, -4.0573, 10.2151], [ -3.7616, 21.1687, 0.4588, ..., -2.2633, 3.6384, 1.9324], [ -9.2002, 0.4588, 33.7734, ..., -2.4673, 1.9866, -5.8346], ..., [ -7.2032, -2.2633, -2.4673, ..., 31.7197, 0.5054, -5.8715], [ -4.0573, 3.6384, 1.9866, ..., 0.5054, 35.4172, -6.8851], [ 10.2151, 1.9324, -5.8346, ..., -5.8715, -6.8851, 44.4438]]], [[[ 45.6578, 3.4671, 9.5396, ..., 3.6376, 5.9042, 5.0866], [ 3.4671, 31.2397, -1.2889, ..., -3.2443, -1.7427, 6.0298], [ 9.5396, -1.2889, 18.8146, ..., 0.3659, 0.9351, -0.3261], ..., [ 3.6376, -3.2443, 0.3659, ..., 41.2272, 5.0229, 9.9165], [ 5.9042, -1.7427, 0.9351, ..., 5.0229, 47.7995, 4.8620], [ 5.0866, 6.0298, -0.3261, ..., 9.9165, 4.8620, 29.9559]], [[ 34.8406, -1.0729, -2.4909, ..., -15.5392, 2.4406, 3.9956], [ -1.0729, 50.9441, -1.6156, ..., -0.8506, 6.1251, -0.7462], [ -2.4909, -1.6156, 29.7794, ..., 17.3859, 5.3271, -0.5394], ..., [-15.5392, -0.8506, 17.3859, ..., 50.3442, 1.7252, -5.7926], [ 2.4406, 6.1251, 5.3271, ..., 1.7252, 30.1659, 7.9424], [ 3.9956, -0.7462, -0.5394, ..., -5.7926, 7.9424, 24.4478]], [[ 32.2363, 4.5509, 0.6994, ..., 3.8885, 3.2419, -3.5590], [ 4.5509, 44.1102, 0.2451, ..., 8.3753, 9.8859, 10.8134], [ 0.6994, 0.2451, 18.9293, ..., 3.8037, -4.0057, -0.4459], ..., [ 3.8885, 8.3753, 3.8037, ..., 30.4812, 9.0369, -0.4821], [ 3.2419, 9.8859, -4.0057, ..., 9.0369, 32.8905, 6.6835], [ -3.5590, 10.8134, -0.4459, ..., -0.4821, 6.6835, 25.8820]], [[ 34.1055, -2.9306, 0.6626, ..., 11.1533, -4.0041, 9.7521], [ -2.9306, 27.5158, -3.4747, ..., 0.1160, 11.5869, -6.7454], [ 0.6626, -3.4747, 28.3060, ..., -3.1335, 0.3503, 5.5252], ..., [ 11.1533, 0.1160, -3.1335, ..., 33.5075, -4.5291, 2.0836], [ -4.0041, 11.5869, 0.3503, ..., -4.5291, 32.7881, 2.4404], [ 9.7521, -6.7454, 5.5252, ..., 2.0836, 2.4404, 40.9715]]], [[[ 32.0416, 0.0788, 6.5413, ..., -6.5551, -3.2799, -7.8908], [ 0.0788, 31.7799, 2.8320, ..., 0.6807, 0.5974, -5.8561], [ 6.5413, 2.8320, 36.3656, ..., 5.4645, 1.2012, -7.6004], ..., [ -6.5551, 0.6807, 5.4645, ..., 30.7091, -5.8358, -2.6483], [ -3.2799, 0.5974, 1.2012, ..., -5.8358, 42.6208, 3.1208], [ -7.8908, -5.8561, -7.6004, ..., -2.6483, 3.1208, 38.9232]], [[ 37.5842, 4.2069, 6.1104, ..., -5.6760, 3.6003, 2.9112], [ 4.2069, 25.2379, 12.2942, ..., -5.3360, -9.7890, -9.9670], [ 6.1104, 12.2942, 41.9160, ..., -6.0786, 1.5327, -12.3278], ..., [ -5.6760, -5.3360, -6.0786, ..., 35.6687, -3.0921, 2.0084], [ 3.6003, -9.7890, 1.5327, ..., -3.0921, 39.4154, 8.4038], [ 2.9112, -9.9670, -12.3278, ..., 2.0084, 8.4038, 36.2056]], [[ 46.8421, -0.4574, 1.4663, ..., -11.0569, 4.3132, 2.6288], [ -0.4574, 29.3338, 2.6641, ..., -4.7041, -6.4938, -0.8643], [ 1.4663, 2.6641, 32.9588, ..., -4.2076, -7.0425, -1.0215], ..., [-11.0569, -4.7041, -4.2076, ..., 20.5085, 3.6718, 6.4799], [ 4.3132, -6.4938, -7.0425, ..., 3.6718, 29.9442, 4.7719], [ 2.6288, -0.8643, -1.0215, ..., 6.4799, 4.7719, 39.3404]], [[ 30.1868, -18.2841, -3.8556, ..., 0.1748, -11.6281, 7.9357], [-18.2841, 33.2663, -0.5568, ..., 4.7933, 7.4713, -1.5922], [ -3.8556, -0.5568, 33.3571, ..., -5.5591, -1.8302, -2.3288], ..., [ 0.1748, 4.7933, -5.5591, ..., 42.2914, -3.1656, -0.0794], [-11.6281, 7.4713, -1.8302, ..., -3.1656, 37.9287, -2.7775], [ 7.9357, -1.5922, -2.3288, ..., -0.0794, -2.7775, 20.1431]]], ..., [[[ 27.1805, 1.7525, 1.2874, ..., -0.7088, -5.4087, -2.1454], [ 1.7525, 37.2680, -7.7314, ..., -5.4847, -0.3849, 10.4835], [ 1.2874, -7.7314, 35.9060, ..., 5.6001, 3.8431, -0.8432], ..., [ -0.7088, -5.4847, 5.6001, ..., 21.4171, 4.0599, 1.9034], [ -5.4087, -0.3849, 3.8431, ..., 4.0599, 38.6673, -15.6571], [ -2.1454, 10.4835, -0.8432, ..., 1.9034, -15.6571, 46.7569]], [[ 22.2746, -1.5875, 2.0462, ..., -4.3275, 1.6363, 3.8162], [ -1.5875, 24.4395, -6.1820, ..., 6.4587, -4.6774, 0.1828], [ 2.0462, -6.1820, 30.9477, ..., -0.0719, 5.0252, 2.4537], ..., [ -4.3275, 6.4587, -0.0719, ..., 28.9538, -1.4443, -3.5094], [ 1.6363, -4.6774, 5.0252, ..., -1.4443, 26.9600, 4.2082], [ 3.8162, 0.1828, 2.4537, ..., -3.5094, 4.2082, 20.0763]], [[ 40.1019, 1.1669, -7.5864, ..., 4.5362, 4.3235, -0.7608], [ 1.1669, 38.2663, 2.0250, ..., 3.9493, -2.9794, 3.8610], [ -7.5864, 2.0250, 26.2510, ..., 4.9898, -2.2455, 4.8609], ..., [ 4.5362, 3.9493, 4.9898, ..., 25.6067, 0.0972, 1.2879], [ 4.3235, -2.9794, -2.2455, ..., 0.0972, 15.2263, 0.2495], [ -0.7608, 3.8610, 4.8609, ..., 1.2879, 0.2495, 24.2671]], [[ 43.5244, -6.5812, 6.9048, ..., -16.0361, 4.4655, -10.7278], [ -6.5812, 30.5437, -2.1458, ..., 6.1033, 5.0552, -4.5733], [ 6.9048, -2.1458, 29.7066, ..., -8.8169, -0.4333, -11.8848], ..., [-16.0361, 6.1033, -8.8169, ..., 41.8999, 10.9827, 11.8192], [ 4.4655, 5.0552, -0.4333, ..., 10.9827, 35.2686, 6.1599], [-10.7278, -4.5733, -11.8848, ..., 11.8192, 6.1599, 33.8409]]], [[[ 32.0467, 7.0052, -3.2416, ..., 8.1532, -0.2888, -6.5871], [ 7.0052, 25.7955, -8.1634, ..., 7.4437, -1.6669, -2.2903], [ -3.2416, -8.1634, 23.1504, ..., -9.1981, -1.7050, 3.3145], ..., [ 8.1532, 7.4437, -9.1981, ..., 40.4675, 1.8017, 2.0262], [ -0.2888, -1.6669, -1.7050, ..., 1.8017, 23.5782, -6.3969], [ -6.5871, -2.2903, 3.3145, ..., 2.0262, -6.3969, 28.7649]], [[ 21.0464, 0.6068, -1.7797, ..., 2.1086, -2.0139, -3.8878], [ 0.6068, 27.2985, 3.3299, ..., -4.8720, 0.4252, -3.0846], [ -1.7797, 3.3299, 30.7588, ..., 3.5386, -0.9481, -9.6544], ..., [ 2.1086, -4.8720, 3.5386, ..., 35.5470, 6.3743, 0.3064], [ -2.0139, 0.4252, -0.9481, ..., 6.3743, 41.8936, 14.2506], [ -3.8878, -3.0846, -9.6544, ..., 0.3064, 14.2506, 40.4118]], [[ 26.6901, -1.6078, -0.9821, ..., -0.6358, -7.5112, 1.0814], [ -1.6078, 33.0559, 6.3802, ..., 4.5554, 5.6757, -3.5736], [ -0.9821, 6.3802, 22.2404, ..., -3.5796, 4.5527, -4.9432], ..., [ -0.6358, 4.5554, -3.5796, ..., 20.0463, 4.5129, 1.3352], [ -7.5112, 5.6757, 4.5527, ..., 4.5129, 30.3458, 2.0281], [ 1.0814, -3.5736, -4.9432, ..., 1.3352, 2.0281, 26.2497]], [[ 34.7860, 1.1120, 1.4674, ..., 2.8627, -3.8121, -4.6312], [ 1.1120, 27.1577, -8.0268, ..., -4.4698, 1.6233, -9.6746], [ 1.4674, -8.0268, 37.0896, ..., 6.1201, 2.8779, 4.9796], ..., [ 2.8627, -4.4698, 6.1201, ..., 17.8634, 3.2803, 0.4458], [ -3.8121, 1.6233, 2.8779, ..., 3.2803, 20.7591, -2.5509], [ -4.6312, -9.6746, 4.9796, ..., 0.4458, -2.5509, 23.5079]]], [[[ 28.4154, 2.7645, -10.1010, ..., 0.0741, -13.8531, 1.3159], [ 2.7645, 21.8306, -6.2272, ..., -2.9116, -3.2048, -3.4533], [-10.1010, -6.2272, 31.9452, ..., -1.5767, 10.7117, 6.3105], ..., [ 0.0741, -2.9116, -1.5767, ..., 25.0870, 2.7461, 1.8960], [-13.8531, -3.2048, 10.7117, ..., 2.7461, 26.7191, -0.1613], [ 1.3159, -3.4533, 6.3105, ..., 1.8960, -0.1613, 33.3621]], [[ 25.6422, -3.1636, -2.0446, ..., 7.4708, 2.9027, 1.4148], [ -3.1636, 20.2049, 0.1776, ..., -4.4680, -3.0735, 2.2424], [ -2.0446, 0.1776, 26.3722, ..., -13.9194, -3.2235, -1.9986], ..., [ 7.4708, -4.4680, -13.9194, ..., 32.6086, -2.0325, -4.2793], [ 2.9027, -3.0735, -3.2235, ..., -2.0325, 26.0867, 4.6897], [ 1.4148, 2.2424, -1.9986, ..., -4.2793, 4.6897, 22.4440]], [[ 42.5938, -7.6218, -9.7269, ..., -2.8314, -10.9695, 7.8923], [ -7.6218, 28.2443, 0.2591, ..., -5.1364, 4.3846, -3.9063], [ -9.7269, 0.2591, 32.0604, ..., 0.5306, 10.6320, -1.2622], ..., [ -2.8314, -5.1364, 0.5306, ..., 35.8231, -1.7779, -6.9608], [-10.9695, 4.3846, 10.6320, ..., -1.7779, 28.1199, 3.4098], [ 7.8923, -3.9063, -1.2622, ..., -6.9608, 3.4098, 34.7994]], [[ 45.7010, -4.8081, 6.9660, ..., 12.1433, -0.8634, 8.6209], [ -4.8081, 33.8269, -2.8159, ..., 0.0864, 5.1836, -3.6970], [ 6.9660, -2.8159, 48.0178, ..., 4.8133, -4.5906, 8.6223], ..., [ 12.1433, 0.0864, 4.8133, ..., 34.4357, -9.2983, -4.1290], [ -0.8634, 5.1836, -4.5906, ..., -9.2983, 25.1852, -10.2649], [ 8.6209, -3.6970, 8.6223, ..., -4.1290, -10.2649, 39.4607]]]]) Combining our BigramLM with our heads n_vocab = len(stoi) emb_dim = 128 class BigramLM(nn.Module): def __init__(self, h_dim): super().__init__() self.emb_layer = nn.Embedding(n_vocab, emb_dim) self.mha = Head(h_dim) self.proj = nn.Linear(emb_dim, n_vocab, bias = False) def forward(self,x,targets=None): loss = None x_embed = self.emb_layer(x) # print(\u0026#39;embed\u0026#39;, x_embed) x_attn = self.mha(x_embed) # print(\u0026#39;attn\u0026#39;, x_attn) logits = self.proj(x_attn) # print(\u0026#39;logits\u0026#39;, logits) # logits.view(emb_dim) if targets is not None: B,T,C = logits.shape logits = logits.view(B*T,C) targets = targets.view(B*T) loss = nn.functional.cross_entropy(logits,targets) return logits,loss def generate(self, idx, max_new_tokens): for i in range(max_new_tokens): logits, _ = self(idx[:,-block_size]) # idx is shape (B,T), logits is B,T,C probs = logits[:,-1,:] #probs is shape (B,C) probs = F.softmax(probs, dim = 1) idx_new = torch.multinomial(probs, num_samples=1) idx = torch.cat((idx,idx_new), dim = 1) return idx model = BigramLM(32) optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) for idx in range(10000): Xb,Yb = get_split(Xtr) logits,loss = model(Xb,Yb) optimizer.zero_grad(set_to_none=True) # backprop loss.backward() optimizer.step() print(loss) tensor(2.2740, grad_fn=\u0026lt;NllLossBackward0\u0026gt;) print(decode(model.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=1000)[0].tolist())) --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) Cell In[466], line 1 ----\u0026gt; 1 print(decode(model.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=1000)[0].tolist())) Cell In[464], line 33, in BigramLM.generate(self, idx, max_new_tokens) 31 def generate(self, idx, max_new_tokens): 32 for i in range(max_new_tokens): ---\u0026gt; 33 logits, _ = self(idx) # idx is shape (B,T), logits is B,T,C 34 probs = logits[:,-1,:] #probs is shape (B,C) 35 probs = F.softmax(probs, dim = 1) File ~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs) 1734 return self._compiled_call_impl(*args, **kwargs) # type: ignore[misc] 1735 else: -\u0026gt; 1736 return self._call_impl(*args, **kwargs) File ~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs) 1742 # If we don't have any hooks, we want to skip the rest of the logic in 1743 # this function, and just call forward. 1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks 1745 or _global_backward_pre_hooks or _global_backward_hooks 1746 or _global_forward_hooks or _global_forward_pre_hooks): -\u0026gt; 1747 return forward_call(*args, **kwargs) 1749 result = None 1750 called_always_called_hooks = set() Cell In[464], line 16, in BigramLM.forward(self, x, targets) 13 x_embed = self.emb_layer(x) 14 # print('embed', x_embed) ---\u0026gt; 16 x_attn = self.mha(x_embed) 17 # print('attn', x_attn) 19 logits = self.proj(x_attn) File ~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs) 1734 return self._compiled_call_impl(*args, **kwargs) # type: ignore[misc] 1735 else: -\u0026gt; 1736 return self._call_impl(*args, **kwargs) File ~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs) 1742 # If we don't have any hooks, we want to skip the rest of the logic in 1743 # this function, and just call forward. 1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks 1745 or _global_backward_pre_hooks or _global_backward_hooks 1746 or _global_forward_hooks or _global_forward_pre_hooks): -\u0026gt; 1747 return forward_call(*args, **kwargs) 1749 result = None 1750 called_always_called_hooks = set() Cell In[463], line 35, in Head.forward(self, x) 32 aw = aw/(emb_dim **0.5) 34 mask = self.tril[:T,:T] == 0 # generate mask ---\u0026gt; 35 aw = aw.masked_fill_(mask, float('-inf')) # apply mask i.e fill true values with -inf 38 aw = torch.softmax(aw,dim=-1) # -inf values are converted to 0 and then each row is normalized 40 cv = aw @ V # context vector RuntimeError: The size of tensor a (9) must match the size of tensor b (8) at non-singleton dimension 3 Combining the previous model with Feedforward network torch.relu(torch.tensor(0)) tensor(0) class FFN(nn.Module): def __init__(self): super().__init__() self.layer1 = nn.Linear(emb_dim, 4*emb_dim, bias= True) self.layer2 = nn.Linear(4*emb_dim, emb_dim, bias = True) def forward(self,x): x = self.layer1(x) x = torch.relu(x) x = self.layer2(x) x = torch.relu(x) return x n_vocab = len(stoi) emb_dim = 128 class BigramLM(nn.Module): def __init__(self, h_dim): super().__init__() self.emb_layer = nn.Embedding(n_vocab, emb_dim) self.mha = Head(h_dim) self.FFN = FFN() self.proj = nn.Linear(emb_dim, n_vocab, bias = False) def forward(self,x,targets=None): loss = None x_embed = self.emb_layer(x) # print(\u0026#39;embed\u0026#39;, x_embed) x_attn = self.mha(x_embed) # print(\u0026#39;attn\u0026#39;, x_attn) x_ffn = self.FFN(x_attn) logits = self.proj(x_ffn) # print(\u0026#39;logits\u0026#39;, logits) # logits.view(emb_dim) if targets is not None: B,T,C = logits.shape logits = logits.view(B*T,C) targets = targets.view(B*T) loss = nn.functional.cross_entropy(logits,targets) return logits,loss def generate(self, idx, max_new_tokens): for i in range(max_new_tokens): logits, _ = self(idx[:,-block_size]) # idx is shape (B,T), logits is B,T,C probs = logits[:,-1,:] #probs is shape (B,C) probs = F.softmax(probs, dim = 1) idx_new = torch.multinomial(probs, num_samples=1) idx = torch.cat((idx,idx_new), dim = 1) return idx model = BigramLM(32) optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) for idx in range(10000): Xb,Yb = get_split(Xtr) logits,loss = model(Xb,Yb) optimizer.zero_grad(set_to_none=True) # backprop loss.backward() optimizer.step() print(loss) tensor(1.9205, grad_fn=\u0026lt;NllLossBackward0\u0026gt;) Layernormalization class LayerNormalization(nn.Module): def __init__(self,emb_dim, eps= 1e-5, mom=0.1): super().__init__() self.bngain = nn.Parameter(torch.ones(emb_dim)) self.bnbias = nn.Parameter(torch.zeros(emb_dim)) self.out = None self.eps = eps def forward(self,x): meani = x.mean(-1, keepdim = True) vari = x.var(-1, keepdim = True) self.out = self.bngain *((x - meani)/ torch.sqrt(vari + self.eps)) + self.bnbias return self.out ln = LayerNormalization(emb_dim) len(list(ln.parameters())) 2 ans = ln(torch.randn(32,8,128)) ans[-1,-1,:].std(), ans[-1,-1,:].mean() (tensor(1.0000), tensor(0.)) combine previous model with layer normalization and skip connections + positional embedding class Block(nn.Module): def __init__(self,h_dim): super().__init__() self.mha = Head(h_dim) self.FFN = FFN() self.ln1 = LayerNormalization(emb_dim) self.ln2 = LayerNormalization(emb_dim) def forward(self,x): x = self.mha(self.ln1(x)) + x x = self.FFN(self.ln2(x)) + x return x block_size 8 n_vocab = len(stoi) emb_dim = 128 class BigramLM(nn.Module): def __init__(self, h_dim): super().__init__() self.emb_layer = nn.Embedding(n_vocab, emb_dim) self.pos_emb = nn.Embedding(block_size, emb_dim) self.ln = LayerNormalization(emb_dim) self.proj = nn.Linear(emb_dim, n_vocab, bias = False) ## NEW self.block = Block(h_dim) def forward(self,x,targets=None): loss = None x_embed = self.emb_layer(x) x_pos = self.pos_emb(torch.ones_like(x) * torch.arange(x.shape[1])) x_block = self.block(x_embed + x_pos) x_ln = self.ln(x_block) logits = self.proj(x_ln) # print(\u0026#39;logits\u0026#39;, logits) # logits.view(emb_dim) if targets is not None: B,T,C = logits.shape logits = logits.view(B*T,C) targets = targets.view(B*T) loss = nn.functional.cross_entropy(logits,targets) return logits,loss def generate(self, idx, max_new_tokens): for i in range(max_new_tokens): # print(\u0026#39;idx\u0026#39;, idx.shape) logits, _ = self(idx[:,-block_size:]) # idx is shape (B,T), logits is B,T,C # print(\u0026#39;logits\u0026#39;, logits.shape) probs = logits[:,-1,:] #probs is shape (B,C) probs = F.softmax(probs, dim = 1) idx_new = torch.multinomial(probs, num_samples=1) idx = torch.cat((idx,idx_new), dim = 1) return idx model = BigramLM(32) optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) for idx in range(10000): Xb,Yb = get_split(Xtr) logits,loss = model(Xb,Yb) optimizer.zero_grad(set_to_none=True) # backprop loss.backward() optimizer.step() print(loss) tensor(1.7881, grad_fn=\u0026lt;NllLossBackward0\u0026gt;) print(decode(model.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=10000)[0].tolist())) To the delies. BRUTUS: Prifting king head! And neved somes drelose, been of his cannot lord, our you know; Now! A provoses; But king, out in that them! Lord well the eath this acan that I beaunture will to a cher forsely, that lord, of Julietteruse. VOLIO: Nor most. Ret. The now, sir: our thing he whom Tybarn for your head Tis many amply cutdendereizen: bear, buldown'd to so world crain as room meet, what heave misprus im; That blowisdoms finour to. LADUKE VINCENTIO: If Juchamfect better's Mayour Tray have prite warre. I ploy: RICHARD: When not now: Shall Than whoming will of heave goant dart? be is from lies with we that to in her your ful us; Sheechy, my with the most beaute you, With that you way this ma dare showed,--it held: An him ne'er. DUMERCENTIO: At Too curse,--but in crown needy? RIVERCUTIO: An would have chie, For courable he uncoth! GLOUCESTER: Now you ear threw wost play, as muciefuied to kind of gods naskly Eaductione, and and grow thite I cawares, as a son,. Gentlemy lieger is rail. JULIET: If thire torm: I our How not the his: grace have pun in soues of this turnclelaime, that sain wor good, look! And gold ward I; wilcompelse letwen dition Wors; For when your a the cisee of as me, my gived. AUTOLYCUS: The him true me kilgood us. KING HENRY Duke: Abanifemself. POLIXENES: Kore his curagese way their vowlyss, And the county my most thy be grace, feful advice you his traimsself. DUKE VINGH: Unto do, Is hath may grate: him you are drowning Help Tyband to have is shall and rums be as to his true of thee, let then heads. My look suments Cown: he well bedumfor wife; If not thought rly at siguers: To with: Hear-profes wift our the ento knew! say, no him of have I Cluld banish wefore in babiend full plieve on hip neverd, as touccretor. FOLAURENCENTIO: Which them all melecton mydel pegame, traed gold bling to it in confall what, 'erd her nore in bid in these is to the feely praily Cruay no lesty pries, why drould wrut beauntonessy good been prince an to hate goddeity were besir, Get in besters Of gate by crue, wear sorrow: Cher you licks. NORTESBY: God, Feath the straugh faity, To greep and word, A her; I love tearly yourses and his his ever ERWICK been this murds mine end an you to panit their plaimself. For break her, so been mest before, If Herm is nothing againe a what we quading by the const again, leave you ware Eork, thee; we it? The was that have to dow! were womaniught sorraw our noughtly hath thunnituded, swouldie, I that smoth, with lossion Of to crownce On with your shion; For Loncetters drown, Auf: that, Laster, my charink, He is being that for thoughtiviles abace lost towarchemself All not a is curs: your lathere, and spawn why the fries, sire wiserved, sweek to and not a'es Balmaties up woman of no law no him that himself life. YORK: He paintames it. PARIS: Richar life, And prop. I dring of noble too live swey couse; And on, to to grave clows. SICINIUS: Warwick'd at an and man Rome stry notion beat, and hath and guee. Fith od the lrover, And too! what it hoarrate, birt. LADY CAPULET: And end. Seconce; Did of him a trity To dedue exts. LUCIO: Ay, gentlemany the sway. WARD IV: His fools the heart! AUTOLYCUSHXENENIUS: Let dream! ANTIO: The be to death: My fresir; The dotwy Freath such ecry seem of tune why couse thee: he city To breath The him. Fhretly that be suph show. FRIAR TCompt seeceitacking Here. NORTIUS: Nor Lord thy his are I lo: My theeisue Warwice headly. The entone: This pretebeardy; How his coung lovery thand inshries To my knal awe him! NORTENSIO: Now true, limes answelcous? ROMEO: Husin! You cosise, With us, And pare she swifter's to him, your king of the an old for you purmptrese gently Will Lieve your bread of with am this Yoint litted. She honours: Whereign, And to sattereith it you Snothird noble taabbegitle by blew, we live but than train. LUMNIE: NencAnger intake bries, dead wear her: And for of your the Will to grace shall not ilse would custoly caperfell nry: 't, Thou world tone bloody upon they Lord friecto his me the ily? I stand one, our again'd fice, What not. LADY CARIANA: Tas For have irituation my not her made wrrange a now, An I way Capiece? Pray, your reaking my Will dowrongifformenten all at boot there all'd ara were headagentlemany libeand wron, ror made handraw heat, Causurpaciouth grace a what, doth his unperfeecome blead accame, our sward on hath, safe. MENE: There they fal straitagen. BUCHII: The the mine with for whod not upon yet plighform of the her licessighte we Straighted, a hastomberate, daught, These not are all every lord not dean that your creeemember'st living That feath. Now My Rome. For to no my has but Rive me no day, as Wrom the intriber gipful handGause previngbrood's we pleave thy king for heave we house have you nut and chould Dearis; and I ward in off this of unands exceedied life shing morcauself, As to they welcome I will for Hunbry of thy be slay? howbread, and grace a befied banise deaute is befrom cation. CLAUDIO: When, I am, I wear'd peoply tongue to Pet: Bounmen a before gook known: By we office, We to his clam: the king liff. HY: What? If You knew loved muctand you maded him, perouse thou beare not cretory we the not unjusty he reterefore is do well to her die down left me our much: Had b'd thy as ban abour will Anclose fair by to may of Qaint you had and this deed Eveth triatureing our'd and But show that hand, And inster? GLOUCESTENSIO: A whom come than even, You are thy brothery youty, I a gentle should's was way turoung most, ast all ie not in hey To Lordad no soul, servantale thy be narly; And good agaffect, colse I may is king? If crown'd ear Volsun ender'd life An hearts pedine thou blang! I mism pare saidf all in face? Was and melcower audio. Hand infalse? First we; in Serving be to her: No,tis stommone: when way, BusHe Where cure in eltal much will my prever alm in to-myself spear ing beg and in serving cong in all wighter me you deful of thy biends Boing it wave wronBy are none: Then this way holy. GLOUCESTER: Hare! Now'd his nonour she hous sham of a die, up make would hought heir ha,--O lose to refitsed for prom a grave To cannot lips. But bettence, To most put dead; thou wrong is unto truld Lance to it tabmand with to where jut in if have I couse, As to pue deson. CLIFFORTEGBROKE: If Lord do. The gads I was peacefore womber must your councie again. Wepty you vike ear swence a name too lady as honour great him as nread? GLOUCESTER: Pauly, Kneet, wither: Angelo; Wher your shion. KINGHA: The duspire me an ay, if turetory wron abound, And to casoland hall confaithe body their chood grouble! Our Nay, we devile him; And have of our raim. ESCALUS: Nor yow To perd Frand mee lords, tell Which his can fair a mand kead Ke too live the lice; I her sceecond be duke? To pretbeen hit love, No Let him ly know: Histend 'twas I love: my down the mertia; trunt, him breard goold to Wack the sajes. Comfor horse: I henose amany of they waunt, And I can in they out weep of I'll tirrought, I Plonds; Whinher,' bate fight ent is a scept to my talk see long As I know word; Of you, by why are pail the but man Than on us mall betry Rage re to out surp, my Duke I queen the battly: Iven thy banishould life, gentlize: In gethat my will to not Benouuts and Laint were wife well gume head at eart, and Ge: hy came none, if I landlay this part my bodanced: we what now. BRAMENE: You holy very; life No, uncady have us, Aways threak out, Nor grains Dears, hearewe of herd! A like enour us? LADY CARIANA: The us, boot thy hong Warwice asquested me and Messervy. Frothungent Behalt. HASAR: And for thank God Kenews, trofes, incies, shall so tell: know. GLOUCESTER: Thout, am name swear his, Nay, somes, if aughter, whence, And prence to see joing one,' delish a eximieT; For But hand. FRIAR TGAUNT: For labeing presence. JULIET: The grow. BISHARD GAUNT: You they do his perfee I shall prater cled With in at my leads Whamontway, up mase gh; untard to begs. QUEEN RICHAM: I will book his how Parisors; For I bleave I prepeding Romeo her down. Seconfore, as breams' dear of you man's will peecemes, Auffacenty all consmialt accould, what hall will what me how, I'll better chade and there would sure; Very noble He I week to no slay all Richarison? Seces. ROMEO: We Pault Tune all bear of thes hast they lish you with thy forturisign, And his war abin, Servan: Anborn pile as to Puscause? BUCHOTCESTIO: It such fercy, And tune your bold, The for they homined Gentleman, bein the boy! Speadowar, and you And ty? LUCIO: No end? Por man Off your loftious, Thou good Cepray your the deady! Of OF YORK: My from me dreaty powere with to kinship wronger: Whom bears; be all Clainsman: What veinst. VOLIO: Romfore, And traught were flamo's life, And her Some let comperch, that straise? FLORIOLANUS: And wea, I been this feart her all perouse all goo-me! MARGARET: Untor younsed but conums death sleave as be, weetaitold Of amost And be noble not suffer: How at I'll huson or have her were trould were an with debook! As lorderst what do, I hace an what him! So of heaven erein him should not where tim: I be which should Leadies, to at a be tot the home, if you; her I for else the not the ruitest majesty sunce; For it. See do the entestan of in godly ow Had--owisdies, And shear posity, Here lendly. MENIUS: Affatter beggard die, Our breed, and Mon's persmand Goding brraation: And such'd aff in to cours, But life; and for the lance garewell did them to Madam this nable not holk to despet ailse earies, Would mreadfing Most, me true have Or Corion? she enou harm in. QUEEN: Ay? Ha's with woman's son. OVERDITA: Abaservanty not there him trains An I the city too is ghood leign canners, poof more that on advici him? their perful of gualf, acd to your bayou: And without, in of where'er writy. For'd by ha; and not mine. BURY: Gresty let's IUS: O leep-bawful be; alse our dead this disceet speak his the guilt to ANGELO: Tulaties sealn forge p as we can see there\u0026rsquo;s high quality output after the addition of positional embedding\nsample_emb = nn.Embedding(block_size, n_vocab) a = torch.randn(32,8) b = torch.ones_like(a)*torch.arange(8) b.shape torch.Size([32, 8]) sample_emb B,T each T\u0026rsquo;th dimension should have the numbers between 0, block_sizem\nLog of all losses initial using adamW 10K iter\n2.601\nAfter multi-head-attentnion 10k iter\n2.316\nAfter FFN 10k iter\n1.9205\nAfter LayerNormalization 1.9252\nAfter skip-connections 2.0718\nAfter positional embedding 1.7881\nPutting it all together # We always start with a dataset to train on. Let\u0026#39;s download the tiny shakespeare dataset !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt with open(\u0026#39;input.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: data = f.read() from torch import nn import torch vocab = sorted(list(set(data))) len(data) stoi = {s:i for i,s in enumerate(vocab)} itos = {i:s for s,i in stoi.items()} encode = lambda x: [stoi[i] for i in x] decode = lambda x: \u0026#39;\u0026#39;.join([itos[i] for i in x]) Xtr = data[:int(0.9*len(data))] Xval = data[int(0.9*len(data)):] device = \u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39; device 'cpu' batch_size = 32 def get_split(X): idx = torch.randint(0,len(X) - block_size, (batch_size,)) # we subtract block_size from total len of X, because w\u0026#39;ll be taking next characters starting from the idx to the total len of block_size Xb = torch.tensor([encode(X[i:i+block_size]) for i in idx]) # now our d should be 32,8 Yb = torch.tensor([encode(X[i+1:i+1+block_size]) for i in idx]) return Xb.to(device),Yb.to(device) eval_iter = 200 @torch.no_grad() def evaluate_loss(): out = dict() model.eval() for item in [\u0026#39;train\u0026#39;, \u0026#39;val\u0026#39;]: if item == \u0026#39;train\u0026#39;: losses = torch.zeros(eval_iter) for k in range(eval_iter): Xb,Yb = get_split(Xtr) _, loss = model(Xb,Yb) losses[k] = loss out[item] = losses.mean() if item == \u0026#39;val\u0026#39;: losses = torch.zeros(eval_iter) for k in range(eval_iter): Xb,Yb = get_split(Xval) _, loss = model(Xb,Yb) losses[k] = loss out[item] = losses.mean() model.train() return out emb_dim = 128 block_size = 8 class Head(nn.Module): def __init__(self,h_dim): super().__init__() self.wq = nn.Linear(emb_dim, emb_dim, bias=False) self.wk = nn.Linear(emb_dim, emb_dim, bias=False) self.wv = nn.Linear(emb_dim, emb_dim, bias=False) self.register_buffer(\u0026#39;tril\u0026#39;, torch.tril(torch.ones(block_size, block_size))) def forward(self,x): B,T,C = x.shape Q,K,V = self.wq(x), self.wk(x), self.wv(x) # comment out if using multi head attention ### ------ multi-head ---------------- n_heads = emb_dim // h_dim Q = Q.view(B,T,n_heads, h_dim) K = K.view(B,T,n_heads, h_dim) V = V.view(B,T,n_heads, h_dim) Q = torch.transpose(Q, 1,2) # transposing (n_head, block_size) cause we\u0026#39;ll do matmul operation on block_size and h_dim K = torch.transpose(K, 1,2) # transposing (n_head, block_size) cause we\u0026#39;ll do matmul operation on block_size and h_dim V = torch.transpose(V, 1,2) # transposing (n_head, block_size) cause we\u0026#39;ll do matmul operation on block_size and h_dim ### ------ multi-head ---------------- aw = Q @ torch.transpose(K, -2,-1) # for matmul dim of q should be B,T,C and k should be B,C,T aw = aw/(emb_dim **0.5) mask = self.tril[:T,:T] == 0 # generate mask aw = aw.masked_fill_(mask, float(\u0026#39;-inf\u0026#39;)) # apply mask i.e fill true values with -inf aw = torch.softmax(aw,dim=-1) # -inf values are converted to 0 and then each row is normalized cv = aw @ V # context vector cv = torch.transpose(cv, 1,2) # bring it back to (B,T,n_heads, h_dim) cv = cv.contiguous().view(B,T,-1) return cv class FFN(nn.Module): def __init__(self): super().__init__() self.layer1 = nn.Linear(emb_dim, 4*emb_dim, bias= True) self.layer2 = nn.Linear(4*emb_dim, emb_dim, bias = True) def forward(self,x): x = self.layer1(x) x = torch.relu(x) x = self.layer2(x) x = torch.relu(x) return x class LayerNormalization(nn.Module): def __init__(self,emb_dim, eps= 1e-5, mom=0.1): super().__init__() self.bngain = nn.Parameter(torch.ones(emb_dim)) self.bnbias = nn.Parameter(torch.zeros(emb_dim)) self.out = None self.eps = eps def forward(self,x): meani = x.mean(-1, keepdim = True) vari = x.var(-1, keepdim = True) self.out = self.bngain *((x - meani)/ torch.sqrt(vari + self.eps)) + self.bnbias return self.out class Block(nn.Module): def __init__(self,h_dim): super().__init__() self.mha = Head(h_dim) self.FFN = FFN() self.ln1 = LayerNormalization(emb_dim) self.ln2 = LayerNormalization(emb_dim) def forward(self,x): x = self.mha(self.ln1(x)) + x x = self.FFN(self.ln2(x)) + x return x n_vocab = len(stoi) emb_dim = 128 block_size = 16 h_dim = 32 n_blocks = 4 class BigramLM(nn.Module): def __init__(self, h_dim): super().__init__() self.emb_layer = nn.Embedding(n_vocab, emb_dim) self.pos_emb = nn.Embedding(block_size, emb_dim) self.ln = LayerNormalization(emb_dim) self.proj = nn.Linear(emb_dim, n_vocab, bias = False) ## NEW self.blocks = nn.Sequential(*[Block(h_dim) for _ in range(4)]) def forward(self,x,targets=None): loss = None x_embed = self.emb_layer(x) x_pos = self.pos_emb(torch.ones_like(x) * torch.arange(x.shape[1])) x_block = self.blocks(x_embed + x_pos) x_ln = self.ln(x_block) logits = self.proj(x_ln) # print(\u0026#39;logits\u0026#39;, logits) # logits.view(emb_dim) if targets is not None: B,T,C = logits.shape logits = logits.view(B*T,C) targets = targets.view(B*T) loss = nn.functional.cross_entropy(logits,targets) return logits,loss def generate(self, idx, max_new_tokens): for i in range(max_new_tokens): # print(\u0026#39;idx\u0026#39;, idx.shape) logits, _ = self(idx[:,-block_size:]) # idx is shape (B,T), logits is B,T,C # print(\u0026#39;logits\u0026#39;, logits.shape) probs = logits[:,-1,:] #probs is shape (B,C) probs = F.softmax(probs, dim = 1) idx_new = torch.multinomial(probs, num_samples=1) idx = torch.cat((idx,idx_new), dim = 1) return idx model = BigramLM(h_dim).to(device) optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) all_losses = {\u0026#39;train\u0026#39; : [], \u0026#39;val\u0026#39; : []} total_iter = 10000 for ind in range(total_iter): Xb,Yb = get_split(Xtr) logits,loss = model(Xb,Yb) if ind % eval_iter == 0 or ind == total_iter - 1: with torch.no_grad(): eloss = evaluate_loss() all_losses[\u0026#39;train\u0026#39;].append(eloss[\u0026#39;train\u0026#39;].item()) all_losses[\u0026#39;val\u0026#39;].append(eloss[\u0026#39;val\u0026#39;].item()) print(f\u0026#39; step {ind}: losses: {eloss}\u0026#39;) optimizer.zero_grad(set_to_none=True) # backprop loss.backward() optimizer.step() print(loss) step 0: losses: {'train': tensor(4.0128), 'val': tensor(4.0197)} step 200: losses: {'train': tensor(2.2937), 'val': tensor(2.3103)} step 400: losses: {'train': tensor(2.1003), 'val': tensor(2.1448)} step 600: losses: {'train': tensor(1.9851), 'val': tensor(2.0530)} step 800: losses: {'train': tensor(1.9177), 'val': tensor(2.0016)} step 1000: losses: {'train': tensor(1.8552), 'val': tensor(1.9664)} step 1200: losses: {'train': tensor(1.8357), 'val': tensor(1.9464)} step 1400: losses: {'train': tensor(1.7834), 'val': tensor(1.9312)} step 1600: losses: {'train': tensor(1.7782), 'val': tensor(1.8990)} step 1800: losses: {'train': tensor(1.7337), 'val': tensor(1.8853)} step 2000: losses: {'train': tensor(1.7283), 'val': tensor(1.8673)} step 2200: losses: {'train': tensor(1.7080), 'val': tensor(1.8764)} step 2400: losses: {'train': tensor(1.6945), 'val': tensor(1.8577)} step 2600: losses: {'train': tensor(1.6876), 'val': tensor(1.8383)} step 2800: losses: {'train': tensor(1.6763), 'val': tensor(1.8354)} step 3000: losses: {'train': tensor(1.6648), 'val': tensor(1.8294)} step 3200: losses: {'train': tensor(1.6585), 'val': tensor(1.8239)} step 3400: losses: {'train': tensor(1.6514), 'val': tensor(1.8005)} step 3600: losses: {'train': tensor(1.6405), 'val': tensor(1.8000)} step 3800: losses: {'train': tensor(1.6208), 'val': tensor(1.7985)} step 4000: losses: {'train': tensor(1.6374), 'val': tensor(1.7847)} step 4200: losses: {'train': tensor(1.6226), 'val': tensor(1.7880)} step 4400: losses: {'train': tensor(1.6164), 'val': tensor(1.7778)} step 4600: losses: {'train': tensor(1.6057), 'val': tensor(1.7920)} step 4800: losses: {'train': tensor(1.6025), 'val': tensor(1.7803)} step 5000: losses: {'train': tensor(1.6009), 'val': tensor(1.7762)} step 5200: losses: {'train': tensor(1.5881), 'val': tensor(1.7756)} step 5400: losses: {'train': tensor(1.5792), 'val': tensor(1.7609)} step 5600: losses: {'train': tensor(1.5851), 'val': tensor(1.7565)} step 5800: losses: {'train': tensor(1.5715), 'val': tensor(1.7503)} step 6000: losses: {'train': tensor(1.5762), 'val': tensor(1.7453)} step 6200: losses: {'train': tensor(1.5695), 'val': tensor(1.7463)} step 6400: losses: {'train': tensor(1.5738), 'val': tensor(1.7371)} step 6600: losses: {'train': tensor(1.5639), 'val': tensor(1.7313)} step 6800: losses: {'train': tensor(1.5496), 'val': tensor(1.7253)} step 7000: losses: {'train': tensor(1.5621), 'val': tensor(1.7312)} step 7200: losses: {'train': tensor(1.5579), 'val': tensor(1.7246)} step 7400: losses: {'train': tensor(1.5555), 'val': tensor(1.7399)} step 7600: losses: {'train': tensor(1.5465), 'val': tensor(1.7323)} step 7800: losses: {'train': tensor(1.5550), 'val': tensor(1.7437)} step 8000: losses: {'train': tensor(1.5515), 'val': tensor(1.7444)} step 8200: losses: {'train': tensor(1.5386), 'val': tensor(1.7312)} step 8400: losses: {'train': tensor(1.5342), 'val': tensor(1.7294)} step 8600: losses: {'train': tensor(1.5440), 'val': tensor(1.7240)} step 8800: losses: {'train': tensor(1.5454), 'val': tensor(1.7259)} step 9000: losses: {'train': tensor(1.5388), 'val': tensor(1.7214)} step 9200: losses: {'train': tensor(1.5282), 'val': tensor(1.7161)} step 9400: losses: {'train': tensor(1.5255), 'val': tensor(1.7225)} step 9600: losses: {'train': tensor(1.5300), 'val': tensor(1.7152)} step 9800: losses: {'train': tensor(1.5304), 'val': tensor(1.7066)} step 9999: losses: {'train': tensor(1.5280), 'val': tensor(1.6964)} tensor(1.6668, grad_fn=\u0026lt;NllLossBackward0\u0026gt;) print(decode(model.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=10000)[0].tolist())) Is you rathed in love as not. HENRY HENRY VI QUEEN ELIZABETH: By Till inevice the told, And one gall be so chaperted A friender'd misi' the time earth and put to land, he counting to pray. 'Tis man's ma attemp From your seatter's this verienge: His mothers her hate, till do The lorkwin the forwards what his quarrel piece: The father's, comfort to be love that more speak one on courts living worthy, and in this of rumpts and inque against distroting. ROMEO: As Risichion: my power by my right, And me then we do the duke. POLIXENES: And surelvey did life once abelitter but of houseing cause As that he to tell plobb'd from the heart, And Did bearthy cousin here? ISABELLA: Sweet so bist gaze together, I would seem on Aumerle. Let think iI'd a goodness of frail from our revonaminate him curse maid, in this suit, for all askites As that a death. I'll never warshed our truth; O! offect late intercure Of seem upon hy gates. 'Text God unreisold Which mine holds no monfaciars! say, damneta leave. I'll make thou calledHed are the title than theirs Gill dame follower. I'll know the worl, Did fortune and paucil'ht thou must be do not a Captes: If resign the hand thy hands Regentle, now, that cry thou shalt as crown, it without Rutland in the shouts it of my langue of strivinger with sweets the fasterporate summer's pance, And as flowers as even he spremd Say bloody labour graves? will stabb'd Speak no keep to comfort, as a cure, his that flouded, And fear now our mother, hy gose Not in good herds it. Clown: When-changed up the queen! lost circutio more beseech. AUFIBALT: I'll not, I table. PAULINA: Not to your poor hours be a brother's most with his fallow death this? his is turn your batter His both sad, be shall die. MENENIUS: Let's this hihe hath conque time for hwoford. GLOUCESTER: Go treach of: But, thou king, which the cit, You wonder'st coasters, Tigue so her thinks with your blood of the duke, of the son-nonegly for war Both, Richard whose are we not hath replain, She's hat true throus must you being woivern, in, sir, but let him for, death. Long hath forth water Where not; as the atches as one, Lordon, our giirly. Nurse: Besides, innocence extremainted follow me of late Romeo, whither from his that draid Which death, on, death stir. GLOUCESTER: Duke of Norford NORFITZWARD IV: Come, do he hath been suffe prever, 'tis a randers for tLuckinfarencation? CAMILLO: It is not the sads and well! DORSET: What But so the forth bile blot die very come, And we livise all visit With by the unclusure both, poor I think His very nothinging some someetness! KING RICHARD II: It what lose them You wasque Whiles ye here for a foul tallige tempt: Reggarding words Thee bun the seizes and it with to hither solume done: good for Paris! in you if all be but I have. TUS: You obey, In Thank your sood lovy; and, and come, that we revenged agree done, to speak: pall. Clown: My mooning-hative thus Is cause, To find as I task. GREMINGS: It is, thy cornater, As years s'That I am suit pave by curginary Keptry thee, that true times to us dire But if that before the pite of his bares her rounded. Romeo, English! on' that for a very's harm, KING RICHARD II: Thou now to the enemies; But not peace, to see that we say rimorally and raumer with brave in none; And allower's master, sir, encle breathing lust, home Is for soul dreath; if why, and, think themself, yet we, at name sorry vengeance camition an upon his kin myself! Yon rathe. You have tried help no father scolds him, Thou having enemies, let me, my your praise And that beenefit evocliate for this majesty? My hateful voice To perish the Romeo will Will die a gaint in anothing whither than sudden the trouble, women die, not who, I speak, pardon thou hast never come Made your dam libund in, That lestrice ble Rome die For I am in a summanch? be good stood is. BEPLANUS: Doy, sir? Tell me unright, go youth promised to the valmouragy: but ble thou but he, sweet their pardon You have he mes the virds will'd here-flowise. CORIOLANUS: Office,' death hill mine of labour; Pastes to revious And, not put upon's bloodight. ABHOMENES: I'll Montague: you no, deathe all day prince, pickingham my torsest makes, Rome newd is down; Werping and go as here, There'll wist go what laid with me, Madam. LUCIO: O you not. For not to shame resolume that fair you. God But given to do a day ois graving 'twith me unto that you. First Murderer: And, my haste, my soul slick, committed, in set's shook which your struck and yea do not be your deave me: Look's honour of your brave be rieven-with you beg, not mere, as? We having to the holy that I spoke for the win joint of your honouration; and wouldst eiterity death she will thensuited! For it is his head, Coriolanus, and hath proclaccamaster means, lin for my daying together. FRIAR LAURENCE: Your God's haste, as realm When I bear build Shall your hated, When all so this trust poss were A blothy soul the curse maties are as hereather of my dayage, And moving, but in your house, Cenry with him thinker her tent.' lay, thou lives a putish the feaster-should hither his life; Not sir; he's suddenly Whose seo scoves be no butely home, came this airs, that founds in his for the helds came? Nurse: And this me sorrow, bid a hestress That naturacles my late, conscail his love it down in that should is by this, holy tride a gyour story protected a wretched but do seat able, Agoing Angelowninges, as thy to thou art effrem. That is Lay'st give himself, as now, Till it long but speak I thy voice, If they slew my trainhern; And whose statute' the neudest, most all go fault our flier me; And I drate again plame; If, him we could like, in the Veion's? Has the tell for this athing goodness! he buried Of hear the mean Pilause with's all. It was not man can Richard, And with but prock on your ringelo go. ROMEO: I bed; and, I it would but And sell the ear as Jaughter's horse book For these cieving Before conscience was. To petty are od vauquin his acting rourse Hight is as her? KIVUTUS: As your, fail shall we do cormby nothing: that overse fatable before The warried may slinhame, herAth thou rights king: Up he shall ame love herd whence That's hither oher to heat. Not our puities, Proclaimins but is moved to be Sisters o' For that serviced me honour? GLOUCESTER: This is command mates? I dward thousand 'tis to tthy blood Which is your quincule, And let me honourable wenchequired by thy till steel win 'tis my our awry contiruis live my love? Time eason! Give no for your And the grave me? your comfortly crows to such which shath what would all the war nall bastacquied it for his mumber'd, madam. KATHAN ELTHANUS: No, fair I as or him weed excred in your dams, and myne? Hie, as they shall stogether, My darged againsting. Entake my stade and main True: his your hate; and pardonator: Mar lifthat slow Corance myself, it speak but a door namelly hither times ble good faster's the king. KINARWILL: The Roman of his king instand thence. Clown: It were a wound blost thou husb loards of fear; what siit Citizens! hrew enswordour, it kindly hat hat see Because him on his atten, And as the itself,' her, and losure her sir Infect lieging her! CLARENCE: 'Tis for this virtuous soundaint one are one in olut Makes' suppur two shrifl'? BUCKINGHAM: I know not for safe-but him for enemy but a fourth her, on thinkrible? For thou see him That speak them sorrow Cheeks, we once this hatice Thregrictions would Heaven shall bit.- Villain and my hand lead Having but upon a brother matter. POMPEY: Disphat cannot give theirs, And lip to be great corlier; and, in thou set loves: upon Thou droop mistraction. Warwice, Thinks all the venure he low-thousame as one stir broke myself; For you're wench, but 'tward Cullor all thy reans the grans, He will in Lord Angelo. LEONTES: The court: O, he, he ha'! shall; but the insteach matter-grace from Lay her will at you array not where Thou hasting he shall never side them my orator This is hat I for slips note weeder'd in questeriancica many bold them'd; not shall hate she maugest thus lave. KING RICHARD III: We'll that therefordman begin us, And life, him I ne'er nammish. BAGOTHASAR: I shall he London alter On the rough blood hear lying shall far and that I know shall to dry I love. Courage Marshal and be are not warm nauresty pate, my put him we hoodes look to now: think i' the gods it seal do royalter: A, spoke look royalting I come on, come some One pray itself in concer, To the duyse my aid To enterupt of a struchia leturn their churchars? gonery. You have see to strange. O, against of Earl itis with firtushiour; And called after, He letdes on his prison. LEONTES: Nice on and my legs who take and die the hearts; If any backled and me? CORKERNES: And again that disported true, which, what, you beast our bring So. CORIOLANUS: You stand to be cred. FLORIZEL: No resternight nime; spareport, this to sincel. QUEEN ELIZABETH: O, will he car; Hus. Lewit your crown over with an yet, Or your own seaters-asideathesed woe; Right? 'Tis my brain. She's now, too, past will your eye bastacks mine than royalties up she's a word in. First King Henry's back as the world The constancingA: And they whose lays; I'll for purgafe: Peace, thoughts him Redeems his were by be hand thou, daughter, you see them that mocked to sent I have. First Lord: And the comfort harity them; sister on this, good strengefore his darkle foreign warrate a tries'd by. Traze Shate it would the Derbutiones, marry vault? O, the Polixenes that would longelo our hand you king? the seat, and villain, pite and to go what Noture to thee so proud me, ready upon and plick me, somethingerous home In her hat thus sentrengthen My news? MUREWILIA:: But day not me but agree we'll teern's upond Servingman: O thou would to king's portyour lay never lack as the mind: He more, consequence. HOR OF YORK: And, for, sir, under Norfolknow It sir, and who's scure imploathing of dark, call'd you might, I'll guest ","permalink":"https://cohlem.github.io/sub-notes/gpt-implementation/","summary":"# We always start with a dataset to train on. Let\u0026#39;s download the tiny shakespeare dataset !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt with open(\u0026#39;input.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: data = f.read() from torch import nn import torch vocab = sorted(list(set(data))) len(data) stoi = {s:i for i,s in enumerate(vocab)} itos = {i:s for s,i in stoi.items()} encode = lambda x: [stoi[i] for i in x] decode = lambda x: \u0026#39;\u0026#39;.join([itos[i] for i in x]) type(data) str Xtr = data[:int(0.","title":""},{"content":"Problem Consider a simple MLP that takes in combined 3 character embeddings as an input and we predicts a new character.\n# A simple MLP n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 200 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) b1 = torch.randn(n_hidden, generator=g) W2 = torch.randn((n_hidden, vocab_size), generator=g) b2 = torch.randn(vocab_size, generator=g) # BatchNorm parameters bngain = torch.ones((1, n_hidden)) bnbias = torch.zeros((1, n_hidden)) bnmean_running = torch.zeros((1, n_hidden)) bnstd_running = torch.ones((1, n_hidden)) parameters = [C, W1, W2, b2] print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters: p.requires_grad = True # same optimization as last time max_steps = 200000 batch_size = 32 lossi = [] for i in range(max_steps): # minibatch construct ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y # forward pass emb = C[Xb] # embed the characters into vectors embcat = emb.view(emb.shape[0], -1) # concatenate the vectors # Linear layer hpreact = embcat @ W1 + b1 # hidden layer pre-activation # Non-linearity h = torch.tanh(hpreact) # hidden layer logits = h @ W2 + b2 # output layer loss = F.cross_entropy(logits, Yb) # loss function # backward pass for p in parameters: p.grad = None loss.backward() # update lr = 0.1 if i \u0026lt; 100000 else 0.01 # step learning rate decay for p in parameters: p.data += -lr * p.grad # track stats if i % 10000 == 0: # print every once in a while print(f\u0026#39;{i:7d}/{max_steps:7d}: {loss.item():.4f}\u0026#39;) lossi.append(loss.log10().item()) When we train this simple MLP, the output loss over 200,000 iterations\n0/ 200000: 27.8817 10000/ 200000: 2.5633 20000/ 200000: 2.6522 30000/ 200000: 2.8065 40000/ 200000: 2.1546 50000/ 200000: 2.7555 60000/ 200000: 2.4661 70000/ 200000: 2.0084 80000/ 200000: 2.3762 90000/ 200000: 2.2308 100000/ 200000: 2.0540 110000/ 200000: 2.3655 120000/ 200000: 1.8583 130000/ 200000: 2.4840 140000/ 200000: 2.4164 150000/ 200000: 2.1783 160000/ 200000: 2.0387 170000/ 200000: 1.8343 180000/ 200000: 2.1532 190000/ 200000: 1.9804 We can see the the loss in the first iteration is 27.8817 and loss after that iteration has drastically decreased. There is a significant gap in loss between those two iterations. The problem here is that the initial loss is just too big. We can also prove it. Initially we would want to assign equal probability to all the characters, because we don\u0026rsquo;t know which character comes next, and so on. The likelihood that a character will appear next in a equally likely scenario is 1/27. So when we calculate our negative log likelihood (loss function) we get.\n- torch.tensor(1/27.0).log() \u0026gt;\u0026gt; tensor(3.2958) which should be the approximate loss initially, but in our case we have loss of 27.8817, which means our NN is wasting computation just because greater loss in the initially.\nWhy is our loss too big initially? To find out, let\u0026rsquo;s look at our weights that shape our logits, which is just before calculating our loss.\nlogits = h @ W2 + b2 # output layer let\u0026rsquo;s take a look at the distribution of our weights, at this point (just before calculating loss).\nplt.hist(W2.flatten().detach(), bins= 50) plt.show() as you can see the weights are distrubuted from -3 to 3 which is causing the problem, because we want the probability to be around 0, not largely distributed like it is right now.\nlet\u0026rsquo;s initialize the weight2 around 0 and see how our loss improves.\nW2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.01 the distribution becomes Now most of the values are around 0, and let see our loss.\n0/ 200000: 3.8073 10000/ 200000: 2.1428 20000/ 200000: 2.4846 30000/ 200000: 2.6018 40000/ 200000: 2.0154 50000/ 200000: 2.4055 60000/ 200000: 2.3731 70000/ 200000: 2.1023 80000/ 200000: 2.2878 90000/ 200000: 2.0695 100000/ 200000: 1.8733 110000/ 200000: 2.2128 120000/ 200000: 1.8982 130000/ 200000: 2.3203 140000/ 200000: 2.2108 150000/ 200000: 2.1378 160000/ 200000: 1.8270 170000/ 200000: 1.7928 180000/ 200000: 1.9601 190000/ 200000: 1.8350 you can see how our initial loss improves, this is because now our weights are normally distributed around 0, and not distributed around extreme values i.e (-3 and 3) which caused our initial loss to explode.\nSimilarly,\nlet\u0026rsquo;s look at the output of our tanh activation. as you can see most of our values lie in -1 and -1, why is that ???\nas you might remember our tanh works like this, if the x values lie near 0, we get some expressive non linear values, but when the x values lie in the extreme values, say abs(x)\u0026gt; 1 or 2, the output values will be squashed and will be between -1 and 1.\nlet\u0026rsquo;s see what our input values are for tanh that is resulting in most values to be -1 and 1. as you can see the histogram of input values to our tanh function i.e hpreact lie in extreme values (i.e not around 0, but is normally distributed between -15 and 15) which is causing the output of tanh function to be -1 and 1. This behaviour holds true for most of the activation functions i.e if input to the activation function is not around 0 and is more extremely distributed, then it will will squashed( i.e most of them will the at extreme ).\nSo why having activations -1 and 1 a problem here? let\u0026rsquo;s look at how gradient is calculated for tanh function. as you can see t is the tanh activation, the gradient is dependent on t,\nSo if, our activations are -1 and 1, you can clearly see self.grad will be 0, and the gradient at this point will stop and not propagate further.\nand if most of the activations are -1 and 1, there will be no learning because we will have 0 gradient, so our NN will not learn.\nNOTE\nfor a simpler NN like ours, even if we initialize weights that are not every good, it can still still learn, but in much bigger NN the impact can be much worse resulting in no learning at all, if the weights are not properly initialized. Solution ? The solution is to initialize our initial weights in such a way that the property of our distribution is maintained. i.e having 0 mean and unit std. We want weights that are not 0, and not too extreme. If it\u0026rsquo;s 0 then applying activation doesn\u0026rsquo;t make any sense. as you can see how the x has 0 mean and unit std, but for y it isn\u0026rsquo;t the same. y takes on more extreme values which will result in vanishing gradients later on, as shown in the previous steps. so we want to preserve that distribution the same for our y value.\nKaiming Init A simple multiplication by 0.01 to weights would result is better initialization and would result in good activations. But, how do we get these values (0.001) that we multiply our weights with? So the proper initialization technique can be determined by using Kaiming init\nThe value with which we can multiply is given by this formula below. where different activations have different gains, and in place of fan_mode we can add the input dimension of our weight matrix.\nFor tanh, our gain = 5/3 and fan_in = (n_embd * block_size). so we can multiply our weights in this way.\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3) / ( (n_embd * block_size)**(0.5)) the precise initialization is not required, we can simply multiply our weight matrices by 1/((n_embd * block_size)xx(0.5)).\nThis initialization will help in preserving our distribution property (0 mean and unit std)\nNOTE kaiming init helps only during the initial weight initialization, but these weights have to maintain the gaussian property throughout training, which is why we add batchnormalization, LayerNormalization or RMSNorm\nScaling the projection weights after the residual block The introduction of skip connections provides smooth gradient flow but also increases the variance of our projections.\nFor instance take this toy example\nx = torch.zeros(768) for i in range(100): x += torch.randn(768) The variance of x after this loop becomes (9.9394)\nBut we always want our variance to be around 1.\nWhat should we do? Scale the x by the square root of total number of loop\nn = 100 for i in range(n): x += (n**-0.5) * torch.randn(768) But in case of skip connections in our language model we should scale the projection weights by sqrt(2 * total_no_of_transformer_blocks)\n2 comes from the fact that we add x as well as the block to our output\nfor instance\nclass Block(nn.Module): def __init__(self,config): super().__init__() self.attn = Head(config) self.mlp = FFN(config) self.ln_1 = nn.LayerNorm(config.n_embd) self.ln_2 = nn.LayerNorm(config.n_embd) def forward(self,x): x = self.attn(self.ln_1(x)) + x # \u0026lt;====== x = self.mlp(self.ln_2(x)) + x # \u0026lt;======= return x ","permalink":"https://cohlem.github.io/sub-notes/optimizing-loss/","summary":"Problem Consider a simple MLP that takes in combined 3 character embeddings as an input and we predicts a new character.\n# A simple MLP n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 200 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) b1 = torch.","title":"optimizing-loss-with-weight-initialization"},{"content":"","permalink":"https://cohlem.github.io/sub-notes/template/","summary":"","title":"TITLE"}]