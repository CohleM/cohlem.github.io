[{"content":"Source: The spelled-out intro to neural networks and backpropagation: building micrograd\nBackpropagation on paper It implements backpropagation for two arithmetic operation (multiplication and addition) which are quite straightforward.\nImplementation is for this equation.\na = Value(2.0, label=\u0026#39;a\u0026#39;) b = Value(-3.0, label=\u0026#39;b\u0026#39;) c = Value(10.0, label=\u0026#39;c\u0026#39;) e = a*b; e.label = \u0026#39;e\u0026#39; d = e + c; d.label = \u0026#39;d\u0026#39; f = Value(-2.0, label=\u0026#39;f\u0026#39;) L = d * f; L.label = \u0026#39;L\u0026#39; L The most important thing to note here is the gradient accumulation step (shown at the bottom-left). If a node takes part two times building up to the final node. The gradient for that node is accumulated. For instance, in the figure node b takes part two times. First, it is involved in equation e = a * b, and another is e = b + m (not in the equation above).\nCode\nclass Value: def __init__(self,data, _children = (), _op = \u0026#39;\u0026#39;, label=None): self.data = data self.label = label self.grad = 0.0 self._prev = set(_children) self._op = _op self._backward = lambda: None def __repr__(self): return f\u0026#34;Value({self.label}, {self.data})\u0026#34; def __add__(self, other): result = Value(self.data + other.data, (self, other), \u0026#39;+\u0026#39;) def _backward(): self.grad = 1.0 * result.grad other.grad = 1.0 * result.grad result._backward = _backward return result def __sub__(self,other): result = Value(self.data - other.data) result._prev = [self, other] result._op = \u0026#39;-\u0026#39; return result def __mul__(self,other): result = Value(self.data * other.data, (self, other), \u0026#39;*\u0026#39;) def _backward(): self.grad = other.data * result.grad other.grad = self.data * result.grad result._backward = _backward return result def backward(self): topo = [] visited = set() self.grad = 1 def build_topo(v): if v not in visited: visited.add(v) for child in v._prev: build_topo(child) topo.append(v) build_topo(self) topo = list(reversed(topo)) print(\u0026#39;gg\u0026#39;, topo) for i in topo: print(i) i._backward() ","permalink":"https://cohlem.github.io/sub-notes/backpropagation-from-scratch/","summary":"Source: The spelled-out intro to neural networks and backpropagation: building micrograd\nBackpropagation on paper It implements backpropagation for two arithmetic operation (multiplication and addition) which are quite straightforward.\nImplementation is for this equation.\na = Value(2.0, label=\u0026#39;a\u0026#39;) b = Value(-3.0, label=\u0026#39;b\u0026#39;) c = Value(10.0, label=\u0026#39;c\u0026#39;) e = a*b; e.label = \u0026#39;e\u0026#39; d = e + c; d.label = \u0026#39;d\u0026#39; f = Value(-2.0, label=\u0026#39;f\u0026#39;) L = d * f; L.label = \u0026#39;L\u0026#39; L The most important thing to note here is the gradient accumulation step (shown at the bottom-left).","title":"Backpropagation from scratch"},{"content":"Backpropagation Backpropagationfrom scratch Loss function Maximum likelihood estimate as loss function Why we add regularization to loss functioÃ± Optimization Optimizing loss with weight initialization BatchNormalization ","permalink":"https://cohlem.github.io/notes/deep-learning-notes/","summary":"Backpropagation Backpropagationfrom scratch Loss function Maximum likelihood estimate as loss function Why we add regularization to loss functioÃ± Optimization Optimizing loss with weight initialization BatchNormalization ","title":"Deep Learning Notes"},{"content":" it penalizes the weights, and prioritizes uniformity in weights. How does it penalize the weights? Now when we do the backprop and gradient descent.\nThe gradient of loss w.r.t some weights become as we can see it penalizes the weight by reducing the weights\u0026rsquo;s value by some higher amount compared to the some minimial weight update when we only used loss function.\nSo overall, the model tries to balance the Loss (L) as well as keep the weights small. This balance prevents the model from relying excessively on any particular weight.\n","permalink":"https://cohlem.github.io/sub-notes/why-we-need-regularization/","summary":"it penalizes the weights, and prioritizes uniformity in weights. How does it penalize the weights? Now when we do the backprop and gradient descent.\nThe gradient of loss w.r.t some weights become as we can see it penalizes the weight by reducing the weights\u0026rsquo;s value by some higher amount compared to the some minimial weight update when we only used loss function.\nSo overall, the model tries to balance the Loss (L) as well as keep the weights small.","title":"Why we add regularization in loss function"},{"content":"","permalink":"https://cohlem.github.io/posts/backpropogation/","summary":"","title":"Backpropagation from scratch"},{"content":"BackStory This is a simple fun little project that I did almost a year ago, At that time I used to see a lot of CodeBullet\u0026rsquo;s videos and wanted to learn the gist behind these evolutionary algorithms, and I can\u0026rsquo;t get into my head if I don\u0026rsquo;t do it from scratch so I wanted to implement this project from scratch. At that time, I wanted to document the learning and the implementation process, I even thought of making a youtube video about this topic but could not complete it, at that time, I had made some video animations about this process but could not complete it because I started doing something else and when I can back it was already too much mess and could not complete the video animations, but I\u0026rsquo;ll include the video animations that I had made earlier.\nSnake using Neural Network and Genetic Algorithm This is a simple demonstration of how neural network can be used along with genetic algorithm to play snake game. Now when I say its a snake game, this is just basically learning how to optimize a function, just like what we do with gradient descent. But that\u0026rsquo;s the interesting part we\u0026rsquo;re not using Gradient Descent, no fancy optimizers, even the initial weights were initialized randomly, I now wish(at the time of writing this blog) that I had used some initialization techniques like Xavier initializations, it would have learned to play so much faster. But even the random initialization works. Let\u0026rsquo;s get to the point.\nWe are playing a snake game let\u0026rsquo;s imagine that has a brain, So the brain here are weights which we initialize using Feed-forward layer and then we use genetic algorithm to evolve that brain(weights) and use this brain(weights) to predict which move to take(up, down, left or right) at every step in the game. So the term Neural Network here only represents the feed-forward layer, we don\u0026rsquo;t use any of the backpropogation.\nThe whole code for this project can be found here: Github\nDemo Video Explanation Following are the explanations that I\u0026rsquo;ve implemented for this project.\nSnake\u0026rsquo;s Eye It is what snake sees at each step while it\u0026rsquo;s playing game. Before performing anything, it has to see and pass it to the brain inorder to process what it has seen. So let\u0026rsquo;s understand what it sees at each step by looking at the video below.\nOur snake sees in 8 different directions(d1 to d8) as shown in the figure above. And at each direction it sees 3 different things Wall, Food, Body(it\u0026rsquo;s own body)\nSnake\u0026rsquo;s Brain Now that we have our snake eyes let\u0026rsquo;s make the brain to process the things that it has seen. We use a simple two layer neural network. The first layer has 8 hidden layers and the second is output layer of 4 units. We only use the feed forward part of neural network and discard all the other backpropagation part. We will later use genetic algorithm to improve our weights.\nThe figure below contains two parts\nSnake\u0026rsquo;s eye(descibed above), and Snake\u0026rsquo;s brain Input to neural network Now that we now our snake sees a body, wall and food. Let\u0026rsquo;s assign values to these metrics.\nSince the neural networks only understand numbers we represent them with a metric, 1 for favorable conditions and 0 for unfavorable.\nMetric for body: One is given when no body is present in that direction and Zero is given when no blocks are present in between Head and Body.\n$body = number of blocks between head and body/(total number of blocks - 1 )$.\nMetric for Wall: One is given when the number of blocks between Head and Wall are maximum and Zero is given when no blocks are present in between Head and Wall.\n$Wall = number of blocks between head and wall/(total number of blocks - 1 )$.\nMetric for Food: It is good for snake to move in the direction of food, so One is given when no blocks are present in between Head and Food and Zero is given when no food is present in that direction.\n$Food = ( total number of blocks - number of blocks between head and wall - 1)/(total number of blocks - 1 )$.\nThe another input to our snake is the it\u0026rsquo;s directions. It should keep. track of where it currently is to make move in another direction.\nValue for Head Direction: This is a simple one-hot encoding for the direction.\nBrain of Snake The brain of our snake is the weights inside our hidden layers and output layers.\nOur first hidden layer will have 8 * 28 weights with 8 bias. Our second layer will have 4 * 8 weights with 4 bias.\nSo, the total weights + bias counts to 268 which is the actual brain of our snake. So our neural network uses that brain to make a prediction in 4 directions. [Up, Right, Down, Left]\nGenetic Algorithm Since we do not use the backpropagation of our neural network to improve weights we will use use Genetic algorithm to improve weights.\nFive phases are considered in a genetic algorithm. You should first read these steps below and then you can come back to the video below.\nInitial brain (Population) We randomly generate the brain of size (1,268). Remember this is the initial phase where we initialize our weigths (brain) for our snake which described above. Later, this shape (1,268) is flattened into the structure of our feed-forward neural network which is described above.\nnp.random.choice(np.arange(-1,1, step =0.001),size = (population_size,weight_size), replace= True)\nweight size is 268 and population_size is the total number of snakes we want to train. I\u0026rsquo;ve trained 500 snakes in each generations. Thing to remember here is, each snake will have different brain. The random function above generates a linear vector which is later converted to matrices of sizes (8,28) for weights, (8,1) for bias and (4,8) for weights and (4,1) for bias using vector_to_matrix() function. The first two matrices is for hidden layer whereas the other two are for output layer.\nFitness function Now that we have 500 different snakes in each generation. We have to differentiate the great Snake from weak. Snake\u0026rsquo;s fitness is based on its score and number of steps it taken to achieve that score, so we created a function by using Steps and Score as variables which will helps the Snake in getting maximum score in less steps. I\u0026rsquo;ve used the following fitness function that evaluates fitness of snakes relative to score and steps taken.\nIf two snakes have the same score then the snake that achieves the score in less number of steps is considered.\nSelection This process selects the best fit snakes from the entire population according to their fitness value described above. It is then used produce a new population of offspring which will be used in a next generation. I\u0026rsquo;ve selected top 50 snakes initially according to their fitness value. The selected snakes will be called parents.\nCrossover We take the parents selected from the selection process to produce a new set of offsprings. We take 50 parents and iterate over (population_size - parent_length) times and use uniform crossover method to produce new offspring from these parents. we later add our 50 parents to the population set. This process will preserve the best fit snakes, even if the crossover and mutation yield bad set of population. The unifrom crossover can be explained likewise.\nMutation This example explains mutating by flipping bits.\n1\t0\t1\t0\t0\t1 0\nâ†“\n1\t0\t1\t0\t1\t1\t0\nBut in our case we change the value of our snake\u0026rsquo;s brain. Among 268 brain cells we will change 13 of them randomly between -0.5 to 0.5.\nRunning We run the game through many generation(similar to epoch) to evolve our snake\u0026rsquo;s brain by applying the method explained above. We do it until the snake has learned to score desired number of points.\nI know this is shitty writing but still Thank You for reading till the end. ","permalink":"https://cohlem.github.io/posts/using-genetic-algorithm-for-weights-optimization/","summary":"BackStory This is a simple fun little project that I did almost a year ago, At that time I used to see a lot of CodeBullet\u0026rsquo;s videos and wanted to learn the gist behind these evolutionary algorithms, and I can\u0026rsquo;t get into my head if I don\u0026rsquo;t do it from scratch so I wanted to implement this project from scratch. At that time, I wanted to document the learning and the implementation process, I even thought of making a youtube video about this topic but could not complete it, at that time, I had made some video animations about this process but could not complete it because I started doing something else and when I can back it was already too much mess and could not complete the video animations, but I\u0026rsquo;ll include the video animations that I had made earlier.","title":"Using Genetic Algorithm for Weights Optimization"},{"content":"This is my experience and experimentation that I did while building a product for the use case of using LLMs for our own data for question answering. If you\u0026rsquo;re doing something similar, this could be of some help.\nThe most commonly used methods while using LLMs with our own data were typically\nFine-tuning the model with your own data Using Retrieval Augmented Generation (RAG) techniques Fine-tuning the model with your own data This is the initial method and follows the general structure of training a model\nData preparation Train Evaluate For my task, I chose to train OpenAI\u0026rsquo;s davinci base model\nThere were mostly no hyperparameters to tune, as OpenAI takes care of it outside the box. Training the model involved more instruction tuning, instructing the model to act in a similar way, rather than just training the model to save data in its model weights. The effectiveness of instruction tuning mostly depended on the data preparation process. Data preparation involved formatting the data into pairs of \u0026lt;instruction, completion\u0026gt;.\nThis is a crucial step for better outputs and depends on the size of the training data. When the model was trained with a small amount of data, it mostly followed the instructions but had limited knowledge and memory of facts from the dataset. In most cases, while testing the model, it followed the instructions but often produced incorrect answers. When the scale of data was increased, it started to follow both the instructions and retain more knowledge. So, this process was crucial in identifying when training OpenAI\u0026rsquo;s models worked best. If you\u0026rsquo;re using it to train on a small scale of data, this would not yield the desired output, and I would recommend using the other process I employed for small-sized data.\nUsing Retrieval Augmented Generation (RAG) techniques This technique is sometimes referred to as In-context Learning. This is one of the most trending topics since the release of ChatGPT, and you might have heard the phrase \u0026ldquo;chat with your own data.\u0026rdquo; This process is simple yet very effective when you have your own small-scale data, which I used for question answering. The process involves:\nDividing the data into chunks Here, the format of the data doesn\u0026rsquo;t really matter. The only thing we need to take care of is the size of the chunks. The chunk size should always be smaller than the context length of LLMs, providing space for prompt and completion texts. In my case, the context length of gpt-3.5-turbo was 4,096 tokens, and I divided the chunks into token sizes of 1000. I chose a token size of 1000 for and retrieved top-3 chunks to be passed to the LLM.\nConverting the chunks into embeddings This process generates embeddings, which are vector representations of our chunks. I experimented with a couple of embedding models, each having its pros and cons. My recommendations for each of the embedding models are as follows:\nall-MiniLM-L12-v2: Useful when you need fast conversion from chunks to embeddings. It has a relatively small dimension of 384 and does a decent job in converting to embeddings.\nOpenAI\u0026rsquo;s text-embedding-ada-002: Useful when you need to generate highly accurate embeddings. If you are using it in real-time, it would be too slow due to its high dimension size of 1536, and API calling makes it even slower.\nInstructor: Useful when you need the accuracy level of text-embedding-ada-002 and fast conversion from text to embeddings. This model is blazingly fast and would save on cost when you embed lots of data.\nI went with the Instructor-XL model.\nStoring the embeddings Many vector database companies have risen around this use case of storing embeddings, such as Pinecone, Chroma, and many more. The trend is to follow the hype and opt for vector databases, which, in fact, are completely useless. If your embeddings are really big, I would recommend using vector databases; otherwise, for medium to small-scale data, ndarray would do a great job. I decided to just use a numpy array.\nRetrieval Relevant chunks are retrieved based on the similarity between the user\u0026rsquo;s query\u0026rsquo;s embeddings and the chunks\u0026rsquo; embeddings. Metrics like dot product, cosine similarity, are widely adopted, whereas cosine similarity would suffice. After evaluating the similarity, the top-k chunks are retrieved. We can use reranking to improve the quality of relevant chunks, but top-k would suffice. After retrieving, we are ready to feed the retrieved chunks and the user\u0026rsquo;s query to LLM by combining them with prompting.\nPrompting Prompting has been the most important step in getting the desired output. For me, this has turned out to be harder than doing research, writing code, and debugging combined. Writing quality prompts is hard. I experimented with a couple of prompting techniques in this project.\nFew-shot prompting, few-shot combined with Chain of Thought (CoT) prompting, but I couldn\u0026rsquo;t achieve the desired output. I noticed some problems with these techniques for my use case. These prompting techniques caused too many logic jumps, and the desired logic was never analyzed by the LLM. What worked for me was the map-reduce method and double prompting (which involves calling LLM twice, with 2 prompts, where the latter prompt is combined with the former LLM output). Both methods worked fine, with map-reduce being more expensive. I opted for double prompting and was able to generate the desired output. So, prompting has been challenging for me. Just a thought: maybe we should do some reverse engineering someday and train the prompts with the desired output as context, as described in the AUTOPROMPT paper ðŸ˜….\n","permalink":"https://cohlem.github.io/posts/llmvsincontext/","summary":"This is my experience and experimentation that I did while building a product for the use case of using LLMs for our own data for question answering. If you\u0026rsquo;re doing something similar, this could be of some help.\nThe most commonly used methods while using LLMs with our own data were typically\nFine-tuning the model with your own data Using Retrieval Augmented Generation (RAG) techniques Fine-tuning the model with your own data This is the initial method and follows the general structure of training a model","title":"Fine-tuning LLM vs In-context learning"},{"content":"As we saw in our previous note how important it is to have the pre-activation values to be roughly gaussian (0 mean, and unit std). We saw how we can initialize our weights that make our pre-activation roughly gaussian by using Kaiming init. But, how do we always maintain our pre-activations to be roughly gaussian?\nAnswer: BatchNormalization\nBenefits\nstable training preserves vanishing gradients BatchNormalization As the name suggests, batches are normalized (across batches), by normalizing across batches we preserve the gaussian property of our pre-activations.\nThis is our initial code\n# same optimization as last time max_steps = 200000 batch_size = 32 lossi = [] for i in range(max_steps): # minibatch construct ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y # forward pass emb = C[Xb] # embed the characters into vectors embcat = emb.view(emb.shape[0], -1) # concatenate the vectors # Linear layer hpreact = (embcat @ W1 + b1) # hidden layer pre-activation # Non-linearity h = torch.tanh(hpreact) # hidden layer logits = h @ W2 + b2 # output layer loss = F.cross_entropy(logits, Yb) # loss function # backward pass for p in parameters: p.grad = None hpreact.retain_grad() logits.retain_grad() loss.backward() # update lr = 0.1 if i \u0026lt; 100000 else 0.01 # step learning rate decay for p in parameters: p.data += -lr * p.grad # track stats if i % 10000 == 0: # print every once in a while print(f\u0026#39;{i:7d}/{max_steps:7d}: {loss.item():.4f}\u0026#39;) lossi.append(loss.log10().item()) We add batch normalization just before the all activation layers. Right now, we only have one linear layer so only one tanh activation is applied, in a big NN, we have to add those batch normalization before applying activations.\nApplying batch normalization is quite simple.\nhpreact = (hpreact - hpreact.mean(0, keepdim = True)) / hpreact.std(0, keepdim =True) but by doing this we are forcing it to lie is a particular space. To add a little bit of entropy (randomness) and let to model learn itself (by backpropagating) where it wants to go, we introduce scaling and shifting. The model will learn itself the direction it wants to go, by back propagating because these scaling and shifting are differentiable.\n# BatchNorm parameters bngain = torch.ones((1, n_hidden)) bnbias = torch.zeros((1, n_hidden)) hpreact = bngain*(hpreact - hpreact.mean(0, keepdim = True)) / hpreact.std(0, keepdim =True) + bnbias by introducing batchNormalization, we are making our pre-activation of one input depend on all the other input, this is because we are subtracting the mean from it and this mean is the depended on all the other inputs.\nNow that we have introduced BatchNormalization, inorder to do inference, we would need the mean and std of the whole dataset, which we can keep track of in running manner, and it is used while doing inference.\n# same optimization as last time max_steps = 200000 batch_size = 32 lossi = [] for i in range(max_steps): # minibatch construct ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y # forward pass emb = C[Xb] # embed the characters into vectors embcat = emb.view(emb.shape[0], -1) # concatenate the vectors # Linear layer hpreact = (embcat @ W1 + b1) # hidden layer pre-activation bnmeani = hpreact.mean(0, keepdim = True) bnstdi = hpreact.std(0, keepdim =True) hpreact = bngain*((hpreact - bnmeani) / bnstdi) + bnbias ## --------------------- keeping track of whole mean and std ------------ with torch.no_grad(): bnmean_running = 0.999*bnmean_running + 0.001*bnmeani bnstd_running = 0.999*bnstd_running + 0.001*bnstdi # ------------------------------------------------------------------------ # Non-linearity h = torch.tanh(hpreact) # hidden layer logits = h @ W2 + b2 # output layer loss = F.cross_entropy(logits, Yb) # loss function # backward pass for p in parameters: p.grad = None hpreact.retain_grad() logits.retain_grad() loss.backward() # update lr = 0.1 if i \u0026lt; 100000 else 0.01 # step learning rate decay for p in parameters: p.data += -lr * p.grad # track stats if i % 10000 == 0: # print every once in a while print(f\u0026#39;{i:7d}/{max_steps:7d}: {loss.item():.4f}\u0026#39;) lossi.append(loss.log10().item()) ","permalink":"https://cohlem.github.io/sub-notes/batchnormalization/","summary":"As we saw in our previous note how important it is to have the pre-activation values to be roughly gaussian (0 mean, and unit std). We saw how we can initialize our weights that make our pre-activation roughly gaussian by using Kaiming init. But, how do we always maintain our pre-activations to be roughly gaussian?\nAnswer: BatchNormalization\nBenefits\nstable training preserves vanishing gradients BatchNormalization As the name suggests, batches are normalized (across batches), by normalizing across batches we preserve the gaussian property of our pre-activations.","title":"BatchNormalization"},{"content":" ","permalink":"https://cohlem.github.io/sub-notes/maximum-likelihood-estimate-as-loss/","summary":" ","title":"Maximum likelihood estimate as loss function"},{"content":"Problem Consider a simple MLP that takes in combined 3 character embeddings as an input and we predicts a new character.\n# A simple MLP n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 200 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) b1 = torch.randn(n_hidden, generator=g) W2 = torch.randn((n_hidden, vocab_size), generator=g) b2 = torch.randn(vocab_size, generator=g) # BatchNorm parameters bngain = torch.ones((1, n_hidden)) bnbias = torch.zeros((1, n_hidden)) bnmean_running = torch.zeros((1, n_hidden)) bnstd_running = torch.ones((1, n_hidden)) parameters = [C, W1, W2, b2] print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters: p.requires_grad = True # same optimization as last time max_steps = 200000 batch_size = 32 lossi = [] for i in range(max_steps): # minibatch construct ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y # forward pass emb = C[Xb] # embed the characters into vectors embcat = emb.view(emb.shape[0], -1) # concatenate the vectors # Linear layer hpreact = embcat @ W1 + b1 # hidden layer pre-activation # Non-linearity h = torch.tanh(hpreact) # hidden layer logits = h @ W2 + b2 # output layer loss = F.cross_entropy(logits, Yb) # loss function # backward pass for p in parameters: p.grad = None loss.backward() # update lr = 0.1 if i \u0026lt; 100000 else 0.01 # step learning rate decay for p in parameters: p.data += -lr * p.grad # track stats if i % 10000 == 0: # print every once in a while print(f\u0026#39;{i:7d}/{max_steps:7d}: {loss.item():.4f}\u0026#39;) lossi.append(loss.log10().item()) When we train this simple MLP, the output loss over 200,000 iterations\n0/ 200000: 27.8817 10000/ 200000: 2.5633 20000/ 200000: 2.6522 30000/ 200000: 2.8065 40000/ 200000: 2.1546 50000/ 200000: 2.7555 60000/ 200000: 2.4661 70000/ 200000: 2.0084 80000/ 200000: 2.3762 90000/ 200000: 2.2308 100000/ 200000: 2.0540 110000/ 200000: 2.3655 120000/ 200000: 1.8583 130000/ 200000: 2.4840 140000/ 200000: 2.4164 150000/ 200000: 2.1783 160000/ 200000: 2.0387 170000/ 200000: 1.8343 180000/ 200000: 2.1532 190000/ 200000: 1.9804 We can see the the loss in the first iteration is 27.8817 and loss after that iteration has drastically decreased. There is a significant gap in loss between those two iterations. The problem here is that the initial loss is just too big. We can also prove it. Initially we would want to assign equal probability to all the characters, because we don\u0026rsquo;t know which character comes next, and so on. The likelihood that a character will appear next in a equally likely scenario is 1/27. So when we calculate our negative log likelihood (loss function) we get.\n- torch.tensor(1/27.0).log() \u0026gt;\u0026gt; tensor(3.2958) which should be the approximate loss initially, but in our case we have loss of 27.8817, which means our NN is wasting computation just because greater loss in the initially.\nWhy is our loss too big initially? To find out, let\u0026rsquo;s look at our weights that shape our logits, which is just before calculating our loss.\nlogits = h @ W2 + b2 # output layer let\u0026rsquo;s take a look at the distribution of our weights, at this point (just before calculating loss).\nplt.hist(W2.flatten().detach(), bins= 50) plt.show() as you can see the weights are distrubuted from -3 to 3 which is causing the problem, because we want the probability to be around 0, not largely distributed like it is right now.\nlet\u0026rsquo;s initialize the weight2 around 0 and see how our loss improves.\nW2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.01 the distribution becomes Now most of the values are around 0, and let see our loss.\n0/ 200000: 3.8073 10000/ 200000: 2.1428 20000/ 200000: 2.4846 30000/ 200000: 2.6018 40000/ 200000: 2.0154 50000/ 200000: 2.4055 60000/ 200000: 2.3731 70000/ 200000: 2.1023 80000/ 200000: 2.2878 90000/ 200000: 2.0695 100000/ 200000: 1.8733 110000/ 200000: 2.2128 120000/ 200000: 1.8982 130000/ 200000: 2.3203 140000/ 200000: 2.2108 150000/ 200000: 2.1378 160000/ 200000: 1.8270 170000/ 200000: 1.7928 180000/ 200000: 1.9601 190000/ 200000: 1.8350 you can see how our initial loss improves, this is because now our weights are normally distributed around 0, and not distributed around extreme values i.e (-3 and 3) which caused our initial loss to explode.\nSimilarly,\nlet\u0026rsquo;s look at the output of our tanh activation. as you can see most of our values lie in -1 and -1, why is that ???\nas you might remember our tanh works like this, if the x values lie near 0, we get some expressive non linear values, but when the x values lie in the extreme values, say abs(x)\u0026gt; 1 or 2, the output values will be squashed and will be between -1 and 1.\nlet\u0026rsquo;s see what our input values are for tanh that is resulting in most values to be -1 and 1. as you can see the histogram of input values to our tanh function i.e hpreact lie in extreme values (i.e not around 0, but is normally distributed between -15 and 15) which is causing the output of tanh function to be -1 and 1. This behaviour holds true for most of the activation functions i.e if input to the activation function is not around 0 and is more extremely distributed, then it will will squashed( i.e most of them will the at extreme ).\nSo why having activations -1 and 1 a problem here? let\u0026rsquo;s look at how gradient is calculated for tanh function. as you can see t is the tanh activation, the gradient is dependent on t,\nSo if, our activations are -1 and 1, you can clearly see self.grad will be 0, and the gradient at this point will stop and not propagate further.\nand if most of the activations are -1 and 1, there will be no learning because we will have 0 gradient, so our NN will not learn.\nNOTE\nfor a simpler NN like ours, even if we initialize weights that are not every good, it can still still learn, but in much bigger NN the impact can be much worse resulting in no learning at all, if the weights are not properly initialized. Solution ? The solution is to initialize our initial weights in such a way that the property of our distribution is maintained. i.e having 0 mean and unit std. We want weights that are not 0, and not too extreme. If it\u0026rsquo;s 0 then applying activation doesn\u0026rsquo;t make any sense. as you can see how the x has 0 mean and unit std, but for y it isn\u0026rsquo;t the same. y takes on more extreme values which will result in vanishing gradients later on, as shown in the previous steps. so we want to preserve that distribution the same for our y value.\nKaiming Init A simple multiplication by 0.01 to weights would result is better initialization and would result in good activations. But, how do we get these values (0.001) that we multiply our weights with? So the proper initialization technique can be determined by using Kaiming init\nThe value with which we can multiply is given by this formula below. where different activations have different gains, and in place of fan_mode we can add the input dimension of our weight matrix.\nFor tanh, our gain = 5/3 and fan_in = (n_embd * block_size). so we can multiply our weights in this way.\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3) / ( (n_embd * block_size)**(0.5)) the precise initialization is not required, we can simply multiply our weight matrices by 1/((n_embd * block_size)xx(0.5)).\nThis initialization will help in preserving our distribution property (0 mean and unit std)\n","permalink":"https://cohlem.github.io/sub-notes/optimizing-loss/","summary":"Problem Consider a simple MLP that takes in combined 3 character embeddings as an input and we predicts a new character.\n# A simple MLP n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 200 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) b1 = torch.","title":"optimizing-loss-with-weight-initialization"},{"content":"","permalink":"https://cohlem.github.io/sub-notes/template/","summary":"","title":"TITLE"}]