[{"content":"Source: The spelled-out intro to neural networks and backpropagation: building micrograd\nBackpropagation on paper It implements backpropagation for two arithmetic operation (multiplication and addition) which are quite straightforward.\nImplementation is for this equation.\na = Value(2.0, label=\u0026#39;a\u0026#39;) b = Value(-3.0, label=\u0026#39;b\u0026#39;) c = Value(10.0, label=\u0026#39;c\u0026#39;) e = a*b; e.label = \u0026#39;e\u0026#39; d = e + c; d.label = \u0026#39;d\u0026#39; f = Value(-2.0, label=\u0026#39;f\u0026#39;) L = d * f; L.label = \u0026#39;L\u0026#39; L The most important thing to note here is the gradient accumulation step (shown at the bottom-left). If a node takes part two times building up to the final node. The gradient for that node is accumulated. For instance, in the figure node b takes part two times. First, it is involved in equation e = a * b, and another is e = b + m (not in the equation above).\nCode\nclass Value: def __init__(self,data, _children = (), _op = \u0026#39;\u0026#39;, label=None): self.data = data self.label = label self.grad = 0.0 self._prev = set(_children) self._op = _op self._backward = lambda: None def __repr__(self): return f\u0026#34;Value({self.label}, {self.data})\u0026#34; def __add__(self, other): result = Value(self.data + other.data, (self, other), \u0026#39;+\u0026#39;) def _backward(): self.grad = 1.0 * result.grad other.grad = 1.0 * result.grad result._backward = _backward return result def __sub__(self,other): result = Value(self.data - other.data) result._prev = [self, other] result._op = \u0026#39;-\u0026#39; return result def __mul__(self,other): result = Value(self.data * other.data, (self, other), \u0026#39;*\u0026#39;) def _backward(): self.grad = other.data * result.grad other.grad = self.data * result.grad result._backward = _backward return result def backward(self): topo = [] visited = set() self.grad = 1 def build_topo(v): if v not in visited: visited.add(v) for child in v._prev: build_topo(child) topo.append(v) build_topo(self) topo = list(reversed(topo)) print(\u0026#39;gg\u0026#39;, topo) for i in topo: print(i) i._backward() ","permalink":"https://cohlem.github.io/sub-notes/backpropagation-from-scratch/","summary":"Source: The spelled-out intro to neural networks and backpropagation: building micrograd\nBackpropagation on paper It implements backpropagation for two arithmetic operation (multiplication and addition) which are quite straightforward.\nImplementation is for this equation.\na = Value(2.0, label=\u0026#39;a\u0026#39;) b = Value(-3.0, label=\u0026#39;b\u0026#39;) c = Value(10.0, label=\u0026#39;c\u0026#39;) e = a*b; e.label = \u0026#39;e\u0026#39; d = e + c; d.label = \u0026#39;d\u0026#39; f = Value(-2.0, label=\u0026#39;f\u0026#39;) L = d * f; L.label = \u0026#39;L\u0026#39; L The most important thing to note here is the gradient accumulation step (shown at the bottom-left).","title":"Backpropagation from scratch"},{"content":"Backpropagation from scratch ","permalink":"https://cohlem.github.io/notes/deep-learning-notes/","summary":"Backpropagation from scratch ","title":"Deep Learning Notes"},{"content":" it penalizes the weights, and prioritizes uniformity in weights. How does it penalize the weights? Now when we do the backprop and gradient descent.\nThe gradient of loss w.r.t some weights become as we can see it penalizes the weight by reducing the weights\u0026rsquo;s value by some higher amount compared to the some minimial weight update when we only used loss function.\nSo overall, the model tries to balance the Loss (L) as well as keep the weights small. This balance prevents the model from relying excessively on any particular weight.\n","permalink":"https://cohlem.github.io/sub-notes/why-we-need-regularization/","summary":"it penalizes the weights, and prioritizes uniformity in weights. How does it penalize the weights? Now when we do the backprop and gradient descent.\nThe gradient of loss w.r.t some weights become as we can see it penalizes the weight by reducing the weights\u0026rsquo;s value by some higher amount compared to the some minimial weight update when we only used loss function.\nSo overall, the model tries to balance the Loss (L) as well as keep the weights small.","title":"Why we add regularization in loss function"},{"content":"","permalink":"https://cohlem.github.io/posts/backpropogation/","summary":"","title":"Backpropagation from scratch"},{"content":"BackStory This is a simple fun little project that I did almost a year ago, At that time I used to see a lot of CodeBullet\u0026rsquo;s videos and wanted to learn the gist behind these evolutionary algorithms, and I can\u0026rsquo;t get into my head if I don\u0026rsquo;t do it from scratch so I wanted to implement this project from scratch. At that time, I wanted to document the learning and the implementation process, I even thought of making a youtube video about this topic but could not complete it, at that time, I had made some video animations about this process but could not complete it because I started doing something else and when I can back it was already too much mess and could not complete the video animations, but I\u0026rsquo;ll include the video animations that I had made earlier.\nSnake using Neural Network and Genetic Algorithm This is a simple demonstration of how neural network can be used along with genetic algorithm to play snake game. Now when I say its a snake game, this is just basically learning how to optimize a function, just like what we do with gradient descent. But that\u0026rsquo;s the interesting part we\u0026rsquo;re not using Gradient Descent, no fancy optimizers, even the initial weights were initialized randomly, I now wish(at the time of writing this blog) that I had used some initialization techniques like Xavier initializations, it would have learned to play so much faster. But even the random initialization works. Let\u0026rsquo;s get to the point.\nWe are playing a snake game let\u0026rsquo;s imagine that has a brain, So the brain here are weights which we initialize using Feed-forward layer and then we use genetic algorithm to evolve that brain(weights) and use this brain(weights) to predict which move to take(up, down, left or right) at every step in the game. So the term Neural Network here only represents the feed-forward layer, we don\u0026rsquo;t use any of the backpropogation.\nThe whole code for this project can be found here: Github\nDemo Video Explanation Following are the explanations that I\u0026rsquo;ve implemented for this project.\nSnake\u0026rsquo;s Eye It is what snake sees at each step while it\u0026rsquo;s playing game. Before performing anything, it has to see and pass it to the brain inorder to process what it has seen. So let\u0026rsquo;s understand what it sees at each step by looking at the video below.\nOur snake sees in 8 different directions(d1 to d8) as shown in the figure above. And at each direction it sees 3 different things Wall, Food, Body(it\u0026rsquo;s own body)\nSnake\u0026rsquo;s Brain Now that we have our snake eyes let\u0026rsquo;s make the brain to process the things that it has seen. We use a simple two layer neural network. The first layer has 8 hidden layers and the second is output layer of 4 units. We only use the feed forward part of neural network and discard all the other backpropagation part. We will later use genetic algorithm to improve our weights.\nThe figure below contains two parts\nSnake\u0026rsquo;s eye(descibed above), and Snake\u0026rsquo;s brain Input to neural network Now that we now our snake sees a body, wall and food. Let\u0026rsquo;s assign values to these metrics.\nSince the neural networks only understand numbers we represent them with a metric, 1 for favorable conditions and 0 for unfavorable.\nMetric for body: One is given when no body is present in that direction and Zero is given when no blocks are present in between Head and Body.\n$body = number of blocks between head and body/(total number of blocks - 1 )$.\nMetric for Wall: One is given when the number of blocks between Head and Wall are maximum and Zero is given when no blocks are present in between Head and Wall.\n$Wall = number of blocks between head and wall/(total number of blocks - 1 )$.\nMetric for Food: It is good for snake to move in the direction of food, so One is given when no blocks are present in between Head and Food and Zero is given when no food is present in that direction.\n$Food = ( total number of blocks - number of blocks between head and wall - 1)/(total number of blocks - 1 )$.\nThe another input to our snake is the it\u0026rsquo;s directions. It should keep. track of where it currently is to make move in another direction.\nValue for Head Direction: This is a simple one-hot encoding for the direction.\nBrain of Snake The brain of our snake is the weights inside our hidden layers and output layers.\nOur first hidden layer will have 8 * 28 weights with 8 bias. Our second layer will have 4 * 8 weights with 4 bias.\nSo, the total weights + bias counts to 268 which is the actual brain of our snake. So our neural network uses that brain to make a prediction in 4 directions. [Up, Right, Down, Left]\nGenetic Algorithm Since we do not use the backpropagation of our neural network to improve weights we will use use Genetic algorithm to improve weights.\nFive phases are considered in a genetic algorithm. You should first read these steps below and then you can come back to the video below.\nInitial brain (Population) We randomly generate the brain of size (1,268). Remember this is the initial phase where we initialize our weigths (brain) for our snake which described above. Later, this shape (1,268) is flattened into the structure of our feed-forward neural network which is described above.\nnp.random.choice(np.arange(-1,1, step =0.001),size = (population_size,weight_size), replace= True)\nweight size is 268 and population_size is the total number of snakes we want to train. I\u0026rsquo;ve trained 500 snakes in each generations. Thing to remember here is, each snake will have different brain. The random function above generates a linear vector which is later converted to matrices of sizes (8,28) for weights, (8,1) for bias and (4,8) for weights and (4,1) for bias using vector_to_matrix() function. The first two matrices is for hidden layer whereas the other two are for output layer.\nFitness function Now that we have 500 different snakes in each generation. We have to differentiate the great Snake from weak. Snake\u0026rsquo;s fitness is based on its score and number of steps it taken to achieve that score, so we created a function by using Steps and Score as variables which will helps the Snake in getting maximum score in less steps. I\u0026rsquo;ve used the following fitness function that evaluates fitness of snakes relative to score and steps taken.\nIf two snakes have the same score then the snake that achieves the score in less number of steps is considered.\nSelection This process selects the best fit snakes from the entire population according to their fitness value described above. It is then used produce a new population of offspring which will be used in a next generation. I\u0026rsquo;ve selected top 50 snakes initially according to their fitness value. The selected snakes will be called parents.\nCrossover We take the parents selected from the selection process to produce a new set of offsprings. We take 50 parents and iterate over (population_size - parent_length) times and use uniform crossover method to produce new offspring from these parents. we later add our 50 parents to the population set. This process will preserve the best fit snakes, even if the crossover and mutation yield bad set of population. The unifrom crossover can be explained likewise.\nMutation This example explains mutating by flipping bits.\n1\t0\t1\t0\t0\t1 0\n↓\n1\t0\t1\t0\t1\t1\t0\nBut in our case we change the value of our snake\u0026rsquo;s brain. Among 268 brain cells we will change 13 of them randomly between -0.5 to 0.5.\nRunning We run the game through many generation(similar to epoch) to evolve our snake\u0026rsquo;s brain by applying the method explained above. We do it until the snake has learned to score desired number of points.\nI know this is shitty writing but still Thank You for reading till the end. ","permalink":"https://cohlem.github.io/posts/using-genetic-algorithm-for-weights-optimization/","summary":"BackStory This is a simple fun little project that I did almost a year ago, At that time I used to see a lot of CodeBullet\u0026rsquo;s videos and wanted to learn the gist behind these evolutionary algorithms, and I can\u0026rsquo;t get into my head if I don\u0026rsquo;t do it from scratch so I wanted to implement this project from scratch. At that time, I wanted to document the learning and the implementation process, I even thought of making a youtube video about this topic but could not complete it, at that time, I had made some video animations about this process but could not complete it because I started doing something else and when I can back it was already too much mess and could not complete the video animations, but I\u0026rsquo;ll include the video animations that I had made earlier.","title":"Using Genetic Algorithm for Weights Optimization"},{"content":"This is my experience and experimentation that I did while building a product for the use case of using LLMs for our own data for question answering. If you\u0026rsquo;re doing something similar, this could be of some help.\nThe most commonly used methods while using LLMs with our own data were typically\nFine-tuning the model with your own data Using Retrieval Augmented Generation (RAG) techniques Fine-tuning the model with your own data This is the initial method and follows the general structure of training a model\nData preparation Train Evaluate For my task, I chose to train OpenAI\u0026rsquo;s davinci base model\nThere were mostly no hyperparameters to tune, as OpenAI takes care of it outside the box. Training the model involved more instruction tuning, instructing the model to act in a similar way, rather than just training the model to save data in its model weights. The effectiveness of instruction tuning mostly depended on the data preparation process. Data preparation involved formatting the data into pairs of \u0026lt;instruction, completion\u0026gt;.\nThis is a crucial step for better outputs and depends on the size of the training data. When the model was trained with a small amount of data, it mostly followed the instructions but had limited knowledge and memory of facts from the dataset. In most cases, while testing the model, it followed the instructions but often produced incorrect answers. When the scale of data was increased, it started to follow both the instructions and retain more knowledge. So, this process was crucial in identifying when training OpenAI\u0026rsquo;s models worked best. If you\u0026rsquo;re using it to train on a small scale of data, this would not yield the desired output, and I would recommend using the other process I employed for small-sized data.\nUsing Retrieval Augmented Generation (RAG) techniques This technique is sometimes referred to as In-context Learning. This is one of the most trending topics since the release of ChatGPT, and you might have heard the phrase \u0026ldquo;chat with your own data.\u0026rdquo; This process is simple yet very effective when you have your own small-scale data, which I used for question answering. The process involves:\nDividing the data into chunks Here, the format of the data doesn\u0026rsquo;t really matter. The only thing we need to take care of is the size of the chunks. The chunk size should always be smaller than the context length of LLMs, providing space for prompt and completion texts. In my case, the context length of gpt-3.5-turbo was 4,096 tokens, and I divided the chunks into token sizes of 1000. I chose a token size of 1000 for and retrieved top-3 chunks to be passed to the LLM.\nConverting the chunks into embeddings This process generates embeddings, which are vector representations of our chunks. I experimented with a couple of embedding models, each having its pros and cons. My recommendations for each of the embedding models are as follows:\nall-MiniLM-L12-v2: Useful when you need fast conversion from chunks to embeddings. It has a relatively small dimension of 384 and does a decent job in converting to embeddings.\nOpenAI\u0026rsquo;s text-embedding-ada-002: Useful when you need to generate highly accurate embeddings. If you are using it in real-time, it would be too slow due to its high dimension size of 1536, and API calling makes it even slower.\nInstructor: Useful when you need the accuracy level of text-embedding-ada-002 and fast conversion from text to embeddings. This model is blazingly fast and would save on cost when you embed lots of data.\nI went with the Instructor-XL model.\nStoring the embeddings Many vector database companies have risen around this use case of storing embeddings, such as Pinecone, Chroma, and many more. The trend is to follow the hype and opt for vector databases, which, in fact, are completely useless. If your embeddings are really big, I would recommend using vector databases; otherwise, for medium to small-scale data, ndarray would do a great job. I decided to just use a numpy array.\nRetrieval Relevant chunks are retrieved based on the similarity between the user\u0026rsquo;s query\u0026rsquo;s embeddings and the chunks\u0026rsquo; embeddings. Metrics like dot product, cosine similarity, are widely adopted, whereas cosine similarity would suffice. After evaluating the similarity, the top-k chunks are retrieved. We can use reranking to improve the quality of relevant chunks, but top-k would suffice. After retrieving, we are ready to feed the retrieved chunks and the user\u0026rsquo;s query to LLM by combining them with prompting.\nPrompting Prompting has been the most important step in getting the desired output. For me, this has turned out to be harder than doing research, writing code, and debugging combined. Writing quality prompts is hard. I experimented with a couple of prompting techniques in this project.\nFew-shot prompting, few-shot combined with Chain of Thought (CoT) prompting, but I couldn\u0026rsquo;t achieve the desired output. I noticed some problems with these techniques for my use case. These prompting techniques caused too many logic jumps, and the desired logic was never analyzed by the LLM. What worked for me was the map-reduce method and double prompting (which involves calling LLM twice, with 2 prompts, where the latter prompt is combined with the former LLM output). Both methods worked fine, with map-reduce being more expensive. I opted for double prompting and was able to generate the desired output. So, prompting has been challenging for me. Just a thought: maybe we should do some reverse engineering someday and train the prompts with the desired output as context, as described in the AUTOPROMPT paper 😅.\n","permalink":"https://cohlem.github.io/posts/llmvsincontext/","summary":"This is my experience and experimentation that I did while building a product for the use case of using LLMs for our own data for question answering. If you\u0026rsquo;re doing something similar, this could be of some help.\nThe most commonly used methods while using LLMs with our own data were typically\nFine-tuning the model with your own data Using Retrieval Augmented Generation (RAG) techniques Fine-tuning the model with your own data This is the initial method and follows the general structure of training a model","title":"Fine-tuning LLM vs In-context learning"},{"content":" ]]\n","permalink":"https://cohlem.github.io/sub-notes/maximum-likelihood-estimate-as-loss/maximum-likelihood-estimate-as-loss-function/","summary":"]]","title":"Maximum likelihood estimate as loss function"},{"content":"","permalink":"https://cohlem.github.io/sub-notes/template/","summary":"","title":"TITLE"}]