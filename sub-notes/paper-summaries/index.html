<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Papers Summaries | CohleM</title>
<meta name="keywords" content="">
<meta name="description" content="Papers that I&rsquo;ve read with their respective notes.
LLaMA: Open and Efficient Foundation Language Models Trained on 1.4T tokens. Wikipedia and Books domain trained for 2 epochs (maybe because its cleaner, smaller, offers coherent long sequences) use manual backprop for training efficiency i.e save checkpoints of activations that take longer to compute (linear layers) and use them during backprop and generate others such as (ReLu) on the fly. SmolLM2 including specific data eg.">
<meta name="author" content="CohleM">
<link rel="canonical" href="https://cohlem.github.io/sub-notes/paper-summaries/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css" integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe&#43;FVUFzPh7U=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://cohlem.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://cohlem.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://cohlem.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://cohlem.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://cohlem.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-X6LV4QY2G2"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-X6LV4QY2G2', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Papers Summaries" />
<meta property="og:description" content="Papers that I&rsquo;ve read with their respective notes.
LLaMA: Open and Efficient Foundation Language Models Trained on 1.4T tokens. Wikipedia and Books domain trained for 2 epochs (maybe because its cleaner, smaller, offers coherent long sequences) use manual backprop for training efficiency i.e save checkpoints of activations that take longer to compute (linear layers) and use them during backprop and generate others such as (ReLu) on the fly. SmolLM2 including specific data eg." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cohlem.github.io/sub-notes/paper-summaries/" /><meta property="article:section" content="sub-notes" />
<meta property="article:published_time" content="2025-01-21T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-01-21T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Papers Summaries"/>
<meta name="twitter:description" content="Papers that I&rsquo;ve read with their respective notes.
LLaMA: Open and Efficient Foundation Language Models Trained on 1.4T tokens. Wikipedia and Books domain trained for 2 epochs (maybe because its cleaner, smaller, offers coherent long sequences) use manual backprop for training efficiency i.e save checkpoints of activations that take longer to compute (linear layers) and use them during backprop and generate others such as (ReLu) on the fly. SmolLM2 including specific data eg."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Sub-notes",
      "item": "https://cohlem.github.io/sub-notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Papers Summaries",
      "item": "https://cohlem.github.io/sub-notes/paper-summaries/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Papers Summaries",
  "name": "Papers Summaries",
  "description": "Papers that I\u0026rsquo;ve read with their respective notes.\nLLaMA: Open and Efficient Foundation Language Models Trained on 1.4T tokens. Wikipedia and Books domain trained for 2 epochs (maybe because its cleaner, smaller, offers coherent long sequences) use manual backprop for training efficiency i.e save checkpoints of activations that take longer to compute (linear layers) and use them during backprop and generate others such as (ReLu) on the fly. SmolLM2 including specific data eg.",
  "keywords": [
    
  ],
  "articleBody": "Papers that I’ve read with their respective notes.\nLLaMA: Open and Efficient Foundation Language Models Trained on 1.4T tokens. Wikipedia and Books domain trained for 2 epochs (maybe because its cleaner, smaller, offers coherent long sequences) use manual backprop for training efficiency i.e save checkpoints of activations that take longer to compute (linear layers) and use them during backprop and generate others such as (ReLu) on the fly. SmolLM2 including specific data eg. math doesn’t only do well in math, but also seems to improve reasoning.\nrather than training on one specific dataset, training on mixture of datasets yields better results, for instance, 60-40 mixture of FineWeb-Edu and DCLM yielded almost similar performance to only training on FineWeb-Edu\ndecontamination of curated dataset is generally done, using some bi-gram matching using the eval dataset.\nthey do a multi-stage training approach rather than fixed-data mixture.\nLR decay\nWarmup Phase (Steps 0–2,000):\nLearning rate increases linearly from near 0 to 5.0×10−45.0×10−4. Stable Phase (Steps 2,000–N):\nLearning rate remains constant at 5.0×10−45.0×10−4. Decay Phase (Last 10% of Steps):\nLearning rate decreases linearly from 5.0×10−45.0×10−4 to 0. had loss spikes during stage 3, which remained persistent even after rewinding the trianing, and changing the data that caused the spike. The cause of spike remains undetermined, however the eval metrics recovered in the end.\nThey include high quality math data in the end, and decay the to 0\nThey expand the context length from 2k to 8k before the final 75 billion tokens of training and the mixture was adjusted to include 40% long-context documents\nthey curate their own instruction dataset named SmolTalk, because of low performance after training on previously available dataset i.e MagPie-Pro and OpenHermes2.5.\nFilter high conversational dataset and deduplicate using gte-large embedding models.\nin short they do a lot of decontamination (using bi-gram overlaps), deduplication, filtering,\nFor smaller models during sft, they filter smoltalk dataset (e.g., function calling) and hard examples from MagPie-Ultra to better align with the models’ capacity and do DPO on UltraFeedback dataset.\nTraining Compute-Optimal Large Language Models Given a fixed FLOPs budget, how should one trade-off model size and the number of training tokens? https://lifearchitect.ai/chinchilla/#deepmind\n",
  "wordCount" : "357",
  "inLanguage": "en",
  "datePublished": "2025-01-21T00:00:00Z",
  "dateModified": "2025-01-21T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "CohleM"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://cohlem.github.io/sub-notes/paper-summaries/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "CohleM",
    "logo": {
      "@type": "ImageObject",
      "url": "https://cohlem.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://cohlem.github.io" accesskey="h" title="CohleM (Alt + H)">CohleM</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://cohlem.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://cohlem.github.io">Home</a>&nbsp;»&nbsp;<a href="https://cohlem.github.io/sub-notes/">Sub-notes</a></div>
    <h1 class="post-title">
      Papers Summaries
    </h1>
    <div class="post-meta"><span title='2025-01-21 00:00:00 +0000 UTC'>January 21, 2025</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;CohleM

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#llama-open-and-efficient-foundation-language-modelshttpsarxivorgpdf230213971" aria-label="LLaMA: Open and Efficient Foundation Language Models"><a href="https://arxiv.org/pdf/2302.13971">LLaMA: Open and Efficient Foundation Language Models</a></a></li>
                <li>
                    <a href="#smollm2httpsarxivorgpdf250202737" aria-label="SmolLM2"><a href="https://arxiv.org/pdf/2502.02737">SmolLM2</a></a></li>
                <li>
                    <a href="#training-compute-optimal-large-language-modelshttpsarxivorgpdf220315556" aria-label="Training Compute-Optimal Large Language Models"><a href="https://arxiv.org/pdf/2203.15556">Training Compute-Optimal Large Language Models</a></a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Papers that I&rsquo;ve read with their respective notes.</p>
<h3 id="llama-open-and-efficient-foundation-language-modelshttpsarxivorgpdf230213971"><a href="https://arxiv.org/pdf/2302.13971">LLaMA: Open and Efficient Foundation Language Models</a><a hidden class="anchor" aria-hidden="true" href="#llama-open-and-efficient-foundation-language-modelshttpsarxivorgpdf230213971">#</a></h3>
<ul>
<li>Trained on 1.4T tokens.</li>
<li>Wikipedia and Books domain trained for 2 epochs (maybe because its cleaner, smaller, offers coherent long sequences)</li>
<li>use manual backprop for training efficiency i.e save checkpoints of activations that take longer to compute (linear layers) and use them during backprop and generate others such as (ReLu) on the fly.</li>
</ul>
<h3 id="smollm2httpsarxivorgpdf250202737"><a href="https://arxiv.org/pdf/2502.02737">SmolLM2</a><a hidden class="anchor" aria-hidden="true" href="#smollm2httpsarxivorgpdf250202737">#</a></h3>
<ul>
<li>
<p>including specific data eg. math doesn&rsquo;t only do well in math, but also seems to improve reasoning.</p>
</li>
<li>
<p>rather than training on one specific dataset, training on mixture of datasets yields better results, for instance, 60-40 mixture of FineWeb-Edu and DCLM yielded almost similar performance to only training on FineWeb-Edu</p>
</li>
<li>
<p>decontamination of curated dataset is generally done, using some bi-gram matching using the eval dataset.</p>
</li>
<li>
<p>they do a multi-stage training approach rather than fixed-data mixture.</p>
</li>
</ul>
<p>LR decay</p>
<ul>
<li>
<p><strong>Warmup Phase (Steps 0–2,000)</strong>:</p>
<ul>
<li>Learning rate increases linearly from near 0 to 5.0×10−45.0×10−4.</li>
</ul>
</li>
<li>
<p><strong>Stable Phase (Steps 2,000–N)</strong>:</p>
<ul>
<li>Learning rate remains constant at 5.0×10−45.0×10−4.</li>
</ul>
</li>
<li>
<p><strong>Decay Phase (Last 10% of Steps)</strong>:</p>
<ul>
<li>Learning rate decreases linearly from 5.0×10−45.0×10−4 to 0.</li>
</ul>
</li>
<li>
<p>had loss spikes during stage 3, which remained persistent even after rewinding the trianing, and changing the data that caused the spike. The cause of spike remains undetermined, however the eval metrics recovered in the end.</p>
</li>
<li>
<p>They include high quality math data in the end, and decay the to 0</p>
</li>
<li>
<p>They expand the context length from 2k to 8k before the final 75 billion tokens of training and the mixture was adjusted to include 40% long-context documents</p>
</li>
<li>
<p>they curate their own instruction dataset named SmolTalk, because of low performance after training on previously available dataset i.e MagPie-Pro and OpenHermes2.5.</p>
</li>
<li>
<p>Filter high conversational dataset and deduplicate using gte-large embedding models.</p>
</li>
<li>
<p>in short they do a lot of decontamination (using bi-gram overlaps), deduplication, filtering,</p>
</li>
<li>
<p>For smaller models during sft, they filter smoltalk dataset (e.g., function calling) and hard examples from MagPie-Ultra to better align with the models’ capacity and do DPO on UltraFeedback dataset.</p>
</li>
</ul>
<p><img loading="lazy" src="p2.png" alt="p2"  />

<img loading="lazy" src="p3.png" alt="p3"  />
</p>
<h3 id="training-compute-optimal-large-language-modelshttpsarxivorgpdf220315556"><a href="https://arxiv.org/pdf/2203.15556">Training Compute-Optimal Large Language Models</a><a hidden class="anchor" aria-hidden="true" href="#training-compute-optimal-large-language-modelshttpsarxivorgpdf220315556">#</a></h3>
<ul>
<li>Given a fixed FLOPs budget, how should one trade-off model size and the number of training tokens?</li>
</ul>
<p><a href="https://lifearchitect.ai/chinchilla/#deepmind">https://lifearchitect.ai/chinchilla/#deepmind</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://cohlem.github.io/sub-notes/tokenization/">
    <span class="title">« Prev</span>
    <br>
    <span>Tokenization</span>
  </a>
  <a class="next" href="https://cohlem.github.io/sub-notes/kv-cache-gqa/">
    <span class="title">Next »</span>
    <br>
    <span>KV cache and Grouped Query Attention</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://cohlem.github.io">CohleM</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
