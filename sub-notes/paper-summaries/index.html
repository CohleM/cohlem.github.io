<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Papers Summaries | CohleM</title>
<meta name="keywords" content="">
<meta name="description" content="Papers that I&rsquo;ve read with their respective notes.
LLaMA: Open and Efficient Foundation Language Models Trained on 1.4T tokens. Wikipedia and Books domain trained for 2 epochs (maybe because its cleaner, smaller, offers coherent long sequences) use manual backprop for training efficiency i.e save checkpoints of activations that take longer to compute (linear layers) and use them during backprop and generate others such as (ReLu) on the fly. SmolLM2 including specific data eg.">
<meta name="author" content="CohleM">
<link rel="canonical" href="https://cohlem.github.io/sub-notes/paper-summaries/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css" integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe&#43;FVUFzPh7U=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://cohlem.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://cohlem.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://cohlem.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://cohlem.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://cohlem.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script type="text/javascript">
  MathJax = {
    tex: {
      inlineMath: [["\\(", "\\)"], ["$", "$"]],
      displayMath: [["\\[", "\\]"], ["$$", "$$"]]
    },
    options: {
      skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"]
    }
  };
</script>



<script async src="https://www.googletagmanager.com/gtag/js?id=G-X6LV4QY2G2"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-X6LV4QY2G2', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Papers Summaries" />
<meta property="og:description" content="Papers that I&rsquo;ve read with their respective notes.
LLaMA: Open and Efficient Foundation Language Models Trained on 1.4T tokens. Wikipedia and Books domain trained for 2 epochs (maybe because its cleaner, smaller, offers coherent long sequences) use manual backprop for training efficiency i.e save checkpoints of activations that take longer to compute (linear layers) and use them during backprop and generate others such as (ReLu) on the fly. SmolLM2 including specific data eg." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cohlem.github.io/sub-notes/paper-summaries/" /><meta property="article:section" content="sub-notes" />
<meta property="article:published_time" content="2025-01-21T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-01-21T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Papers Summaries"/>
<meta name="twitter:description" content="Papers that I&rsquo;ve read with their respective notes.
LLaMA: Open and Efficient Foundation Language Models Trained on 1.4T tokens. Wikipedia and Books domain trained for 2 epochs (maybe because its cleaner, smaller, offers coherent long sequences) use manual backprop for training efficiency i.e save checkpoints of activations that take longer to compute (linear layers) and use them during backprop and generate others such as (ReLu) on the fly. SmolLM2 including specific data eg."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Sub-notes",
      "item": "https://cohlem.github.io/sub-notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Papers Summaries",
      "item": "https://cohlem.github.io/sub-notes/paper-summaries/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Papers Summaries",
  "name": "Papers Summaries",
  "description": "Papers that I\u0026rsquo;ve read with their respective notes.\nLLaMA: Open and Efficient Foundation Language Models Trained on 1.4T tokens. Wikipedia and Books domain trained for 2 epochs (maybe because its cleaner, smaller, offers coherent long sequences) use manual backprop for training efficiency i.e save checkpoints of activations that take longer to compute (linear layers) and use them during backprop and generate others such as (ReLu) on the fly. SmolLM2 including specific data eg.",
  "keywords": [
    
  ],
  "articleBody": "Papers that I’ve read with their respective notes.\nLLaMA: Open and Efficient Foundation Language Models Trained on 1.4T tokens. Wikipedia and Books domain trained for 2 epochs (maybe because its cleaner, smaller, offers coherent long sequences) use manual backprop for training efficiency i.e save checkpoints of activations that take longer to compute (linear layers) and use them during backprop and generate others such as (ReLu) on the fly. SmolLM2 including specific data eg. math doesn’t only do well in math, but also seems to improve reasoning.\nrather than training on one specific dataset, training on mixture of datasets yields better results, for instance, 60-40 mixture of FineWeb-Edu and DCLM yielded almost similar performance to only training on FineWeb-Edu\ndecontamination of curated dataset is generally done, using some bi-gram matching using the eval dataset.\nthey do a multi-stage training approach rather than fixed-data mixture.\nLR decay\nWarmup Phase (Steps 0–2,000):\nLearning rate increases linearly from near 0 to 5.0×10−45.0×10−4. Stable Phase (Steps 2,000–N):\nLearning rate remains constant at 5.0×10−45.0×10−4. Decay Phase (Last 10% of Steps):\nLearning rate decreases linearly from 5.0×10−45.0×10−4 to 0. had loss spikes during stage 3, which remained persistent even after rewinding the trianing, and changing the data that caused the spike. The cause of spike remains undetermined, however the eval metrics recovered in the end.\nThey include high quality math data in the end, and decay the to 0\nThey expand the context length from 2k to 8k before the final 75 billion tokens of training and the mixture was adjusted to include 40% long-context documents\nthey curate their own instruction dataset named SmolTalk, because of low performance after training on previously available dataset i.e MagPie-Pro and OpenHermes2.5.\nFilter high conversational dataset and deduplicate using gte-large embedding models.\nin short they do a lot of decontamination (using bi-gram overlaps), deduplication, filtering,\nFor smaller models during sft, they filter smoltalk dataset (e.g., function calling) and hard examples from MagPie-Ultra to better align with the models’ capacity and do DPO on UltraFeedback dataset.\nSWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks backend questions Train set : question, reference answer, 10 sample tests generate trajectories from the Train set, sample sequence is like this\nquestion, agent’s answer, human simulator’s answer–\u003e agent’s answer, human simulator’s answer –\u003e end. run the final solution through their 10 sample tests–\u003e record reward. 1 if passed all test else 0. sample 15k of these\nnow train advantage llm using bradley terry loss. $o_t^+$ is from those trajectories which had higher reward,\nNow train the policy using DPO loss, use that 15k samples trajectories and each\nfor each $o_t$ sample 16 $a_t$ then rate it using advantage llm, take top50% as $a_+$ remaining as $a_-$ then calculate loss for those actions. $\\log \\pi (a^+|o_t)$ is the joint probability of all the tokens in $a^+$ Notably, process-only filtering consistently yields the highest accuracy, suggesting that focusing on the procedural aspects of data refinement is more important than the correctness of a training trajectory.\nprocess filtering (filtering trajectory based on whether its action at step a is plausible or not) yields better performance.\nfiltering for correctness usually harms performance (filtering based on its final answer)\n$$ r_t = –| log p_θ(A_truth_t | , A_truth_{",
  "wordCount" : "697",
  "inLanguage": "en",
  "datePublished": "2025-01-21T00:00:00Z",
  "dateModified": "2025-01-21T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "CohleM"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://cohlem.github.io/sub-notes/paper-summaries/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "CohleM",
    "logo": {
      "@type": "ImageObject",
      "url": "https://cohlem.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://cohlem.github.io" accesskey="h" title="CohleM (Alt + H)">CohleM</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://cohlem.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/random/" title="Random">
                    <span>Random</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://cohlem.github.io">Home</a>&nbsp;»&nbsp;<a href="https://cohlem.github.io/sub-notes/">Sub-notes</a></div>
    <h1 class="post-title">
      Papers Summaries
    </h1>
    <div class="post-meta"><span title='2025-01-21 00:00:00 +0000 UTC'>January 21, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;CohleM

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#llama-open-and-efficient-foundation-language-modelshttpsarxivorgpdf230213971" aria-label="LLaMA: Open and Efficient Foundation Language Models"><a href="https://arxiv.org/pdf/2302.13971">LLaMA: Open and Efficient Foundation Language Models</a></a></li>
                <li>
                    <a href="#smollm2httpsarxivorgpdf250202737" aria-label="SmolLM2"><a href="https://arxiv.org/pdf/2502.02737">SmolLM2</a></a></li>
                <li>
                    <a href="#sweet-rl-training-multi-turn-llm-agents-on-collaborative-reasoning-taskshttpsarxivorgabs250315478" aria-label="SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks"><a href="https://arxiv.org/abs/2503.15478">SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks</a></a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Papers that I&rsquo;ve read with their respective notes.</p>
<h3 id="llama-open-and-efficient-foundation-language-modelshttpsarxivorgpdf230213971"><a href="https://arxiv.org/pdf/2302.13971">LLaMA: Open and Efficient Foundation Language Models</a><a hidden class="anchor" aria-hidden="true" href="#llama-open-and-efficient-foundation-language-modelshttpsarxivorgpdf230213971">#</a></h3>
<ul>
<li>Trained on 1.4T tokens.</li>
<li>Wikipedia and Books domain trained for 2 epochs (maybe because its cleaner, smaller, offers coherent long sequences)</li>
<li>use manual backprop for training efficiency i.e save checkpoints of activations that take longer to compute (linear layers) and use them during backprop and generate others such as (ReLu) on the fly.</li>
</ul>
<h3 id="smollm2httpsarxivorgpdf250202737"><a href="https://arxiv.org/pdf/2502.02737">SmolLM2</a><a hidden class="anchor" aria-hidden="true" href="#smollm2httpsarxivorgpdf250202737">#</a></h3>
<ul>
<li>
<p>including specific data eg. math doesn&rsquo;t only do well in math, but also seems to improve reasoning.</p>
</li>
<li>
<p>rather than training on one specific dataset, training on mixture of datasets yields better results, for instance, 60-40 mixture of FineWeb-Edu and DCLM yielded almost similar performance to only training on FineWeb-Edu</p>
</li>
<li>
<p>decontamination of curated dataset is generally done, using some bi-gram matching using the eval dataset.</p>
</li>
<li>
<p>they do a multi-stage training approach rather than fixed-data mixture.</p>
</li>
</ul>
<p>LR decay</p>
<ul>
<li>
<p><strong>Warmup Phase (Steps 0–2,000)</strong>:</p>
<ul>
<li>Learning rate increases linearly from near 0 to 5.0×10−45.0×10−4.</li>
</ul>
</li>
<li>
<p><strong>Stable Phase (Steps 2,000–N)</strong>:</p>
<ul>
<li>Learning rate remains constant at 5.0×10−45.0×10−4.</li>
</ul>
</li>
<li>
<p><strong>Decay Phase (Last 10% of Steps)</strong>:</p>
<ul>
<li>Learning rate decreases linearly from 5.0×10−45.0×10−4 to 0.</li>
</ul>
</li>
<li>
<p>had loss spikes during stage 3, which remained persistent even after rewinding the trianing, and changing the data that caused the spike. The cause of spike remains undetermined, however the eval metrics recovered in the end.</p>
</li>
<li>
<p>They include high quality math data in the end, and decay the to 0</p>
</li>
<li>
<p>They expand the context length from 2k to 8k before the final 75 billion tokens of training and the mixture was adjusted to include 40% long-context documents</p>
</li>
<li>
<p>they curate their own instruction dataset named SmolTalk, because of low performance after training on previously available dataset i.e MagPie-Pro and OpenHermes2.5.</p>
</li>
<li>
<p>Filter high conversational dataset and deduplicate using gte-large embedding models.</p>
</li>
<li>
<p>in short they do a lot of decontamination (using bi-gram overlaps), deduplication, filtering,</p>
</li>
<li>
<p>For smaller models during sft, they filter smoltalk dataset (e.g., function calling) and hard examples from MagPie-Ultra to better align with the models’ capacity and do DPO on UltraFeedback dataset.</p>
</li>
</ul>
<p><img loading="lazy" src="p2.png" alt="p2"  />

<img loading="lazy" src="p3.png" alt="p3"  />
</p>
<h3 id="sweet-rl-training-multi-turn-llm-agents-on-collaborative-reasoning-taskshttpsarxivorgabs250315478"><a href="https://arxiv.org/abs/2503.15478">SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks</a><a hidden class="anchor" aria-hidden="true" href="#sweet-rl-training-multi-turn-llm-agents-on-collaborative-reasoning-taskshttpsarxivorgabs250315478">#</a></h3>
<p>backend questions Train set : question, reference answer, 10 sample tests
generate trajectories from the Train set, sample sequence is like this</p>
<p>question, agent&rsquo;s answer, human simulator&rsquo;s answer&ndash;&gt; agent&rsquo;s answer, human simulator&rsquo;s answer &ndash;&gt; end. run the final solution through their 10 sample tests&ndash;&gt; record reward. 1 if passed all test else 0. sample 15k of these</p>
<p>now train advantage llm using bradley terry loss.
<img loading="lazy" src="p4.png" alt="p4"  />

$o_t^+$ is from those trajectories which had higher reward,</p>
<p><img loading="lazy" src="p5.png" alt="p5"  />
</p>
<p>Now train the policy using DPO loss,
use that 15k samples trajectories and each</p>
<p>for each $o_t$ sample 16 $a_t$ then rate it using advantage llm, take top50% as $a_+$ remaining as $a_-$
then calculate loss for those actions. $\log \pi (a^+|o_t)$ is the joint probability of all the tokens in $a^+$
<img loading="lazy" src="p6.png" alt="p6"  />
</p>
<p>Notably, process-only filtering consistently yields the highest accuracy, suggesting that focusing on the procedural aspects of data refinement is more important than the correctness of a training trajectory.</p>
<p>process filtering (filtering trajectory based on whether its action at step a is plausible or not) yields better performance.</p>
<p>filtering for correctness usually harms performance (filtering based on its final answer)</p>
<p>$$   r_t = –| log p_θ(A_truth_t | <!-- raw HTML omitted -->, A_truth_{&lt;t})– log p_θ(ŷ_t        | <!-- raw HTML omitted -->, ŷ_{&lt;t}) |$$</p>
<p>veRL algorithm
just to know how the variables provided in the .sh file play out in the main algorithm.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> total_epoch:
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">for</span> batch <span style="color:#f92672">in</span> train_dataloader: <span style="color:#75715e"># each batch size is provided by train_batch_size</span>
</span></span><span style="display:flex;"><span>		generate_rollout() <span style="color:#75715e"># if GRPO use actor.rollout.n variable</span>
</span></span><span style="display:flex;"><span>		generate_old_logprobs()
</span></span><span style="display:flex;"><span>		generate_ref_logprobs()
</span></span><span style="display:flex;"><span>		calculate_advantages()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># split batch into mini_batches.</span>
</span></span><span style="display:flex;"><span>		minibatch_dataloader <span style="color:#f92672">=</span> batch<span style="color:#f92672">.</span>split(ppo_mini_batch_size) <span style="color:#75715e"># this is a dataloader with each minibatch of size ppo_mini_batch_size</span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">for</span> e <span style="color:#f92672">in</span> ppo_epoch:
</span></span><span style="display:flex;"><span>			<span style="color:#66d9ef">for</span> minibatch <span style="color:#f92672">in</span> minibatch_dataloader:
</span></span><span style="display:flex;"><span>				<span style="color:#75715e">#split minibatch into microbatches if needed to train on different GPUs</span>
</span></span><span style="display:flex;"><span>				micro_batches <span style="color:#f92672">=</span> minibatch<span style="color:#f92672">.</span>split(ppo_micro_batch_size_per_gpu)
</span></span><span style="display:flex;"><span>				gradient_accumulation <span style="color:#f92672">=</span> ppo_mini_batch_size <span style="color:#f92672">//</span> ppo_micro_batch_size_per_gpu
</span></span><span style="display:flex;"><span>				<span style="color:#66d9ef">for</span> data <span style="color:#f92672">in</span> micro_batches:
</span></span><span style="display:flex;"><span>					generate_logprobs()
</span></span><span style="display:flex;"><span>					loss <span style="color:#f92672">=</span> calculate_ppo_loss() <span style="color:#f92672">/</span> gradient_accumulation
</span></span><span style="display:flex;"><span>					loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>				optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>				
</span></span><span style="display:flex;"><span>		
</span></span></code></pre></div><p>gradient_accumulation step is not used in a sense that we generally do while pretraining, it just maintains the count total number of micro batches that are processed in separate GPU, by dividing loss by gradient_accumulation we obtain loss as if the minibatch was processed directly without using any micro batch splits</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://cohlem.github.io/sub-notes/tokenization/">
    <span class="title">« Prev</span>
    <br>
    <span>Tokenization</span>
  </a>
  <a class="next" href="https://cohlem.github.io/sub-notes/kv-cache-gqa/">
    <span class="title">Next »</span>
    <br>
    <span>KV cache and Grouped Query Attention</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://cohlem.github.io">CohleM</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
