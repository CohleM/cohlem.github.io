<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Sub-notes on CohleM</title>
    <link>https://cohlem.github.io/sub-notes/</link>
    <description>Recent content in Sub-notes on CohleM</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 28 Apr 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://cohlem.github.io/sub-notes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Mixture of Experts</title>
      <link>https://cohlem.github.io/sub-notes/mixture-of-experts/</link>
      <pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/mixture-of-experts/</guid>
      <description>Image Source: https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts
Basic MoE structure Experts are FFNN themselves, instead of passing input representation to only one dense FFNN we now have option to route them to more FFNNs. Since most LLMs have several decoder blocks, a given text will pass through multiple experts before the text is generated.
Down the line it could use multiple experts but at different blocks i.e (layers)
A routing layer is set to choose experts depending on how many experts are selected MoE are categorized into two i.</description>
    </item>
    
    <item>
      <title>Multi-head latent attention</title>
      <link>https://cohlem.github.io/sub-notes/multi-head-latent-attention/</link>
      <pubDate>Mon, 28 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/multi-head-latent-attention/</guid>
      <description>Scaled-dot product Attention Q1 Given the attention equation $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{(xWq)(xWk)^\top}{\sqrt{d_k}}\right)(xWv)W_O $$ Why don&amp;rsquo;t we train by combining $WqWk^\top$ and $WvWo$? because mathematically they seem equivalent $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{x(WqWk^\top)x^\top}{\sqrt{d_k}}\right)x(WvW_O) $$ I initially thought if we could combine those weights, we don&amp;rsquo;t need to calculate $Q,K,V$ meaning there will be less number of matrix multiplication.
Answer We lose the objective of $Q,K,V,O$, they are meant to operate independently.</description>
    </item>
    
    <item>
      <title>math reasoning</title>
      <link>https://cohlem.github.io/sub-notes/math-reasoning/</link>
      <pubDate>Tue, 22 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/math-reasoning/</guid>
      <description>behaviors:
Error recognition and Correction Knowledge Tom plays 6 rounds of golf. He takes an average of 7 strokes per hole. The par value per hole is 5. How many strokes over par was he?&amp;lt;｜Assistant｜&amp;gt; Okay, so Tom is playing golf, and he&amp;rsquo;s done 6 rounds. Hmm, that&amp;rsquo;s a lot, but I guess that&amp;rsquo;s how it goes. Anyway, he takes an average of 7 strokes per hole. Wait, average strokes per hole?</description>
    </item>
    
    <item>
      <title>Steering Vectors</title>
      <link>https://cohlem.github.io/sub-notes/steering-vectors/</link>
      <pubDate>Tue, 08 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/steering-vectors/</guid>
      <description>LoReFT equation
ΦLoReFT​(h)=h+R⊤(Wh+b−Rh)
Analogy
hh: The original photo (pixels, colors, brightness).
RR: A filter that only adjusts &amp;ldquo;warmth&amp;rdquo; and &amp;ldquo;contrast&amp;rdquo; (ignores other settings).
Wh+bWh+b: The ideal warmth/contrast (e.g., &amp;ldquo;make it sunset-like&amp;rdquo;).
RhRh: Current warmth/contrast (e.g., too cold and flat).
Difference: &amp;ldquo;Increase warmth by +20, contrast by +15&amp;rdquo;.
R⊤R⊤: Applies those changes only to warmth/contrast (leaves sharpness unchanged).
New hh: A warmer, more vibrant photo!
Why it fits: LoReFT surgically edits specific features without altering the whole image.</description>
    </item>
    
    <item>
      <title>LoRA</title>
      <link>https://cohlem.github.io/sub-notes/lora/</link>
      <pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/lora/</guid>
      <description>LoRA Main idea is to approximate the change in weights dW by the use of low-rank matrices
Eg: Usually the weight update is done by adding the change in weights dW to the original weight matrix W. dW is obtained through backpropagation, ex if W is 512 x 512 the parameter size of dW is 262,144.
In LoRA, we approximate that dW but by breaking down into two low rank matrices B @ A where B = matrix of size 512 x r and A = matrix of size r x 512,</description>
    </item>
    
    <item>
      <title>Interpretability</title>
      <link>https://cohlem.github.io/sub-notes/interpretability/</link>
      <pubDate>Mon, 03 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/interpretability/</guid>
      <description>Induction circuits Induction behaviour The task of detecting and repeating subsequences in a text by finding some patterns.
For example: If there exist a text containing name &amp;ldquo;James Bond&amp;rdquo; and later in the text when the model sees the word &amp;ldquo;James&amp;rdquo; it predicts/repeats the word &amp;ldquo;Bond&amp;rdquo; because it&amp;rsquo;s already seen the words &amp;ldquo;James Bond&amp;rdquo; and analyzes that &amp;ldquo;bond&amp;rdquo; should come after the word &amp;ldquo;James&amp;rdquo;. Also called &amp;ldquo;Strict Induction&amp;rdquo;
Induction head A head which implements the induction behaviour.</description>
    </item>
    
    <item>
      <title>RLHF</title>
      <link>https://cohlem.github.io/sub-notes/rlhf/</link>
      <pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/rlhf/</guid>
      <description> How&amp;rsquo;s RLHF different from RL setup
No state transition happen, generation of one state does not affect another. Switching from a reward function to a reward model. reward model could be any classification model. </description>
    </item>
    
    <item>
      <title>Flops calculation</title>
      <link>https://cohlem.github.io/sub-notes/flops-calculation/</link>
      <pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/flops-calculation/</guid>
      <description>Calculation of FLOPs multiply accumulate cost: 2FLOPS i.e 1 for multiplication and 1 for accumulation (addition) if we multiply two matrices with sizes (a x b) and (b x c), the flops involved is b Multiply-add operation per the output size (a x c) i.e 2 x b x (a x c) Embedding lookup we initially have tokens with (seq_len,vocab_size) one-hot representation and embedding lookup matrix is (vocab_size, d_model), it will take</description>
    </item>
    
    <item>
      <title>Post Training Strategies</title>
      <link>https://cohlem.github.io/sub-notes/post-training-strategies/</link>
      <pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/post-training-strategies/</guid>
      <description>After training, we generally perform alignment i.e teaching the model how to behave/act in desired manner. Post training mainly consists 1) Supervised Fine-tuning 2) RLHF
the current consensus within the research community seems to be that the optimal approach to alignment is to i) perform SFT over a moderately-sized dataset of examples with very high quality and ii) invest remaining efforts into curating human preference data for fine-tuning via RLHF.</description>
    </item>
    
    <item>
      <title>Notes-while-building-lilLM</title>
      <link>https://cohlem.github.io/sub-notes/building-lillm/</link>
      <pubDate>Wed, 29 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/building-lillm/</guid>
      <description>Pre-training Document packing while pretraining, different documents could be packed inside a sequence. For instance, a model with context_length 1024 can have 256 tokens from one doc and rest from the other. Demilited by EOS token.
The samples may contaminate the attention, for which cross sample attention masking is used. But, it isn&amp;rsquo;t used by DeepSeek v3, lets not use it.
while packing documents. we simply pack them as they appear in order and then add EOS token (used by GPT-2,3).</description>
    </item>
    
    <item>
      <title>Pytorch Commands I forget time to time/ commands that are essential</title>
      <link>https://cohlem.github.io/sub-notes/pytorch/</link>
      <pubDate>Wed, 29 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/pytorch/</guid>
      <description>torch.stack(tensors, dim) stacks the tensors across dim
#usage # data has to be tensor torch.stack([data[i:i+some_number] for i in range(10)]) torch.from_numpy(numpy_array) shares the memory with the numpy_array but is tensor type
a = np.array([1,2,3]) b = torch.tensor(a) # creates copy c = torch.from_numpy(a) # shares memory a[0] = 11 c # outputs: tensor([11, 2, 3]) torch.flatten(input, start,end=-1) flattens the input from dim start to end (-1 by default)
t = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]) torch.</description>
    </item>
    
    <item>
      <title>Tokenization</title>
      <link>https://cohlem.github.io/sub-notes/tokenization/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/tokenization/</guid>
      <description>Unicode Character encoding standard aims to incorporate all the available digital characters Each character in Unicode has a unique 4 to 6-digit hexadecimal number. For Example, the letter &amp;lsquo;A&amp;rsquo; has the code 0041, represented as U+0041. compatible with ASCII first 128 characters in Unicode directly correspond to the characters represented in the 7-bit ASCII table Unicode Transformation Format (UTF-8) uses 1-4 bytes to represent each character can encode all the unicode code points backward compatible with ASCII Example: (1 byte) The character &amp;#39;A&amp;#39; (U+0041) is encoded as `01000001` (0x41 in hexadecimal).</description>
    </item>
    
    <item>
      <title>Papers Summaries</title>
      <link>https://cohlem.github.io/sub-notes/paper-summaries/</link>
      <pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/paper-summaries/</guid>
      <description>Papers that I&amp;rsquo;ve read with their respective notes.
LLaMA: Open and Efficient Foundation Language Models Trained on 1.4T tokens. Wikipedia and Books domain trained for 2 epochs (maybe because its cleaner, smaller, offers coherent long sequences) use manual backprop for training efficiency i.e save checkpoints of activations that take longer to compute (linear layers) and use them during backprop and generate others such as (ReLu) on the fly. SmolLM2 including specific data eg.</description>
    </item>
    
    <item>
      <title>KV cache and Grouped Query Attention</title>
      <link>https://cohlem.github.io/sub-notes/kv-cache-gqa/</link>
      <pubDate>Sat, 18 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/kv-cache-gqa/</guid>
      <description>KV Cache KV cache visual operation In the note blow, I first describe how inferencing is done if we simply do operation without KV cache and then describe how KV cache helps removing redundant operations.
We don&amp;rsquo;t make use of KV cache while training because we already have data filled for each sequence length, we don&amp;rsquo;t need to calculate loss one by one, instead we do it in batches, whereas while inferencing we do it generally for 1 batch with some sequences and then we keep on appending next-predicted token to that sequence one by one.</description>
    </item>
    
    <item>
      <title>RMSNorm</title>
      <link>https://cohlem.github.io/sub-notes/rmsnorm/</link>
      <pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/rmsnorm/</guid>
      <description>Recap of LayerNorm let&amp;rsquo;s first recap by understanding why LayerNorm was used:
We needed to balance the distribution of inputs (internal covariance shift) i.e we want inputs to be roughly gaussian (mean 0, std 1), it not maintained it would result in zeroing out the gradients. output of some blocks (transformer block) may produce large values or very small values that would result in either exploding or vanishing gradient problem, in order to have stable training, we needed to have stable range for those outputs.</description>
    </item>
    
    <item>
      <title>RoPE</title>
      <link>https://cohlem.github.io/sub-notes/rope/</link>
      <pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/rope/</guid>
      <description>Recap of Absolute PE We previously used absolute positional embedding in our GPT-2 model.
Disadvantages No notion of relative information between tokens doesn&amp;rsquo;t work for sequences larger than context length the model is trained with, because we run out of token embeddings for tokens that come at sequence larger than the context length. RoPE pre-requisites This is how we rotate a point by an angel theta in a two dimensional space and this is all we need in RoPE.</description>
    </item>
    
    <item>
      <title>GPUs</title>
      <link>https://cohlem.github.io/sub-notes/gpus/</link>
      <pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/gpus/</guid>
      <description>GPU physcial structure let&amp;rsquo;s first understand the structure of GPU.
Inside a GPU it has a chip named GA102 (depends on architecture, this is for ampere architecture) built from 28.3million transistors (semiconductor device that can switch or amplify electrical signals) and majority area covered by processing cores. processing core is divide into seven Graphics processing clusters (GPCs)
among each GPC there are 12 Streaming Multiprocessors. Inside each SM there are 4 warps and 1 Raytracing core inside a warp there are 32 Cudas and 1 Tensor Core.</description>
    </item>
    
    <item>
      <title>DDP and gradient sync</title>
      <link>https://cohlem.github.io/sub-notes/ddp/</link>
      <pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/ddp/</guid>
      <description>When we have enough resources we would want to train our neural networks in parallel, the way to do this is to train our NN with different data (different batches of data) in each GPU in parallel. For instance, if we have 8X A100 we run 8 different batches of data on each A100 GPU.
The way to do this in pytorch is to use DDP (take a look into their docs)</description>
    </item>
    
    <item>
      <title>Gradient Accumulation</title>
      <link>https://cohlem.github.io/sub-notes/gradient-accumulation/</link>
      <pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/gradient-accumulation/</guid>
      <description>Gradient Accumulation When we want to train a neural network with some predefined set of tokens, but don&amp;rsquo;t have enough GPU resources, what do we do?
Gradient Accumulation We simply accumulate the gradients. For instance, in order to reproduce GPT-2 124B, we need to train the model with 0.5 Million tokens in a single run with 1024 context length, we would need 0.5e6/ 1024 = 488 batches i.e B,T = (488,1024) to calculate the gradients and update them.</description>
    </item>
    
    <item>
      <title>Training Speed Optimization</title>
      <link>https://cohlem.github.io/sub-notes/training-speed-optimization/</link>
      <pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/training-speed-optimization/</guid>
      <description>Precision The more the precision point the less operation (TFLOPS) is performed.
FP64 used for scientific research purposes, where precision is a must. TF32 and BFLOAT16 are mostly used in NN training. INT8 is used for inference. Picture below shows specifications of A100 GPU.
Using these precision points may have some difference in code. See pytorch&amp;rsquo;s docs
torch.compile It works in a similar fashion like the GCC compiler. It works by reducing overheads introduced by the python interpreter and optimizing the GPU read and writes.</description>
    </item>
    
    <item>
      <title>skip-connections</title>
      <link>https://cohlem.github.io/sub-notes/skip-connections/</link>
      <pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/skip-connections/</guid>
      <description>Skip connections are simply skipping the layers by adding the identity of input to it&amp;rsquo;s output as shown in the figure below.
Why add the identity of input x to the output ? We calculate the gradients of parameters using chain rule, as shown in figure above. For deeper layers the gradient start to become close to 0 and the gradient stops propagating, which is a vanishing gradient problem in a deep neural networks.</description>
    </item>
    
    <item>
      <title>Optimization Algorithms (SGD with momentum, RMSProp, Adam)</title>
      <link>https://cohlem.github.io/sub-notes/optimization-algorithms/</link>
      <pubDate>Fri, 27 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/optimization-algorithms/</guid>
      <description>The simplest algorithm is the gradient descent in which we simply calculate loss over all the training data and then update our parameters, but it would be too slow and would consume too much resources. A faster approach is to use SGD where we calculate loss over every single training data and then do the parameter update, but the gradient update could be fuzzy. A more robust approach is to do mini batch SGD.</description>
    </item>
    
    <item>
      <title>manual-backpropagation-on-tensors</title>
      <link>https://cohlem.github.io/sub-notes/manual-backpropagation-on-tensors/</link>
      <pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/manual-backpropagation-on-tensors/</guid>
      <description>Main code n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 64 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) # Layer 1 W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) b1 = torch.randn(n_hidden, generator=g) * 0.1 # using b1 just for fun, it&amp;#39;s useless because of BN # Layer 2 W2 = torch.</description>
    </item>
    
    <item>
      <title>Matrix Visualization</title>
      <link>https://cohlem.github.io/sub-notes/matrix-visualization/</link>
      <pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/matrix-visualization/</guid>
      <description>In deep learning, it&amp;rsquo;s important to visualize a matrix and how it is represented in a dimension space because the operations that we perform on those matrix becomes very much intuitive afterwards.
Visualizing two dimensional matrix. This has to be the most intuitive visualization.
[ [12, 63, 10, 42, 70, 31, 34, 8, 34, 5], [10, 97, 100, 39, 64, 25, 86, 22, 31, 25], [28, 44, 82, 61, 70, 94, 22, 88, 89, 56] ] We can simply imagine rows are some examples and columns as those examples&amp;rsquo; features.</description>
    </item>
    
    <item>
      <title>Diagnostic-tool-while-training-nn</title>
      <link>https://cohlem.github.io/sub-notes/diagnostic-tool-while-training-nn/</link>
      <pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/diagnostic-tool-while-training-nn/</guid>
      <description>source: Building makemore Part 3: Activations &amp;amp; Gradients, BatchNorm
Things to look out for while training NN Take a look at previous notes to understand this note better
consider we have this simple 6 layer NN
# Linear Layer g = torch.Generator().manual_seed(2147483647) # for reproducibility class Layer: def __init__(self,fan_in, fan_out, bias=False): self.w = torch.randn((fan_in, fan_out),generator = g) / (fan_in)**(0.5) # applying kaiming init self.bias = bias if bias: self.b = torch.</description>
    </item>
    
    <item>
      <title>BatchNormalization</title>
      <link>https://cohlem.github.io/sub-notes/batchnormalization/</link>
      <pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/batchnormalization/</guid>
      <description>As we saw in our previous note how important it is to have the pre-activation values to be roughly gaussian (0 mean, and unit std). We saw how we can initialize our weights that make our pre-activation roughly gaussian by using Kaiming init. But, how do we always maintain our pre-activations to be roughly gaussian?
Answer: BatchNormalization
Benefits
stable training preserves vanishing gradients BatchNormalization As the name suggests, batches are normalized (across batches), by normalizing across batches we preserve the gaussian property of our pre-activations.</description>
    </item>
    
    <item>
      <title>Maximum likelihood estimate as loss function</title>
      <link>https://cohlem.github.io/sub-notes/maximum-likelihood-estimate-as-loss/</link>
      <pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/maximum-likelihood-estimate-as-loss/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>Backpropagation from scratch</title>
      <link>https://cohlem.github.io/sub-notes/backpropagation-from-scratch/</link>
      <pubDate>Sun, 08 Dec 2024 21:57:23 +0545</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/backpropagation-from-scratch/</guid>
      <description>Source: The spelled-out intro to neural networks and backpropagation: building micrograd
Backpropagation on paper It implements backpropagation for two arithmetic operation (multiplication and addition) which are quite straightforward.
Implementation is for this equation.
a = Value(2.0, label=&amp;#39;a&amp;#39;) b = Value(-3.0, label=&amp;#39;b&amp;#39;) c = Value(10.0, label=&amp;#39;c&amp;#39;) e = a*b; e.label = &amp;#39;e&amp;#39; d = e + c; d.label = &amp;#39;d&amp;#39; f = Value(-2.0, label=&amp;#39;f&amp;#39;) L = d * f; L.label = &amp;#39;L&amp;#39; L The most important thing to note here is the gradient accumulation step (shown at the bottom-left).</description>
    </item>
    
    <item>
      <title>Why we add regularization in loss function</title>
      <link>https://cohlem.github.io/sub-notes/why-we-need-regularization/</link>
      <pubDate>Sun, 08 Dec 2024 21:57:23 +0545</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/why-we-need-regularization/</guid>
      <description>it penalizes the weights, and prioritizes uniformity in weights. How does it penalize the weights? Now when we do the backprop and gradient descent.
The gradient of loss w.r.t some weights become as we can see it penalizes the weight by reducing the weights&amp;rsquo;s value by some higher amount compared to the some minimial weight update when we only used loss function.
So overall, the model tries to balance the Loss (L) as well as keep the weights small.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://cohlem.github.io/sub-notes/gpt-implementation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/gpt-implementation/</guid>
      <description># We always start with a dataset to train on. Let&amp;#39;s download the tiny shakespeare dataset !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt with open(&amp;#39;input.txt&amp;#39;, &amp;#39;r&amp;#39;, encoding=&amp;#39;utf-8&amp;#39;) as f: data = f.read() from torch import nn import torch vocab = sorted(list(set(data))) len(data) stoi = {s:i for i,s in enumerate(vocab)} itos = {i:s for s,i in stoi.items()} encode = lambda x: [stoi[i] for i in x] decode = lambda x: &amp;#39;&amp;#39;.join([itos[i] for i in x]) type(data) str Xtr = data[:int(0.</description>
    </item>
    
    <item>
      <title>optimizing-loss-with-weight-initialization</title>
      <link>https://cohlem.github.io/sub-notes/optimizing-loss/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/optimizing-loss/</guid>
      <description>Problem Consider a simple MLP that takes in combined 3 character embeddings as an input and we predicts a new character.
# A simple MLP n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 200 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) b1 = torch.</description>
    </item>
    
    <item>
      <title>TITLE</title>
      <link>https://cohlem.github.io/sub-notes/template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/template/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
