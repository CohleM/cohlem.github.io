<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Sub-notes on CohleM</title>
    <link>https://cohlem.github.io/sub-notes/</link>
    <description>Recent content in Sub-notes on CohleM</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 20 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://cohlem.github.io/sub-notes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Diagnostic-tool-while-training-nn</title>
      <link>https://cohlem.github.io/sub-notes/diagnostic-tool-while-training-nn/</link>
      <pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/diagnostic-tool-while-training-nn/</guid>
      <description>source: Building makemore Part 3: Activations &amp;amp; Gradients, BatchNorm
Things to look out for while training NN Take a look at previous notes to understand this note better
consider we have this simple 6 layer NN
# Linear Layer g = torch.Generator().manual_seed(2147483647) # for reproducibility class Layer: def __init__(self,fan_in, fan_out, bias=False): self.w = torch.randn((fan_in, fan_out),generator = g) / (fan_in)**(0.5) # applying kaiming init self.bias = bias if bias: self.b = torch.</description>
    </item>
    
    <item>
      <title>Backpropagation from scratch</title>
      <link>https://cohlem.github.io/sub-notes/backpropagation-from-scratch/</link>
      <pubDate>Sun, 08 Dec 2024 21:57:23 +0545</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/backpropagation-from-scratch/</guid>
      <description>Source: The spelled-out intro to neural networks and backpropagation: building micrograd
Backpropagation on paper It implements backpropagation for two arithmetic operation (multiplication and addition) which are quite straightforward.
Implementation is for this equation.
a = Value(2.0, label=&amp;#39;a&amp;#39;) b = Value(-3.0, label=&amp;#39;b&amp;#39;) c = Value(10.0, label=&amp;#39;c&amp;#39;) e = a*b; e.label = &amp;#39;e&amp;#39; d = e + c; d.label = &amp;#39;d&amp;#39; f = Value(-2.0, label=&amp;#39;f&amp;#39;) L = d * f; L.label = &amp;#39;L&amp;#39; L The most important thing to note here is the gradient accumulation step (shown at the bottom-left).</description>
    </item>
    
    <item>
      <title>Why we add regularization in loss function</title>
      <link>https://cohlem.github.io/sub-notes/why-we-need-regularization/</link>
      <pubDate>Sun, 08 Dec 2024 21:57:23 +0545</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/why-we-need-regularization/</guid>
      <description>it penalizes the weights, and prioritizes uniformity in weights. How does it penalize the weights? Now when we do the backprop and gradient descent.
The gradient of loss w.r.t some weights become as we can see it penalizes the weight by reducing the weights&amp;rsquo;s value by some higher amount compared to the some minimial weight update when we only used loss function.
So overall, the model tries to balance the Loss (L) as well as keep the weights small.</description>
    </item>
    
    <item>
      <title>BatchNormalization</title>
      <link>https://cohlem.github.io/sub-notes/batchnormalization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/batchnormalization/</guid>
      <description>As we saw in our previous note how important it is to have the pre-activation values to be roughly gaussian (0 mean, and unit std). We saw how we can initialize our weights that make our pre-activation roughly gaussian by using Kaiming init. But, how do we always maintain our pre-activations to be roughly gaussian?
Answer: BatchNormalization
Benefits
stable training preserves vanishing gradients BatchNormalization As the name suggests, batches are normalized (across batches), by normalizing across batches we preserve the gaussian property of our pre-activations.</description>
    </item>
    
    <item>
      <title>Maximum likelihood estimate as loss function</title>
      <link>https://cohlem.github.io/sub-notes/maximum-likelihood-estimate-as-loss/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/maximum-likelihood-estimate-as-loss/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>optimizing-loss-with-weight-initialization</title>
      <link>https://cohlem.github.io/sub-notes/optimizing-loss/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/optimizing-loss/</guid>
      <description>Problem Consider a simple MLP that takes in combined 3 character embeddings as an input and we predicts a new character.
# A simple MLP n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 200 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) b1 = torch.</description>
    </item>
    
    <item>
      <title>TITLE</title>
      <link>https://cohlem.github.io/sub-notes/template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cohlem.github.io/sub-notes/template/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
