<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>optimizing-loss-with-weight-initialization | CohleM</title>
<meta name="keywords" content="">
<meta name="description" content="Problem Consider a simple MLP that takes in combined 3 character embeddings as an input and we predicts a new character.
# A simple MLP n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 200 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) b1 = torch.">
<meta name="author" content="CohleM">
<link rel="canonical" href="https://cohlem.github.io/sub-notes/optimizing-loss/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css" integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe&#43;FVUFzPh7U=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://cohlem.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://cohlem.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://cohlem.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://cohlem.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://cohlem.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script type="text/javascript">
  MathJax = {
    tex: {
      inlineMath: [["\\(", "\\)"], ["$", "$"]],
      displayMath: [["\\[", "\\]"], ["$$", "$$"]]
    },
    options: {
      skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"]
    }
  };
</script>



<script async src="https://www.googletagmanager.com/gtag/js?id=G-X6LV4QY2G2"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-X6LV4QY2G2', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="optimizing-loss-with-weight-initialization" />
<meta property="og:description" content="Problem Consider a simple MLP that takes in combined 3 character embeddings as an input and we predicts a new character.
# A simple MLP n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 200 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) b1 = torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cohlem.github.io/sub-notes/optimizing-loss/" /><meta property="article:section" content="sub-notes" />



<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="optimizing-loss-with-weight-initialization"/>
<meta name="twitter:description" content="Problem Consider a simple MLP that takes in combined 3 character embeddings as an input and we predicts a new character.
# A simple MLP n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 200 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) b1 = torch."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Sub-notes",
      "item": "https://cohlem.github.io/sub-notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "optimizing-loss-with-weight-initialization",
      "item": "https://cohlem.github.io/sub-notes/optimizing-loss/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "optimizing-loss-with-weight-initialization",
  "name": "optimizing-loss-with-weight-initialization",
  "description": "Problem Consider a simple MLP that takes in combined 3 character embeddings as an input and we predicts a new character.\n# A simple MLP n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 200 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) b1 = torch.",
  "keywords": [
    
  ],
  "articleBody": "Problem Consider a simple MLP that takes in combined 3 character embeddings as an input and we predicts a new character.\n# A simple MLP n_embd = 10 # the dimensionality of the character embedding vectors n_hidden = 200 # the number of neurons in the hidden layer of the MLP g = torch.Generator().manual_seed(2147483647) # for reproducibility C = torch.randn((vocab_size, n_embd), generator=g) W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) b1 = torch.randn(n_hidden, generator=g) W2 = torch.randn((n_hidden, vocab_size), generator=g) b2 = torch.randn(vocab_size, generator=g) # BatchNorm parameters bngain = torch.ones((1, n_hidden)) bnbias = torch.zeros((1, n_hidden)) bnmean_running = torch.zeros((1, n_hidden)) bnstd_running = torch.ones((1, n_hidden)) parameters = [C, W1, W2, b2] print(sum(p.nelement() for p in parameters)) # number of parameters in total for p in parameters: p.requires_grad = True # same optimization as last time max_steps = 200000 batch_size = 32 lossi = [] for i in range(max_steps): # minibatch construct ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y # forward pass emb = C[Xb] # embed the characters into vectors embcat = emb.view(emb.shape[0], -1) # concatenate the vectors # Linear layer hpreact = embcat @ W1 + b1 # hidden layer pre-activation # Non-linearity h = torch.tanh(hpreact) # hidden layer logits = h @ W2 + b2 # output layer loss = F.cross_entropy(logits, Yb) # loss function # backward pass for p in parameters: p.grad = None loss.backward() # update lr = 0.1 if i \u003c 100000 else 0.01 # step learning rate decay for p in parameters: p.data += -lr * p.grad # track stats if i % 10000 == 0: # print every once in a while print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}') lossi.append(loss.log10().item()) When we train this simple MLP, the output loss over 200,000 iterations\n0/ 200000: 27.8817 10000/ 200000: 2.5633 20000/ 200000: 2.6522 30000/ 200000: 2.8065 40000/ 200000: 2.1546 50000/ 200000: 2.7555 60000/ 200000: 2.4661 70000/ 200000: 2.0084 80000/ 200000: 2.3762 90000/ 200000: 2.2308 100000/ 200000: 2.0540 110000/ 200000: 2.3655 120000/ 200000: 1.8583 130000/ 200000: 2.4840 140000/ 200000: 2.4164 150000/ 200000: 2.1783 160000/ 200000: 2.0387 170000/ 200000: 1.8343 180000/ 200000: 2.1532 190000/ 200000: 1.9804 We can see the the loss in the first iteration is 27.8817 and loss after that iteration has drastically decreased. There is a significant gap in loss between those two iterations. The problem here is that the initial loss is just too big. We can also prove it. Initially we would want to assign equal probability to all the characters, because we don’t know which character comes next, and so on. The likelihood that a character will appear next in a equally likely scenario is 1/27. So when we calculate our negative log likelihood (loss function) we get.\n- torch.tensor(1/27.0).log() \u003e\u003e tensor(3.2958) which should be the approximate loss initially, but in our case we have loss of 27.8817, which means our NN is wasting computation just because greater loss in the initially.\nWhy is our loss too big initially? To find out, let’s look at our weights that shape our logits, which is just before calculating our loss.\nlogits = h @ W2 + b2 # output layer let’s take a look at the distribution of our weights, at this point (just before calculating loss).\nplt.hist(W2.flatten().detach(), bins= 50) plt.show() as you can see the weights are distrubuted from -3 to 3 which is causing the problem, because we want the probability to be around 0, not largely distributed like it is right now.\nlet’s initialize the weight2 around 0 and see how our loss improves.\nW2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.01 the distribution becomes Now most of the values are around 0, and let see our loss.\n0/ 200000: 3.8073 10000/ 200000: 2.1428 20000/ 200000: 2.4846 30000/ 200000: 2.6018 40000/ 200000: 2.0154 50000/ 200000: 2.4055 60000/ 200000: 2.3731 70000/ 200000: 2.1023 80000/ 200000: 2.2878 90000/ 200000: 2.0695 100000/ 200000: 1.8733 110000/ 200000: 2.2128 120000/ 200000: 1.8982 130000/ 200000: 2.3203 140000/ 200000: 2.2108 150000/ 200000: 2.1378 160000/ 200000: 1.8270 170000/ 200000: 1.7928 180000/ 200000: 1.9601 190000/ 200000: 1.8350 you can see how our initial loss improves, this is because now our weights are normally distributed around 0, and not distributed around extreme values i.e (-3 and 3) which caused our initial loss to explode.\nSimilarly,\nlet’s look at the output of our tanh activation. as you can see most of our values lie in -1 and -1, why is that ???\nas you might remember our tanh works like this, if the x values lie near 0, we get some expressive non linear values, but when the x values lie in the extreme values, say abs(x)\u003e 1 or 2, the output values will be squashed and will be between -1 and 1.\nlet’s see what our input values are for tanh that is resulting in most values to be -1 and 1. as you can see the histogram of input values to our tanh function i.e hpreact lie in extreme values (i.e not around 0, but is normally distributed between -15 and 15) which is causing the output of tanh function to be -1 and 1. This behaviour holds true for most of the activation functions i.e if input to the activation function is not around 0 and is more extremely distributed, then it will will squashed( i.e most of them will the at extreme ).\nSo why having activations -1 and 1 a problem here? let’s look at how gradient is calculated for tanh function. as you can see t is the tanh activation, the gradient is dependent on t,\nSo if, our activations are -1 and 1, you can clearly see self.grad will be 0, and the gradient at this point will stop and not propagate further.\nand if most of the activations are -1 and 1, there will be no learning because we will have 0 gradient, so our NN will not learn.\nNOTE\nfor a simpler NN like ours, even if we initialize weights that are not every good, it can still still learn, but in much bigger NN the impact can be much worse resulting in no learning at all, if the weights are not properly initialized. Solution ? The solution is to initialize our initial weights in such a way that the property of our distribution is maintained. i.e having 0 mean and unit std. We want weights that are not 0, and not too extreme. If it’s 0 then applying activation doesn’t make any sense. as you can see how the x has 0 mean and unit std, but for y it isn’t the same. y takes on more extreme values which will result in vanishing gradients later on, as shown in the previous steps. so we want to preserve that distribution the same for our y value.\nKaiming Init A simple multiplication by 0.01 to weights would result is better initialization and would result in good activations. But, how do we get these values (0.001) that we multiply our weights with? So the proper initialization technique can be determined by using Kaiming init\nThe value with which we can multiply is given by this formula below. where different activations have different gains, and in place of fan_mode we can add the input dimension of our weight matrix.\nFor tanh, our gain = 5/3 and fan_in = (n_embd * block_size). so we can multiply our weights in this way.\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3) / ( (n_embd * block_size)**(0.5)) the precise initialization is not required, we can simply multiply our weight matrices by 1/((n_embd * block_size)xx(0.5)).\nThis initialization will help in preserving our distribution property (0 mean and unit std)\nNOTE kaiming init helps only during the initial weight initialization, but these weights have to maintain the gaussian property throughout training, which is why we add batchnormalization, LayerNormalization or RMSNorm\nScaling the projection weights after the residual block The introduction of skip connections provides smooth gradient flow but also increases the variance of our projections.\nFor instance take this toy example\nx = torch.zeros(768) for i in range(100): x += torch.randn(768) The variance of x after this loop becomes (9.9394)\nBut we always want our variance to be around 1.\nWhat should we do? Scale the x by the square root of total number of loop\nn = 100 for i in range(n): x += (n**-0.5) * torch.randn(768) But in case of skip connections in our language model we should scale the projection weights by sqrt(2 * total_no_of_transformer_blocks)\n2 comes from the fact that we add x as well as the block to our output\nfor instance\nclass Block(nn.Module): def __init__(self,config): super().__init__() self.attn = Head(config) self.mlp = FFN(config) self.ln_1 = nn.LayerNorm(config.n_embd) self.ln_2 = nn.LayerNorm(config.n_embd) def forward(self,x): x = self.attn(self.ln_1(x)) + x # \u003c====== x = self.mlp(self.ln_2(x)) + x # \u003c======= return x ",
  "wordCount" : "1460",
  "inLanguage": "en",
  "datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "CohleM"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://cohlem.github.io/sub-notes/optimizing-loss/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "CohleM",
    "logo": {
      "@type": "ImageObject",
      "url": "https://cohlem.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://cohlem.github.io" accesskey="h" title="CohleM (Alt + H)">CohleM</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://cohlem.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/random/" title="Random">
                    <span>Random</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://cohlem.github.io">Home</a>&nbsp;»&nbsp;<a href="https://cohlem.github.io/sub-notes/">Sub-notes</a></div>
    <h1 class="post-title">
      optimizing-loss-with-weight-initialization
    </h1>
    <div class="post-meta">7 min&nbsp;·&nbsp;CohleM

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#problem" aria-label="Problem">Problem</a><ul>
                        
                <li>
                    <a href="#why-is-our-loss-too-big-initially" aria-label="Why is our loss too big initially?">Why is our loss too big initially?</a></li>
                <li>
                    <a href="#so-why-having-activations--1-and-1-a-problem-here" aria-label="So why having activations -1 and 1 a problem here?">So why having activations -1 and 1 a problem here?</a></li>
                <li>
                    <a href="#solution-" aria-label="Solution ?">Solution ?</a></li>
                <li>
                    <a href="#kaiming-init" aria-label="Kaiming Init">Kaiming Init</a></li>
                <li>
                    <a href="#note" aria-label="NOTE">NOTE</a></li>
                <li>
                    <a href="#scaling-the-projection-weights-after-the-residual-block" aria-label="Scaling the projection weights after the residual block">Scaling the projection weights after the residual block</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="problem">Problem<a hidden class="anchor" aria-hidden="true" href="#problem">#</a></h2>
<p>Consider a simple MLP that takes in combined 3 character embeddings as an input and we predicts a new character.</p>
<pre tabindex="0"><code># A simple MLP
n_embd = 10 # the dimensionality of the character embedding vectors
n_hidden = 200 # the number of neurons in the hidden layer of the MLP

g = torch.Generator().manual_seed(2147483647) # for reproducibility
C  = torch.randn((vocab_size, n_embd),            generator=g)
W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) 
b1 = torch.randn(n_hidden,                        generator=g) 
W2 = torch.randn((n_hidden, vocab_size),          generator=g) 
b2 = torch.randn(vocab_size,                      generator=g)

# BatchNorm parameters
bngain = torch.ones((1, n_hidden))
bnbias = torch.zeros((1, n_hidden))
bnmean_running = torch.zeros((1, n_hidden))
bnstd_running = torch.ones((1, n_hidden))

parameters = [C, W1, W2, b2]
print(sum(p.nelement() for p in parameters)) # number of parameters in total
for p in parameters:
    p.requires_grad = True

# same optimization as last time
max_steps = 200000
batch_size = 32
lossi = []

for i in range(max_steps):
  
    # minibatch construct
    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)
    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y

    # forward pass
    emb = C[Xb] # embed the characters into vectors
    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors
    # Linear layer
    hpreact = embcat @ W1 + b1 # hidden layer pre-activation
    # Non-linearity
    h = torch.tanh(hpreact) # hidden layer
    logits = h @ W2 + b2 # output layer
    loss = F.cross_entropy(logits, Yb) # loss function

    # backward pass
    for p in parameters:
        p.grad = None
    loss.backward()

    # update
    lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay
    for p in parameters:
        p.data += -lr * p.grad

    # track stats
    if i % 10000 == 0: # print every once in a while
        print(f&#39;{i:7d}/{max_steps:7d}: {loss.item():.4f}&#39;)
    lossi.append(loss.log10().item())
  
</code></pre><p>When we train this simple MLP, the output loss over 200,000 iterations</p>
<pre tabindex="0"><code>     0/ 200000: 27.8817
  10000/ 200000: 2.5633
  20000/ 200000: 2.6522
  30000/ 200000: 2.8065
  40000/ 200000: 2.1546
  50000/ 200000: 2.7555
  60000/ 200000: 2.4661
  70000/ 200000: 2.0084
  80000/ 200000: 2.3762
  90000/ 200000: 2.2308
 100000/ 200000: 2.0540
 110000/ 200000: 2.3655
 120000/ 200000: 1.8583
 130000/ 200000: 2.4840
 140000/ 200000: 2.4164
 150000/ 200000: 2.1783
 160000/ 200000: 2.0387
 170000/ 200000: 1.8343
 180000/ 200000: 2.1532
 190000/ 200000: 1.9804
</code></pre><p>We can see the the loss in the first iteration is 27.8817 and loss after that iteration has drastically decreased. There is a significant gap in loss between those two iterations. The problem here is that the initial loss is just too big. We can also prove it. Initially we would want to assign equal probability to all the characters, because we don&rsquo;t know which character comes next, and so on. The likelihood that a character will appear next in a equally likely scenario is 1/27. So when we calculate our negative log likelihood (loss function) we get.</p>
<pre tabindex="0"><code>- torch.tensor(1/27.0).log()
&gt;&gt; tensor(3.2958)
</code></pre><p>which should be the approximate loss initially, but in our case we have loss of 27.8817, which means our NN is wasting computation just because greater loss in the initially.</p>
<h3 id="why-is-our-loss-too-big-initially">Why is our loss too big initially?<a hidden class="anchor" aria-hidden="true" href="#why-is-our-loss-too-big-initially">#</a></h3>
<p>To find out, let&rsquo;s look at our weights that shape our logits, which is just before calculating our loss.</p>
<pre tabindex="0"><code>logits = h @ W2 + b2 # output layer
</code></pre><p>let&rsquo;s take a look at the distribution of our weights, at this point (just before calculating loss).</p>
<pre tabindex="0"><code>plt.hist(W2.flatten().detach(), bins= 50)
plt.show()
</code></pre><p><img loading="lazy" src="fig1.png" alt="fig1"  />
</p>
<p>as you can see the weights are distrubuted from -3 to 3 which is causing the problem, because we want the probability to be around 0, not largely distributed like it is right now.</p>
<p>let&rsquo;s initialize the weight2 around 0 and see how our loss improves.</p>
<pre tabindex="0"><code>W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01
</code></pre><p>the distribution becomes
<img loading="lazy" src="fig2.png" alt="fig2"  />
</p>
<p>Now most of the values are around 0, and let see our loss.</p>
<pre tabindex="0"><code>    0/ 200000: 3.8073
  10000/ 200000: 2.1428
  20000/ 200000: 2.4846
  30000/ 200000: 2.6018
  40000/ 200000: 2.0154
  50000/ 200000: 2.4055
  60000/ 200000: 2.3731
  70000/ 200000: 2.1023
  80000/ 200000: 2.2878
  90000/ 200000: 2.0695
 100000/ 200000: 1.8733
 110000/ 200000: 2.2128
 120000/ 200000: 1.8982
 130000/ 200000: 2.3203
 140000/ 200000: 2.2108
 150000/ 200000: 2.1378
 160000/ 200000: 1.8270
 170000/ 200000: 1.7928
 180000/ 200000: 1.9601
 190000/ 200000: 1.8350
</code></pre><p>you can see how our initial loss improves, this is because now our weights are normally distributed around 0, and not distributed around extreme values i.e (-3 and 3) which caused our initial loss to explode.</p>
<p>Similarly,</p>
<p>let&rsquo;s look at the output of our tanh activation.
<img loading="lazy" src="fig3.png" alt="fig3"  />
</p>
<p>as you can see most of our values lie in -1 and -1, why is that ???</p>
<p>as you might remember our tanh works like this, if the x values lie near 0, we get some expressive non linear values, but when the x values lie in the extreme values, say abs(x)&gt; 1 or 2, the output values will be squashed and will be between -1 and 1.</p>
<p><img loading="lazy" src="fig4.png" alt="fig4"  />
</p>
<p>let&rsquo;s see what our input values are for tanh that is resulting in most values to be -1 and 1.
<img loading="lazy" src="fig5.png" alt="fig5"  />

as you can see the histogram of input values to our tanh function i.e <strong>hpreact</strong> lie in extreme values (i.e not around 0, but is normally distributed between -15 and 15) which is causing the output of tanh function to be -1 and 1. This behaviour holds true for most of the activation functions i.e if input to the activation function is not around 0 and is more extremely distributed, then it will will squashed( i.e most of them will the at extreme ).</p>
<h3 id="so-why-having-activations--1-and-1-a-problem-here">So why having activations -1 and 1 a problem here?<a hidden class="anchor" aria-hidden="true" href="#so-why-having-activations--1-and-1-a-problem-here">#</a></h3>
<p>let&rsquo;s look at how gradient is calculated for tanh function.
<img loading="lazy" src="fig6.png" alt="fig6"  />
</p>
<p>as you can see t is the tanh activation, the gradient is dependent on t,</p>
<p>So if, our activations are -1 and 1, you can clearly see self.grad will be 0, and the gradient at this point will stop and not propagate further.</p>
<p>and if most of the activations are -1 and 1, there will be no learning because we will have 0 gradient, so our NN will not learn.</p>
<p><strong>NOTE</strong></p>
<ul>
<li>for a simpler NN like ours, even if we initialize weights that are not every good, it can still still learn, but in much bigger NN the impact can be much worse resulting in no learning at all, if the weights are not properly initialized.</li>
</ul>
<h3 id="solution-">Solution ?<a hidden class="anchor" aria-hidden="true" href="#solution-">#</a></h3>
<p>The solution is to initialize our initial weights in such a way that the property of our distribution is maintained. i.e having 0 mean and unit std. We want weights that are not 0, and not too extreme.
If it&rsquo;s 0 then applying activation doesn&rsquo;t make any sense.
<img loading="lazy" src="fig7.png" alt="fig7"  />
</p>
<p>as you can see how the x has 0 mean and unit std, but for y it isn&rsquo;t the same. y takes on more extreme values which will result in vanishing gradients later on, as shown in the previous steps.
so we want to preserve that distribution the same for our y value.</p>
<h3 id="kaiming-init">Kaiming Init<a hidden class="anchor" aria-hidden="true" href="#kaiming-init">#</a></h3>
<p>A simple multiplication by 0.01 to weights would result is better initialization and would result in good activations. But, how do we get these values (0.001) that we multiply our weights with?
So the proper initialization technique can be determined by using <strong>Kaiming init</strong></p>
<p>The value with which we can multiply is given by this formula below.
<img loading="lazy" src="fig8.png" alt="fig8"  />
</p>
<p>where different activations have different gains, and in place of fan_mode we can add the input dimension of our weight matrix.</p>
<p>For tanh, our gain = 5/3 and fan_in = (n_embd * block_size). so we can multiply our weights in this way.</p>
<pre tabindex="0"><code>W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3) / ( (n_embd * block_size)**(0.5))
</code></pre><p>the precise initialization is not required, we can simply multiply our weight matrices by 1/((n_embd * block_size)xx(0.5)).</p>
<p>This initialization will help in preserving our distribution property (0 mean and unit std)</p>
<h3 id="note">NOTE<a hidden class="anchor" aria-hidden="true" href="#note">#</a></h3>
<p>kaiming init helps only during the initial weight initialization, but these weights have to maintain the gaussian property throughout training, which is why we add batchnormalization, LayerNormalization or RMSNorm</p>
<h3 id="scaling-the-projection-weights-after-the-residual-block">Scaling the projection weights after the residual block<a hidden class="anchor" aria-hidden="true" href="#scaling-the-projection-weights-after-the-residual-block">#</a></h3>
<p>The introduction of skip connections provides smooth gradient flow but also increases the variance of our projections.</p>
<p>For instance take this toy example</p>
<pre tabindex="0"><code>x = torch.zeros(768)
for i in range(100):
    x += torch.randn(768)
</code></pre><p>The variance of x after this loop becomes (9.9394)</p>
<p>But we always want our variance to be around 1.</p>
<p>What should we do?
Scale the x by the square root of total number of loop</p>
<pre tabindex="0"><code>n = 100
for i in range(n):
    x += (n**-0.5) * torch.randn(768)
</code></pre><p>But in case of skip connections in our language model we should scale the projection weights by sqrt(2 * total_no_of_transformer_blocks)</p>
<p>2 comes from the fact that we add x as well as the block to our output</p>
<p>for instance</p>
<pre tabindex="0"><code>class Block(nn.Module):
    def __init__(self,config):
        super().__init__()
        self.attn = Head(config)
        self.mlp = FFN(config)
        self.ln_1 = nn.LayerNorm(config.n_embd)
        self.ln_2 = nn.LayerNorm(config.n_embd)
        
    def forward(self,x):
        x = self.attn(self.ln_1(x)) + x # &lt;======
        x = self.mlp(self.ln_2(x)) + x # &lt;=======
    
        return x
</code></pre>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://cohlem.github.io/sub-notes/rl-framework/">
    <span class="title">« Prev</span>
    <br>
    <span></span>
  </a>
  <a class="next" href="https://cohlem.github.io/sub-notes/template/">
    <span class="title">Next »</span>
    <br>
    <span>TITLE</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://cohlem.github.io">CohleM</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
