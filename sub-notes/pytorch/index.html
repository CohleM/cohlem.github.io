<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Pytorch Commands I forget time to time/ commands that are essential | CohleM</title>
<meta name="keywords" content="">
<meta name="description" content="torch.stack(tensors, dim) stacks the tensors across dim
#usage # data has to be tensor torch.stack([data[i:i&#43;some_number] for i in range(10)]) torch.from_numpy(numpy_array) shares the memory with the numpy_array but is tensor type
a = np.array([1,2,3]) b = torch.tensor(a) # creates copy c = torch.from_numpy(a) # shares memory a[0] = 11 c # outputs: tensor([11, 2, 3]) torch.flatten(input, start,end=-1) flattens the input from dim start to end (-1 by default)
t = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]) torch.">
<meta name="author" content="CohleM">
<link rel="canonical" href="https://cohlem.github.io/sub-notes/pytorch/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css" integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe&#43;FVUFzPh7U=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://cohlem.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://cohlem.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://cohlem.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://cohlem.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://cohlem.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-X6LV4QY2G2"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-X6LV4QY2G2', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Pytorch Commands I forget time to time/ commands that are essential" />
<meta property="og:description" content="torch.stack(tensors, dim) stacks the tensors across dim
#usage # data has to be tensor torch.stack([data[i:i&#43;some_number] for i in range(10)]) torch.from_numpy(numpy_array) shares the memory with the numpy_array but is tensor type
a = np.array([1,2,3]) b = torch.tensor(a) # creates copy c = torch.from_numpy(a) # shares memory a[0] = 11 c # outputs: tensor([11, 2, 3]) torch.flatten(input, start,end=-1) flattens the input from dim start to end (-1 by default)
t = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]) torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cohlem.github.io/sub-notes/pytorch/" /><meta property="article:section" content="sub-notes" />
<meta property="article:published_time" content="2025-01-29T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-01-29T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Pytorch Commands I forget time to time/ commands that are essential"/>
<meta name="twitter:description" content="torch.stack(tensors, dim) stacks the tensors across dim
#usage # data has to be tensor torch.stack([data[i:i&#43;some_number] for i in range(10)]) torch.from_numpy(numpy_array) shares the memory with the numpy_array but is tensor type
a = np.array([1,2,3]) b = torch.tensor(a) # creates copy c = torch.from_numpy(a) # shares memory a[0] = 11 c # outputs: tensor([11, 2, 3]) torch.flatten(input, start,end=-1) flattens the input from dim start to end (-1 by default)
t = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]) torch."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Sub-notes",
      "item": "https://cohlem.github.io/sub-notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Pytorch Commands I forget time to time/ commands that are essential",
      "item": "https://cohlem.github.io/sub-notes/pytorch/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Pytorch Commands I forget time to time/ commands that are essential",
  "name": "Pytorch Commands I forget time to time\/ commands that are essential",
  "description": "torch.stack(tensors, dim) stacks the tensors across dim\n#usage # data has to be tensor torch.stack([data[i:i+some_number] for i in range(10)]) torch.from_numpy(numpy_array) shares the memory with the numpy_array but is tensor type\na = np.array([1,2,3]) b = torch.tensor(a) # creates copy c = torch.from_numpy(a) # shares memory a[0] = 11 c # outputs: tensor([11, 2, 3]) torch.flatten(input, start,end=-1) flattens the input from dim start to end (-1 by default)\nt = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]) torch.",
  "keywords": [
    
  ],
  "articleBody": "torch.stack(tensors, dim) stacks the tensors across dim\n#usage # data has to be tensor torch.stack([data[i:i+some_number] for i in range(10)]) torch.from_numpy(numpy_array) shares the memory with the numpy_array but is tensor type\na = np.array([1,2,3]) b = torch.tensor(a) # creates copy c = torch.from_numpy(a) # shares memory a[0] = 11 c # outputs: tensor([11, 2, 3]) torch.flatten(input, start,end=-1) flattens the input from dim start to end (-1 by default)\nt = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]) torch.flatten(t) torch.flatten(t, start_dim=1) # (2,2,2) --\u003e (2,2*2) [5, 6, 7, 8]]) torch.stack and torch.cat((tensors), dim) torch.stack stacks tensors along new dim, whereas torch.cat concatenates along that specific dim.\nexample:\na = torch.randn(2,5,8,32) b = torch.randn(2,1,8,32) torch.cat((a,b), dim=1).shape #outputs : torch.Size([2, 6, 8, 32]) a = torch.randn(3,5,8,32) b = torch.randn(3,5,8,32) torch.stack((a,b), dim=1).shape #outputs: torch.Size([3, 2, 5, 8, 32]) For the past 2 years I’ve been involved in training and experimenting machine learning systems, mostly using third party packages such as sklearn, huggingface and so on. Sometimes the experiments become too specific and the abstraction provided by these packages become a bottleneck for the performance optimization. My research goal is to understand these bottlenecks in deep and write my own optimized code for hardware-specific optimization which enables resource efficient training or inference.\n",
  "wordCount" : "207",
  "inLanguage": "en",
  "datePublished": "2025-01-29T00:00:00Z",
  "dateModified": "2025-01-29T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "CohleM"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://cohlem.github.io/sub-notes/pytorch/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "CohleM",
    "logo": {
      "@type": "ImageObject",
      "url": "https://cohlem.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://cohlem.github.io" accesskey="h" title="CohleM (Alt + H)">CohleM</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://cohlem.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://cohlem.github.io">Home</a>&nbsp;»&nbsp;<a href="https://cohlem.github.io/sub-notes/">Sub-notes</a></div>
    <h1 class="post-title">
      Pytorch Commands I forget time to time/ commands that are essential
    </h1>
    <div class="post-meta"><span title='2025-01-29 00:00:00 +0000 UTC'>January 29, 2025</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;CohleM

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#torchstacktensors-dim" aria-label="torch.stack(tensors, dim)"><code>torch.stack(tensors, dim)</code></a></li>
                <li>
                    <a href="#torchfrom_numpynumpy_array" aria-label="torch.from_numpy(numpy_array)"><code>torch.from_numpy(numpy_array)</code></a></li>
                <li>
                    <a href="#torchflatteninput-startend-1" aria-label="torch.flatten(input, start,end=-1)"><code>torch.flatten(input, start,end=-1)</code></a></li>
                <li>
                    <a href="#torchstack-and-torchcattensors-dim" aria-label="torch.stack and torch.cat((tensors), dim)">torch.stack and torch.cat((tensors), dim)</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h4 id="torchstacktensors-dim"><code>torch.stack(tensors, dim)</code><a hidden class="anchor" aria-hidden="true" href="#torchstacktensors-dim">#</a></h4>
<p>stacks the tensors across dim</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">#usage</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># data has to be tensor</span>
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>stack([data[i:i<span style="color:#f92672">+</span>some_number] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">10</span>)]) 
</span></span></code></pre></div><h4 id="torchfrom_numpynumpy_array"><code>torch.from_numpy(numpy_array)</code><a hidden class="anchor" aria-hidden="true" href="#torchfrom_numpynumpy_array">#</a></h4>
<p>shares the memory with the numpy_array but is tensor type</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>a <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>])
</span></span><span style="display:flex;"><span>b <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(a) <span style="color:#75715e"># creates copy</span>
</span></span><span style="display:flex;"><span>c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>from_numpy(a) <span style="color:#75715e"># shares memory</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>a[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">11</span>
</span></span><span style="display:flex;"><span>c
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># outputs: tensor([11,  2,  3])</span>
</span></span></code></pre></div><h4 id="torchflatteninput-startend-1"><code>torch.flatten(input, start,end=-1)</code><a hidden class="anchor" aria-hidden="true" href="#torchflatteninput-startend-1">#</a></h4>
<p>flattens the input from dim start to end (-1 by default)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>t <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([[[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>],
</span></span><span style="display:flex;"><span>                   [<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>]],
</span></span><span style="display:flex;"><span>                  [[<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">6</span>],
</span></span><span style="display:flex;"><span>                   [<span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">8</span>]]])
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>flatten(t)
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>flatten(t, start_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># (2,2,2) --&gt; (2,2*2)</span>
</span></span></code></pre></div><pre tabindex="0"><code class="language-tensor([[1," data-lang="tensor([[1,">        [5, 6, 7, 8]])
</code></pre><h4 id="torchstack-and-torchcattensors-dim">torch.stack and torch.cat((tensors), dim)<a hidden class="anchor" aria-hidden="true" href="#torchstack-and-torchcattensors-dim">#</a></h4>
<p>torch.stack stacks tensors along new dim, whereas
torch.cat concatenates along that specific dim.</p>
<p>example:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>a <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">5</span>,<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">32</span>)
</span></span><span style="display:flex;"><span>b <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">32</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>cat((a,b), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#outputs : torch.Size([2, 6, 8, 32])</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>a <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">5</span>,<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">32</span>)
</span></span><span style="display:flex;"><span>b <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">5</span>,<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">32</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>stack((a,b), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#outputs: torch.Size([3, 2, 5, 8, 32])</span>
</span></span></code></pre></div><p>For the past 2 years I&rsquo;ve been involved in training and experimenting machine learning systems, mostly using third party packages such as sklearn, huggingface and so on. Sometimes the experiments become too specific and the abstraction provided by these packages become a bottleneck for the performance optimization. My research goal is to understand these bottlenecks in deep and write my own optimized code for hardware-specific optimization which enables resource efficient training or inference.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://cohlem.github.io/sub-notes/building-lillm/">
    <span class="title">« Prev</span>
    <br>
    <span>Notes-while-building-lilLM</span>
  </a>
  <a class="next" href="https://cohlem.github.io/sub-notes/tokenization/">
    <span class="title">Next »</span>
    <br>
    <span>Tokenization</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://cohlem.github.io">CohleM</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
