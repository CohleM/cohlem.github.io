<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>GPUs | CohleM</title>
<meta name="keywords" content="">
<meta name="description" content="GPU physcial structure let&rsquo;s first understand the structure of GPU.
Inside a GPU it has a chip named GA102 (depends on architecture, this is for ampere architecture) built from 28.3million transistors (semiconductor device that can switch or amplify electrical signals) and majority area covered by processing cores. processing core is divide into seven Graphics processing clusters (GPCs)
among each GPC there are 12 Streaming Multiprocessors. Inside each SM there are 4 warps and 1 Raytracing core inside a warp there are 32 Cudas and 1 Tensor Core.">
<meta name="author" content="CohleM">
<link rel="canonical" href="https://cohlem.github.io/sub-notes/gpus/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css" integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe&#43;FVUFzPh7U=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://cohlem.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://cohlem.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://cohlem.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://cohlem.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://cohlem.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-X6LV4QY2G2"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-X6LV4QY2G2', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="GPUs" />
<meta property="og:description" content="GPU physcial structure let&rsquo;s first understand the structure of GPU.
Inside a GPU it has a chip named GA102 (depends on architecture, this is for ampere architecture) built from 28.3million transistors (semiconductor device that can switch or amplify electrical signals) and majority area covered by processing cores. processing core is divide into seven Graphics processing clusters (GPCs)
among each GPC there are 12 Streaming Multiprocessors. Inside each SM there are 4 warps and 1 Raytracing core inside a warp there are 32 Cudas and 1 Tensor Core." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cohlem.github.io/sub-notes/gpus/" /><meta property="article:section" content="sub-notes" />
<meta property="article:published_time" content="2025-01-08T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-01-08T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="GPUs"/>
<meta name="twitter:description" content="GPU physcial structure let&rsquo;s first understand the structure of GPU.
Inside a GPU it has a chip named GA102 (depends on architecture, this is for ampere architecture) built from 28.3million transistors (semiconductor device that can switch or amplify electrical signals) and majority area covered by processing cores. processing core is divide into seven Graphics processing clusters (GPCs)
among each GPC there are 12 Streaming Multiprocessors. Inside each SM there are 4 warps and 1 Raytracing core inside a warp there are 32 Cudas and 1 Tensor Core."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Sub-notes",
      "item": "https://cohlem.github.io/sub-notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "GPUs",
      "item": "https://cohlem.github.io/sub-notes/gpus/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "GPUs",
  "name": "GPUs",
  "description": "GPU physcial structure let\u0026rsquo;s first understand the structure of GPU.\nInside a GPU it has a chip named GA102 (depends on architecture, this is for ampere architecture) built from 28.3million transistors (semiconductor device that can switch or amplify electrical signals) and majority area covered by processing cores. processing core is divide into seven Graphics processing clusters (GPCs)\namong each GPC there are 12 Streaming Multiprocessors. Inside each SM there are 4 warps and 1 Raytracing core inside a warp there are 32 Cudas and 1 Tensor Core.",
  "keywords": [
    
  ],
  "articleBody": "GPU physcial structure let’s first understand the structure of GPU.\nInside a GPU it has a chip named GA102 (depends on architecture, this is for ampere architecture) built from 28.3million transistors (semiconductor device that can switch or amplify electrical signals) and majority area covered by processing cores. processing core is divide into seven Graphics processing clusters (GPCs)\namong each GPC there are 12 Streaming Multiprocessors. Inside each SM there are 4 warps and 1 Raytracing core inside a warp there are 32 Cudas and 1 Tensor Core.\nAltogether there are\n10752 CUDA Cores 336 Tensor Cores 84 Ray Tracing Cores Each cores have different function.\nCuda Cores cuda core is like a basic calculator with multiplication and addition operations.\nMostly used for processing video frames. perform operations like A x B + C called fused multiply and add (FMA) half of the cuda cores perform FMA on 32-bit floating point numbers other half perform 32-bit integer FMA. other sections perform bit shifting and bit masking as well as collecting and queuing incoming operands, and accumulating output results. So it can be though of like a calculator. performs one 1 multiply and 1 add operation per 1 cycle so altogether 2 operations x 10752 cuda cores x 1.7 GhZ clock speed(10^9 cycles per second) = 35.6 trillion calculations per second. Ray tracing Cores used for executing ray tracing algorithms Depending on the amount of streaming multiprocessors that are damaged during manufacturing they are categoriezed and sold at different prices, for instance RTX 3090 ti has full 10752 cuda where as 3090 might have some damaged SMs. These cards might have different clock speed too.\nGraphics Memory GDDR6X SDRAM these 24GBs of GDDR6X surround the GPU chip In order to run the operations of GPU chip\nThey must be loaded from SSD to these graphics memory. They have certain bandwidth i.e the amount of data they can transfer per second to the GPU chip. They have 1.15 Terbytes/sec bandwidth. Similar to HBM memory in AI chips, would look something like this How are operations executed? Each instruction is executed by a thread (which is matched to a single cuda core) and these threads are combined into 32 cores called warp, 4 warps are combined to form thread blocks that are operated by Streaming Multiprocessor (SM). Similarly SM is combined together. All these are operated by Gigathread Engine If processor runs at 1Ghz, it can run 10^9 cycles per second, assuming 1cycle = 1 basic operation it can execute 10^9 operations.\nits basically like a unit of time, 10^9 cycles per 1 second means, one cycle takes 10^-9 seconds to run. Different operations take varying amount of cycles (i.e latency of the operation) to perform. - Global memory access (up to 80GB): ~380 cycles - L2 cache: ~200 cycles - L1 cache or Shared memory access (up to 128 kb per Streaming Multiprocessor): ~34 cycles - Fused multiplication and addition, a*b+c (FFMA): 4 cycles - Tensor Core matrix multiply: 1 cycle Tensor Cores (Most important) These cores are used for matrix multiplication and matrix additions.\npredominantly used in deep learning calculation. First lets start by understanding the precision formats that’s used.\n1. FP16 (Half-Precision Floating-Point) Full Name: Half-precision floating-point.\nSize: 16 bits (2 bytes).\nBit Allocation:\n1 bit for the sign.\n5 bits for the exponent.\n10 bits for the mantissa.\nPrecision: About 3 decimal digits, 2^10 = 1024, taking log10(1024) = 3 decimal digits.\n2. FP32 (Single-Precision Floating-Point) Full Name: Single-precision floating-point.\nSize: 32 bits (4 bytes).\nBit Allocation:\n1 bit for the sign.\n8 bits for the exponent.\n23 bits for the mantissa.\nPrecision: About 7 decimal digits.\nThe operation performed by Tensor Core is something like this.\n(picture) It performs 64 FMA operations per clock.\nUnderstanding matrix multiplication in tensor cores. Task:\nMatrix multiply A and B with each size 32 x 32\nlets say our tensor cores process 4x4 matrix multiplications per 1 cycle. The step to multiply A and B are\ndivide A and B into tiles of 4x4 matrices i.e 64 4x4 tile for each matrix load the tiles into shared memory (164KB) from global memory takes about 200 cycles. load the tiles into tensor core registers for operation from shared memory, take about 34 cycles. perform matrix multiplication on eight tensor cores all in parallel, take 1 cycle and then accumulate the values. total cycle 200+34 + 1 = 235 cycles. Memory Bandwidth we have seen that Tensor Cores are very fast. So fast, in fact, that they are idle most of the time as they are waiting for memory to arrive from global memory. For example, during GPT-3-sized training, which uses huge matrices — the larger, the better for Tensor Cores — we have a Tensor Core TFLOPS utilization of about 45-65%, meaning that even for the large neural networks about 50% of the time, Tensor Cores are idle.\nNvidia Ampere Architecture 108 streaming multiprocessors (SMs) 432 Tensor Cores 6,912 FP32 Cuda Cores 3,456 FP64 cuda cores (they share the same physical hardware, different in a sense that each require different clock speed and precision ) More details in the picture below.\nThese TFLOPS are calculated using this formula\nNumber of cores for that precision (FP64) x clock speed x Operations per clock (generally 1FMA or 2 Operations)\nTensor Float 32 (TF32):\n156 TFLOPS | 312 TFLOPS*: TF32 is a mixed-precision format optimized for AI workloads. It provides a balance between FP16 and FP32 precision. BFLOAT16:\n312 TFLOPS | 624 TFLOPS*: BFLOAT16 is a 16-bit floating-point format used in deep learning, offering a good trade-off between range and precision. FP16 Tensor Core:\n312 TFLOPS | 624 TFLOPS*: FP16 is used for deep learning training and inference, providing high throughput for lower-precision computations. INT8 Tensor Core:\n624 TOPS | 1248 TOPS*: INT8 is used for inference tasks, where lower precision is acceptable, and high throughput is critical. This figure shows the range and precision of each of these.\nPreferred:\nTF32 for training and mostly FP16 and BF16 for inference. Tensor cores are optimized for matrix multiplications so it can peform more operations per clock rather than just 64 FMA per clock.\nSparsity Matrix contains large number of zeros in it, by using a fine-grained pruning algorithm to compress (essentially removing) small and zero-value matrices, the GPU saves computing resources, power, memory and bandwidth.\nForm Factor They define how the GPU is physically integrated into a system and how it connects to other components like the CPU and memory.\nPCIe is a standard interface used to connect GPUs, SSDs, network cards, and other peripherals to a computer’s motherboard SXM are not standalone cards and is used in data centers. Bandwidth PCIe Gen4: 64 GB/s (x16) NVLink: 600 GB/s (per GPU pair) References How do Graphics Cards Work? Exploring GPU Architecture NVIDIA A100 TENSOR CORE GPU ",
  "wordCount" : "1138",
  "inLanguage": "en",
  "datePublished": "2025-01-08T00:00:00Z",
  "dateModified": "2025-01-08T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "CohleM"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://cohlem.github.io/sub-notes/gpus/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "CohleM",
    "logo": {
      "@type": "ImageObject",
      "url": "https://cohlem.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://cohlem.github.io" accesskey="h" title="CohleM (Alt + H)">CohleM</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://cohlem.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://cohlem.github.io">Home</a>&nbsp;»&nbsp;<a href="https://cohlem.github.io/sub-notes/">Sub-notes</a></div>
    <h1 class="post-title">
      GPUs
    </h1>
    <div class="post-meta"><span title='2025-01-08 00:00:00 +0000 UTC'>January 8, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;CohleM

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#gpu-physcial-structure" aria-label="GPU physcial structure">GPU physcial structure</a></li>
                <li>
                    <a href="#cuda-cores" aria-label="Cuda Cores">Cuda Cores</a></li>
                <li>
                    <a href="#ray-tracing-cores" aria-label="Ray tracing Cores">Ray tracing Cores</a></li>
                <li>
                    <a href="#graphics-memory-gddr6x-sdram" aria-label="Graphics Memory GDDR6X SDRAM">Graphics Memory GDDR6X SDRAM</a></li>
                <li>
                    <a href="#how-are-operations-executed" aria-label="How are operations executed?">How are operations executed?</a></li>
                <li>
                    <a href="#tensor-cores-most-important" aria-label="Tensor Cores (Most important)">Tensor Cores (Most important)</a><ul>
                        
                <li>
                    <a href="#1-fp16-half-precision-floating-point" aria-label="1. FP16 (Half-Precision Floating-Point)"><strong>1. FP16 (Half-Precision Floating-Point)</strong></a></li>
                <li>
                    <a href="#2-fp32-single-precision-floating-point" aria-label="2. FP32 (Single-Precision Floating-Point)"><strong>2. FP32 (Single-Precision Floating-Point)</strong></a></li>
                <li>
                    <a href="#understanding-matrix-multiplication-in-tensor-cores" aria-label="Understanding matrix multiplication in tensor cores.">Understanding matrix multiplication in tensor cores.</a></li>
                <li>
                    <a href="#memory-bandwidth" aria-label="Memory Bandwidth">Memory Bandwidth</a></li></ul>
                </li>
                <li>
                    <a href="#nvidia-ampere-architecture" aria-label="Nvidia Ampere Architecture">Nvidia Ampere Architecture</a></li>
                <li>
                    <a href="#sparsity" aria-label="Sparsity">Sparsity</a></li>
                <li>
                    <a href="#form-factor" aria-label="Form Factor">Form Factor</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="gpu-physcial-structure">GPU physcial structure<a hidden class="anchor" aria-hidden="true" href="#gpu-physcial-structure">#</a></h3>
<p>let&rsquo;s first understand the structure of GPU.</p>
<p>Inside a GPU it has a chip named GA102 (depends on architecture, this is for ampere architecture) built from 28.3million transistors (semiconductor device that can <strong>switch</strong> or <strong>amplify</strong> electrical signals)
<img loading="lazy" src="gpusfig1.png" alt="one"  />

and majority area covered by processing cores.
<img loading="lazy" src="gpusfig2.png" alt="two"  />

processing core is divide into seven Graphics processing clusters (GPCs)</p>
<p><img loading="lazy" src="gpus3.png" alt="three"  />
</p>
<p>among each GPC there are 12 Streaming Multiprocessors.
<img loading="lazy" src="gpus4.png" alt="four"  />

Inside each SM there are 4 warps and 1 Raytracing core
<img loading="lazy" src="gpus5.png" alt="five"  />

inside a warp there are 32 Cudas and 1 Tensor Core.</p>
<p>Altogether there are</p>
<ul>
<li>10752 CUDA Cores</li>
<li>336 Tensor Cores</li>
<li>84 Ray Tracing Cores</li>
</ul>
<p>Each cores have different function.</p>
<h3 id="cuda-cores">Cuda Cores<a hidden class="anchor" aria-hidden="true" href="#cuda-cores">#</a></h3>
<p><img loading="lazy" src="gpus6.png" alt="six"  />

cuda core is like a basic calculator with multiplication and addition operations.</p>
<ul>
<li>Mostly used for processing video frames.</li>
<li>perform operations like A x B + C called fused multiply and add (FMA)</li>
<li>half of the cuda cores perform FMA on 32-bit floating point numbers other half perform 32-bit integer FMA.</li>
<li>other sections perform bit shifting and bit masking as well as collecting and queuing incoming operands, and accumulating output results.</li>
<li>So it can be though of like a calculator.</li>
<li>performs one 1 multiply and 1 add operation per 1 cycle</li>
<li>so altogether 2 operations x 10752 cuda cores x 1.7 GhZ clock speed(10^9 cycles per second) = 35.6 trillion calculations per second.</li>
</ul>
<h3 id="ray-tracing-cores">Ray tracing Cores<a hidden class="anchor" aria-hidden="true" href="#ray-tracing-cores">#</a></h3>
<p><img loading="lazy" src="gpus8.png" alt="eight"  />
</p>
<ul>
<li>used for executing ray tracing algorithms</li>
</ul>
<p>Depending on the amount of streaming multiprocessors that are damaged during manufacturing they are categoriezed and sold at different prices, for instance RTX 3090 ti has full 10752 cuda where as 3090 might have some damaged SMs. These cards might have different clock speed too.</p>
<h3 id="graphics-memory-gddr6x-sdram">Graphics Memory GDDR6X SDRAM<a hidden class="anchor" aria-hidden="true" href="#graphics-memory-gddr6x-sdram">#</a></h3>
<p>these  24GBs of GDDR6X surround the GPU chip
In order to run the operations of GPU chip</p>
<ul>
<li>They must be loaded from SSD to these graphics memory.</li>
<li>They have certain bandwidth i.e the amount of data they can transfer per second to the GPU chip. They have 1.15 Terbytes/sec bandwidth.</li>
<li>Similar to HBM memory in AI chips, would look something like this
<img loading="lazy" src="gpus10.png" alt="ten"  />
</li>
</ul>
<h3 id="how-are-operations-executed">How are operations executed?<a hidden class="anchor" aria-hidden="true" href="#how-are-operations-executed">#</a></h3>
<ul>
<li>Each instruction is executed by a thread (which is matched to a single cuda core) and these threads are combined into 32 cores called warp, 4 warps are combined to form thread blocks that are operated by Streaming Multiprocessor (SM). Similarly SM is combined together. All these are operated by Gigathread Engine</li>
</ul>
<p>If processor runs at 1Ghz, it can run 10^9 cycles per second, assuming 1cycle = 1 basic operation it can execute 10^9 operations.</p>
<ul>
<li>its basically like a unit of time, 10^9 cycles per 1 second means, one cycle takes 10^-9 seconds to run.</li>
<li>Different operations take varying amount of cycles (i.e latency of the operation) to perform.</li>
</ul>
<pre tabindex="0"><code>- Global memory access (up to 80GB): ~380 cycles
- L2 cache: ~200 cycles
- L1 cache or Shared memory access (up to 128 kb per Streaming Multiprocessor): ~34 cycles
- Fused multiplication and addition, a*b+c (FFMA): 4 cycles
- Tensor Core matrix multiply: 1 cycle
</code></pre><h3 id="tensor-cores-most-important">Tensor Cores (Most important)<a hidden class="anchor" aria-hidden="true" href="#tensor-cores-most-important">#</a></h3>
<p>These cores are used for matrix multiplication and matrix additions.</p>
<ul>
<li>predominantly used in deep learning calculation.</li>
</ul>
<p><img loading="lazy" src="gpus7.png" alt="seven"  />
</p>
<p>First lets start by understanding the precision formats that&rsquo;s used.</p>
<h4 id="1-fp16-half-precision-floating-point"><strong>1. FP16 (Half-Precision Floating-Point)</strong><a hidden class="anchor" aria-hidden="true" href="#1-fp16-half-precision-floating-point">#</a></h4>
<ul>
<li>
<p><strong>Full Name</strong>: Half-precision floating-point.</p>
</li>
<li>
<p><strong>Size</strong>: 16 bits (2 bytes).</p>
</li>
<li>
<p><strong>Bit Allocation</strong>:</p>
<ul>
<li>
<p>1 bit for the sign.</p>
</li>
<li>
<p>5 bits for the exponent.</p>
</li>
<li>
<p>10 bits for the mantissa.</p>
</li>
</ul>
</li>
<li>
<p><strong>Precision</strong>: About 3 decimal digits, 2^10 = 1024, taking log10(1024) = 3 decimal digits.</p>
</li>
</ul>
<h4 id="2-fp32-single-precision-floating-point"><strong>2. FP32 (Single-Precision Floating-Point)</strong><a hidden class="anchor" aria-hidden="true" href="#2-fp32-single-precision-floating-point">#</a></h4>
<ul>
<li>
<p><strong>Full Name</strong>: Single-precision floating-point.</p>
</li>
<li>
<p><strong>Size</strong>: 32 bits (4 bytes).</p>
</li>
<li>
<p><strong>Bit Allocation</strong>:</p>
<ul>
<li>
<p>1 bit for the sign.</p>
</li>
<li>
<p>8 bits for the exponent.</p>
</li>
<li>
<p>23 bits for the mantissa.</p>
</li>
</ul>
</li>
<li>
<p><strong>Precision</strong>: About 7 decimal digits.</p>
</li>
</ul>
<p>The operation performed by Tensor Core is something like this.</p>
<p>(picture)
It performs 64 FMA operations per clock.</p>
<h4 id="understanding-matrix-multiplication-in-tensor-cores">Understanding matrix multiplication in tensor cores.<a hidden class="anchor" aria-hidden="true" href="#understanding-matrix-multiplication-in-tensor-cores">#</a></h4>
<p><strong>Task</strong>:</p>
<p>Matrix multiply A and B with each size 32 x 32</p>
<p>lets say our tensor cores process 4x4 matrix multiplications per 1 cycle. The step to multiply A and B are</p>
<ul>
<li>divide A and B into tiles of 4x4 matrices i.e 64 4x4 tile for each matrix</li>
<li>load the tiles into shared memory (164KB) from global memory takes about 200 cycles.</li>
<li>load the tiles into tensor core registers for operation from shared memory, take about 34 cycles.</li>
<li>perform matrix multiplication on eight tensor cores all in parallel, take 1 cycle and then accumulate the values.</li>
<li>total cycle 200+34 + 1 = 235 cycles.</li>
</ul>
<h4 id="memory-bandwidth">Memory Bandwidth<a hidden class="anchor" aria-hidden="true" href="#memory-bandwidth">#</a></h4>
<p> we have seen that Tensor Cores are very fast. So fast, in fact, that they are idle most of the time as they are waiting for memory to arrive from global memory.
 For example, during GPT-3-sized training, which uses huge matrices — the larger, the better for Tensor Cores — we have a Tensor Core TFLOPS utilization of about 45-65%, meaning that even for the large neural networks about 50% of the time, Tensor Cores are idle.</p>
<h3 id="nvidia-ampere-architecture">Nvidia Ampere Architecture<a hidden class="anchor" aria-hidden="true" href="#nvidia-ampere-architecture">#</a></h3>
<ul>
<li><strong>108 streaming multiprocessors (SMs)</strong></li>
<li>432 Tensor Cores</li>
<li>6,912 FP32 Cuda Cores</li>
<li>3,456 FP64 cuda cores</li>
<li>(they share the same physical hardware, different in a sense that each require different clock speed and precision )</li>
</ul>
<p>More details in the picture below.</p>
<p><img loading="lazy" src="gpus12.png" alt="twelve"  />
</p>
<p>These TFLOPS are calculated using this formula</p>
<p>Number of cores for that precision (FP64) x clock speed x Operations per clock (generally 1FMA or 2 Operations)</p>
<ul>
<li>
<p><strong>Tensor Float 32 (TF32)</strong>:</p>
<ul>
<li><strong>156 TFLOPS | 312 TFLOPS</strong>*: TF32 is a mixed-precision format optimized for AI workloads. It provides a balance between FP16 and FP32 precision.</li>
</ul>
</li>
<li>
<p><strong>BFLOAT16</strong>:</p>
<ul>
<li><strong>312 TFLOPS | 624 TFLOPS</strong>*: BFLOAT16 is a 16-bit floating-point format used in deep learning, offering a good trade-off between range and precision.</li>
</ul>
</li>
<li>
<p><strong>FP16 Tensor Core</strong>:</p>
<ul>
<li><strong>312 TFLOPS | 624 TFLOPS</strong>*: FP16 is used for deep learning training and inference, providing high throughput for lower-precision computations.</li>
</ul>
</li>
<li>
<p><strong>INT8 Tensor Core</strong>:</p>
<ul>
<li><strong>624 TOPS | 1248 TOPS</strong>*: INT8 is used for inference tasks, where lower precision is acceptable, and high throughput is critical.</li>
</ul>
</li>
</ul>
<p>This figure shows the range and precision of each of these.</p>
<p><img loading="lazy" src="gpus13.png" alt="thirteen"  />
</p>
<p>Preferred:</p>
<ul>
<li>TF32 for training and mostly FP16 and BF16 for inference.</li>
</ul>
<p>Tensor cores are optimized for matrix multiplications so it can peform more operations per clock rather than just 64 FMA per clock.</p>
<h3 id="sparsity">Sparsity<a hidden class="anchor" aria-hidden="true" href="#sparsity">#</a></h3>
<p>Matrix contains large number of zeros in it, by using a fine-grained pruning algorithm to compress (essentially removing) small and zero-value matrices, the GPU saves computing resources, power, memory and bandwidth.</p>
<h3 id="form-factor">Form Factor<a hidden class="anchor" aria-hidden="true" href="#form-factor">#</a></h3>
<p>They define how the GPU is physically integrated into a system and how it connects to other components like the CPU and memory.</p>
<ul>
<li><strong>PCIe</strong> is a standard interface used to connect GPUs, SSDs, network cards, and other peripherals to a computer’s motherboard</li>
<li><strong>SXM</strong> are not standalone cards and is used in data centers.</li>
</ul>
<table>
<thead>
<tr>
<th><strong>Bandwidth</strong></th>
<th>PCIe Gen4: 64 GB/s (x16)</th>
<th>NVLink: 600 GB/s (per GPU pair)</th>
</tr>
</thead>
</table>
<h3 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h3>
<ul>
<li><a href="https://youtu.be/h9Z4oGN89MU?si=3oqaqgIWJMSQAMz3">How do Graphics Cards Work? Exploring GPU Architecture</a></li>
<li><a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf">NVIDIA A100 TENSOR CORE GPU</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://cohlem.github.io/sub-notes/rope/">
    <span class="title">« Prev</span>
    <br>
    <span>RoPE</span>
  </a>
  <a class="next" href="https://cohlem.github.io/sub-notes/ddp/">
    <span class="title">Next »</span>
    <br>
    <span>DDP and gradient sync</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://cohlem.github.io">CohleM</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
