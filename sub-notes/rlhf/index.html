<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>RLHF | CohleM</title>
<meta name="keywords" content="">
<meta name="description" content="General RL setting $$ J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \right],\qquad{(1)} $$ In RL setting we aim to optimize the objective function $J(\pi)$ by updating the policy $\pi$ given a reward function $r(s,a)$ that takes in a state and the action performed at that state. The next action is determined by $\pi(a|s)$. We find the expected(average) reward over all the trajectories $\tau$. $\gamma$ is the discount factor from 0 to 1 that balances the desirability of of near vs future-rewards.">
<meta name="author" content="CohleM">
<link rel="canonical" href="https://cohlem.github.io/sub-notes/rlhf/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css" integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe&#43;FVUFzPh7U=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://cohlem.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://cohlem.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://cohlem.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://cohlem.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://cohlem.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script type="text/javascript">
  MathJax = {
    tex: {
      inlineMath: [["\\(", "\\)"], ["$", "$"]],
      displayMath: [["\\[", "\\]"], ["$$", "$$"]]
    },
    options: {
      skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"]
    }
  };
</script>



<script async src="https://www.googletagmanager.com/gtag/js?id=G-X6LV4QY2G2"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-X6LV4QY2G2', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="RLHF" />
<meta property="og:description" content="General RL setting $$ J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \right],\qquad{(1)} $$ In RL setting we aim to optimize the objective function $J(\pi)$ by updating the policy $\pi$ given a reward function $r(s,a)$ that takes in a state and the action performed at that state. The next action is determined by $\pi(a|s)$. We find the expected(average) reward over all the trajectories $\tau$. $\gamma$ is the discount factor from 0 to 1 that balances the desirability of of near vs future-rewards." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cohlem.github.io/sub-notes/rlhf/" /><meta property="article:section" content="sub-notes" />
<meta property="article:published_time" content="2025-02-24T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-02-24T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="RLHF"/>
<meta name="twitter:description" content="General RL setting $$ J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \right],\qquad{(1)} $$ In RL setting we aim to optimize the objective function $J(\pi)$ by updating the policy $\pi$ given a reward function $r(s,a)$ that takes in a state and the action performed at that state. The next action is determined by $\pi(a|s)$. We find the expected(average) reward over all the trajectories $\tau$. $\gamma$ is the discount factor from 0 to 1 that balances the desirability of of near vs future-rewards."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Sub-notes",
      "item": "https://cohlem.github.io/sub-notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "RLHF",
      "item": "https://cohlem.github.io/sub-notes/rlhf/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "RLHF",
  "name": "RLHF",
  "description": "General RL setting $$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) \\right],\\qquad{(1)} $$ In RL setting we aim to optimize the objective function $J(\\pi)$ by updating the policy $\\pi$ given a reward function $r(s,a)$ that takes in a state and the action performed at that state. The next action is determined by $\\pi(a|s)$. We find the expected(average) reward over all the trajectories $\\tau$. $\\gamma$ is the discount factor from 0 to 1 that balances the desirability of of near vs future-rewards.",
  "keywords": [
    
  ],
  "articleBody": "General RL setting $$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) \\right],\\qquad{(1)} $$ In RL setting we aim to optimize the objective function $J(\\pi)$ by updating the policy $\\pi$ given a reward function $r(s,a)$ that takes in a state and the action performed at that state. The next action is determined by $\\pi(a|s)$. We find the expected(average) reward over all the trajectories $\\tau$. $\\gamma$ is the discount factor from 0 to 1 that balances the desirability of of near vs future-rewards.\nAn example: If our RL setup is confined to finding a maze, a reward function could be $r(s,a)=-\\text{distance to goal}$\nRLHF RLHF reduces the equation 1 by removing this discounted factor $$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi} \\left[ \\sum_{t=0}^{\\infty} r(s_t, a_t) \\right],\\qquad{(2)} $$ We aim to maximize our objective function by optimizing the policy. The reward function is designed such that the actions must align with the human preferences.\nThe most common reward model predicts the probability that a piece of text was close to a “preferred” piece of text from the training comparisons.\nReward Models Given two prompts $y1$ and $y2$ we want to have a reward model that gives high score to $y1$ and low score to $y2$ meaning $y1$ is always preferred over $y2$. Their relative preference is given by the Bradly Terry model.\n$$ P(i \u003e j) = \\frac{p_i}{p_i + p_j}\\qquad{(3)} $$ It gives the probability of i being preferred over j where $p_{i}$ and $p_{j}$ represent “strengths” for those prompts.\nWe want our model to maximize this probability because later $i$ would represent the text we want (aligned) and $j$ would represent the text we don’t want (not aligned). The training objective can be derived from the equation above. The “strengths” are exponential because we want them to be strictly positive.\n$$ P(y_1 \u003e y_2) = \\frac{\\exp(r(y_1))}{\\exp(r(y_1)) + \\exp(r(y_2))}\\qquad{(4)} $$ The loss function becomes\n$$ \\mathcal{L}(\\theta) = - \\log \\left( \\sigma \\left( r_{\\theta}(x, y_w) - r_{\\theta}(x, y_l) \\right) \\right)\\qquad{(6)} $$ Our existing model can be configured to output just one value by adding a linear head to the model.\nIn case of language models, we want a model to rate our answer i.e (give a score based on how good or bad it is), it is the most important part because it will guide our training, it is providing supervision for our PPO algorithm.\nSteps for training a reward model Collect pairs of data, it could be either contrasting pairs or some pairs of prompt with priority. Eg. We want our llm to be trained (later using PPO) to be to generate positive response. In this case our priority prompt would be positive prompt and non-priority prompt would be negative prompt.\nWe take a language model and add a linear head to it. For instance, for each token a LM outputs 512 dimension vector we add a new head that takes in 512 dimension and outputs a one dimensional vector which gives us the reward.\nThe loss function for the reward model is constructed likewise, which is same the equation 6.\n$$ L = -log(\\sigma(r1 - r2)) $$ where $r1$ is reward for priority prompt and $r2$ is reward for non-priority prompt. We want to maximize this difference $r1 - r2$ and minimize the this function. $\\sigma$ represents sigmoid function.\nNOTES we calculate the reward for the ending token which represents a reward given to the whole token. reward models overfit fast, so we can consider smaller LM for reward model training. ——-Skipping other reward models for now——-\nRegularization We want aligned reward models but would still “not go off the rails” meaning it stays within the limitation of our reference model. It allows the policy being trained to stay close to the reference policy.\n$$ r = r_\\theta - \\lambda r_{\\text{reg.}} \\qquad{(1)} $$ $$ r = r_\\theta - \\lambda_{\\text{KL}} \\mathcal{D}_{\\text{KL}} \\left( \\pi^{\\text{RL}}(y \\mid x) , | , \\pi^{\\text{Ref.}}(y \\mid x) \\right) \\qquad{(2)} $$\nKL divergence is calculated as\n$$ D_{\\text{KL}}(P ,||, Q) = \\mathbb{E}_{x \\sim P} \\left[ \\log P(x) - \\log Q(x) \\right]. \\qquad{(3)} $$\nRejection Sampling This process is kind of a filtering process. We first sample outputs from our base language model that we’ll be training next. For example, we generate 5 example outputs for a sample prompt. We then provide a score to each of the generated examples using our reward function. We sort and only take the top-K elements per prompt and fine-tune our base model on these examples. So it’s kind of like a filtering and only keeping the highly rewarded examples.\nPPO PPO algorithm analogy Suppose we are taking an exam paper. Our objective is to maximize the change of getting good marks by updating our brain (weights and biases).\n$s_t$ is a question that we are looking at and trying to solve. $a_t$ is the our answer for that question. $r_t$ is the immediate reward we get after writing $a_t$, $s_{t+1}$ is the next-question.\n$R_t$ is the actual total exam score.\nSuppose there is an imaginary machine that gives us the expected exam score that we can get just by looking at our question which is $V(s)$.\n$A(s)$ is our advantage i.e how good we are compared to the predicted score.\n$A(s) = R_t - V(s)$\nthis can be modified as as $\\delta_t = R_t - V(s)$ and $A_t = \\delta_t + \\lambda\\cdot\\gamma\\cdot A_{t+1}$. This is just a modified version of advantage to remove the bias and variance.\nThis can be a little confusing as we might have no idea what our actual total score($R_t$) will be while we are still writing some questions $s$ so approximate this $R_t$ with the help of current reward $r$ and future reward that we might get from next question i.e $V(s+1)$ i.e\nthis becomes\n$\\delta_t = r_t + V(s+1) - V(s)$\nso this will still give us our advantage at a point t.\ni.e how good/bad we did at point t= immediate reward(marks) after writing answer to t + expected future reward from new question - expected future reward from previous question t.\nThis will give us our advantage.\nPPO is done using two phases.\nRollout phase Weight update phase. Rollout phase We write a lot of exam paper in this phase in parallel. for each exam paper and for each question in the exam paper we calculate $r_t$, $V(s_t)$ and $R_t$ $A_t$ and use it in our equation. $$ L^{PPO}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}\\left(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon\\right) \\hat{A}_t \\right) - c_1 \\left( V_\\theta(s_t) - V_t^\\text{target} \\right)^2 + c_2 \\mathcal{H}\\left\\pi_\\theta\\right\\right] $$ $$ r_t(\\theta) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_\\text{old}}(a_t | s_t)} $$\nWeight update phase We find this loss and try to maximize this clipped loss and entropy loss, but minimize the value function loss.\nThe PPO clipped surrogate objective is given as:\n$$ L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t \\Big[ \\min \\big( r_t(\\theta) \\hat{A}_t, ; \\mathrm{clip}(r_t(\\theta), 1-\\varepsilon, 1+\\varepsilon) \\hat{A}_t \\big) \\Big] $$\nThe gradient ascent update rule is:\n$$ \\theta \\gets \\theta + \\alpha \\nabla_\\theta L^{\\text{CLIP}}(\\theta) $$ The most important part to understand here is\n$r_t(\\theta) \\hat{A}_t$\nso when this term gets clipped to either $1 + \\varepsilon$ or $1-\\varepsilon$ the gradient of this loss $\\nabla_{\\theta}r_t(\\theta) \\hat{A}_t$ is 0. So no update to the weights. But when this gradient is $\\nabla_{\\theta}r_t(\\theta) \\hat{A}_t$ the update depends on whether $\\hat{A}_t$ is \u003e0 or \u003c0,\nIf $\\hat{A}_t \u003e 0$, the gradient update will be in the direction that increases ${\\pi_{\\theta}}$. If $\\hat{A}_t \u003e 0$, the gradient update will be in the direction that decreases ${\\pi_{\\theta}}$.\nIntegrating reward model to PPO As you might have noticed, the PPO loss comes from the token level, meaning we need loprobs, value, reward, return and advantage for each token. Buuuuut, this reward function we just trained is trained to output only one reward for last token so how does that work?\nAnswer: the reward is propagated.\nThe advantage is calculated reverse recursively i.e advantage at token n is also passed to the n-1 token. This means that we are looking ahead and telling our token n-1 that we already ended up a good state because of the state we are in. Lets look at this from the lens of advantage formula.\n$\\delta_t = R_t - V(s)$ and $A_t = \\delta_t + \\lambda\\cdot\\gamma\\cdot A_{t+1}$\nLets consider we are at 100th token which is the ending token of our sequence\n$$R_t = r_t + V_{t+1}(s) - V_t(s)$$ reward model assigns $r_{100}$ 100 and lets ignore both the value function assuming they cancel out as we are already at the end.\n$$ \\begin{gather} R_t= 100\\\\A_{100}=100, considering A_{101}=0 \\end{gather} $$ At 100th token we are at an advantage, now lets calculate $A_{99}$\n$$ A_{99} = \\delta_{99} + \\lambda\\cdot\\gamma\\cdot A_{100} $$ as you can see the reward 100 is propagated to the 99th token, considering $\\delta_{99}$ is positive here, it tells us that token 99 is still at an advantage because the we can already see the future here i.e 100th token which was at an advantage, so taking action 99 is still an advantage to us.\nReward Hacking our models can find a loophole to maximize their return by generating high reward tokens but no-so-good answer. Example: If we are trying to make our model positive, model may find a way to output tokens such as “thank you” and add it our answer, which will provide a high reward to it, but it is meaningless to us. So we don’t want our new (trained via PPO) model to deviate significantly from the model that we started from (SFT model), so we add KL divergence as a penalty to our each token’s reward.\nAs described earlier, reward is provided to only the ending token, and the reward for other token becomes this KL penalty.\ni.e if we have token_1, token_2, and token_3\nr(token_3) = some reward from the reward model r(token_2) = KL penalty i.e (logprobs_for_token_2_from_model_being_trained - logprobs_for_token_2_from_SFT_model)* -KL_penalty_coefficient. and so on…\nSome key points\nHuman preferences that are used to train LLMs are multi-dimensional but the reward is a single score. LLMs being complex and “intelligent” will always find a way to exploit these rewards thus the reward hacking, so in large scale RLHF, reward models get saturated very fast, and we might need to train a new reward model. GRPO The per token loss for GRPO is likewise..\nKey difference between GRPO and PPO.\nGRPO completely removes values function. as value function is removed the advantage calculate is simplified.\n$$ A_i = \\frac{r_i - \\text{mean}({r_1, r_2, \\cdots, r_G})}{\\text{std}({r_1,r_2, \\cdots,r_G})} $$\nFor each question/prompt different G samples are generated, and for each advantage for each question the reward is normalized to form the advantage.\nPreviously, in PPO we added KL penalty to the rewards themselves, but in GRPO we add it to the loss function directly.\n$$ J(\\theta) = \\frac{1}{G}\\sum_{i=1}^G \\frac{1}{|a_i|} \\sum_{t=1}^{|a_i|} \\left( \\min\\left(\\frac{\\pi_\\theta(a_{i,t}|s_{i,t})}{\\pi_{\\theta_{old}}(a_{i,t}|s_{i,t})}A_{i,t}, \\text{clip} \\left( \\frac{\\pi_\\theta(a_{i,t}|s_{i,t})}{\\pi_{\\theta_{old}}(a_{i,t}|s_{i,t})}, 1-\\varepsilon, 1+\\varepsilon \\right) A_{i,t} \\right) - \\beta D_{KL}(\\pi_\\theta(\\cdot|s_{i,t})||\\pi_{ref}(\\cdot|s_{i,t})) \\right) $$\nconnect to ssh via vscode, transfer notebook using scp source destination open that notebook in vscode install jupyter from extension select from top right\u003e kernels\u003e python kernals \u003e install python environments.\n",
  "wordCount" : "1844",
  "inLanguage": "en",
  "datePublished": "2025-02-24T00:00:00Z",
  "dateModified": "2025-02-24T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "CohleM"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://cohlem.github.io/sub-notes/rlhf/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "CohleM",
    "logo": {
      "@type": "ImageObject",
      "url": "https://cohlem.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://cohlem.github.io" accesskey="h" title="CohleM (Alt + H)">CohleM</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://cohlem.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://cohlem.github.io">Home</a>&nbsp;»&nbsp;<a href="https://cohlem.github.io/sub-notes/">Sub-notes</a></div>
    <h1 class="post-title">
      RLHF
    </h1>
    <div class="post-meta"><span title='2025-02-24 00:00:00 +0000 UTC'>February 24, 2025</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;CohleM

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#general-rl-setting" aria-label="General RL setting">General RL setting</a></li>
                <li>
                    <a href="#rlhf" aria-label="RLHF">RLHF</a></li>
                <li>
                    <a href="#reward-models" aria-label="Reward Models">Reward Models</a><ul>
                        
                <li>
                    <a href="#steps-for-training-a-reward-model" aria-label="Steps for training a reward model">Steps for training a reward model</a></li></ul>
                </li>
                <li>
                    <a href="#notes" aria-label="NOTES">NOTES</a></li>
                <li>
                    <a href="#regularization" aria-label="Regularization">Regularization</a></li>
                <li>
                    <a href="#rejection-sampling" aria-label="Rejection Sampling">Rejection Sampling</a></li>
                <li>
                    <a href="#ppo" aria-label="PPO">PPO</a><ul>
                        
                <li>
                    <a href="#ppo-algorithm-analogy" aria-label="PPO algorithm analogy">PPO algorithm analogy</a></li>
                <li>
                    <a href="#rollout-phase" aria-label="Rollout phase">Rollout phase</a></li>
                <li>
                    <a href="#weight-update-phase" aria-label="Weight update phase">Weight update phase</a></li></ul>
                </li>
                <li>
                    <a href="#integrating-reward-model-to-ppo" aria-label="Integrating reward model to PPO">Integrating reward model to PPO</a></li>
                <li>
                    <a href="#reward-hacking" aria-label="Reward Hacking">Reward Hacking</a></li>
                <li>
                    <a href="#grpo" aria-label="GRPO">GRPO</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="general-rl-setting">General RL setting<a hidden class="anchor" aria-hidden="true" href="#general-rl-setting">#</a></h3>
<p>$$
J(\pi) =
\mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t,
a_t) \right],\qquad{(1)}
$$
In RL setting we aim to optimize the objective function $J(\pi)$ by updating the policy $\pi$
given a reward function $r(s,a)$ that takes in a state and the action performed at that state. The next action is determined by $\pi(a|s)$. We find the expected(average) reward over all the trajectories $\tau$. $\gamma$ is the discount factor from 0 to 1 that balances the desirability of of near vs future-rewards.</p>
<p>An example: If our RL setup is confined to finding a maze, a reward function could be $r(s,a)=-\text{distance to goal}$</p>
<h3 id="rlhf">RLHF<a hidden class="anchor" aria-hidden="true" href="#rlhf">#</a></h3>
<p>RLHF reduces the equation 1 by removing this discounted factor
$$
J(\pi) =
\mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{\infty} r(s_t,
a_t) \right],\qquad{(2)}
$$
We aim to maximize our objective function by optimizing the policy. The reward function is designed such that the actions must align with the human preferences.</p>
<p>The most common reward model predicts the probability that a piece of text was close to a &ldquo;preferred&rdquo; piece of text from the training comparisons.</p>
<h3 id="reward-models">Reward Models<a hidden class="anchor" aria-hidden="true" href="#reward-models">#</a></h3>
<p>Given two prompts $y1$ and $y2$ we want to have a reward model that gives high score to $y1$ and low score to $y2$ meaning $y1$ is always preferred over $y2$. Their relative preference is given by the Bradly Terry model.</p>
<p>$$
P(i &gt; j) =
\frac{p_i}{p_i + p_j}\qquad{(3)}
$$
It gives the probability of i being preferred over j where $p_{i}$ and $p_{j}$ represent &ldquo;strengths&rdquo; for those prompts.</p>
<p>We want our model to maximize this probability because later $i$ would represent the text we want (aligned) and $j$ would represent the text we don&rsquo;t want (not aligned). The training objective can be derived from the equation above. The &ldquo;strengths&rdquo; are exponential because we want them to be strictly positive.</p>
<p>$$
P(y_1 &gt;
y_2) = \frac{\exp(r(y_1))}{\exp(r(y_1)) +
\exp(r(y_2))}\qquad{(4)}
$$
The loss function becomes</p>
<p>$$
\mathcal{L}(\theta) = - \log \left( \sigma
\left( r_{\theta}(x, y_w) - r_{\theta}(x, y_l) \right)
\right)\qquad{(6)}
$$
Our existing model can be configured to output just one value by adding a linear head to the model.</p>
<p>In case of language models, we want a model to rate our answer i.e (give a score based on how good or bad it is), it is the most important part because it will guide our training, it is providing supervision for our PPO algorithm.</p>
<h4 id="steps-for-training-a-reward-model">Steps for training a reward model<a hidden class="anchor" aria-hidden="true" href="#steps-for-training-a-reward-model">#</a></h4>
<p>Collect pairs of data, it could be either contrasting pairs or some pairs of prompt with priority. Eg. We want our llm to be trained (later using PPO) to be to generate positive response. In this case our priority prompt would be positive prompt and non-priority prompt would be negative prompt.</p>
<p>We take a language model and add a linear head to it. For instance, for each token a LM outputs 512 dimension vector we add a new head that takes in 512 dimension and outputs a one dimensional vector which gives us the reward.</p>
<p>The loss function for the reward model is constructed likewise, which is same the equation 6.</p>
<p>$$
L = -log(\sigma(r1 - r2))
$$
where $r1$ is reward for priority prompt and $r2$ is reward for non-priority prompt. We want to maximize this difference $r1 - r2$ and minimize the this function. $\sigma$ represents sigmoid function.</p>
<h3 id="notes">NOTES<a hidden class="anchor" aria-hidden="true" href="#notes">#</a></h3>
<ul>
<li>we calculate the reward for the ending token which represents a reward given to the whole token.</li>
<li>reward models overfit fast, so we can consider smaller LM for reward model training.</li>
</ul>
<p>&mdash;&mdash;-<strong>Skipping other reward models for now</strong>&mdash;&mdash;-</p>
<h3 id="regularization">Regularization<a hidden class="anchor" aria-hidden="true" href="#regularization">#</a></h3>
<p>We want aligned reward models but would still &ldquo;not go off the rails&rdquo; meaning it stays within the limitation of our reference model. It allows the policy being trained to stay close to the reference policy.</p>
<p>$$
r = r_\theta -
\lambda r_{\text{reg.}} \qquad{(1)}
$$
$$
r = r_\theta - \lambda_{\text{KL}} \mathcal{D}_{\text{KL}} \left(
\pi^{\text{RL}}(y \mid x) , | , \pi^{\text{Ref.}}(y \mid x) \right)
\qquad{(2)}
$$</p>
<p>KL divergence is calculated as</p>
<p>$$
D_{\text{KL}}(P ,||, Q) = \mathbb{E}_{x \sim P} \left[ \log P(x) -
\log Q(x) \right].
\qquad{(3)}
$$</p>
<h3 id="rejection-sampling">Rejection Sampling<a hidden class="anchor" aria-hidden="true" href="#rejection-sampling">#</a></h3>
<p>This process is kind of a filtering process. We first sample outputs from our base language model that we&rsquo;ll be training next. For example, we generate 5 example outputs for a sample prompt. We then provide a score to each of the generated examples using our reward function. We sort and only take the top-K elements per prompt and fine-tune our base model on these examples. So it&rsquo;s kind of like a filtering and only keeping the highly rewarded examples.</p>
<h3 id="ppo">PPO<a hidden class="anchor" aria-hidden="true" href="#ppo">#</a></h3>
<h4 id="ppo-algorithm-analogy">PPO algorithm analogy<a hidden class="anchor" aria-hidden="true" href="#ppo-algorithm-analogy">#</a></h4>
<p>Suppose we are taking an exam paper. Our objective is to maximize the change of getting good marks by updating our brain (weights and biases).</p>
<p>$s_t$ is a question that we are looking at and trying to solve. $a_t$ is the our answer for that question. $r_t$ is the immediate reward we get after writing $a_t$, $s_{t+1}$ is the next-question.</p>
<p>$R_t$ is the actual total exam score.</p>
<p>Suppose there is an imaginary machine that gives us the expected exam score that we can get just by looking at our question which is $V(s)$.</p>
<p>$A(s)$ is our advantage i.e how good we are compared to the predicted score.</p>
<p>$A(s) = R_t - V(s)$</p>
<p>this can be modified as  as $\delta_t = R_t - V(s)$ and $A_t = \delta_t + \lambda\cdot\gamma\cdot A_{t+1}$. This is just a modified version of advantage to remove the bias and variance.</p>
<p>This can be a little confusing as we might have no idea what our actual total score($R_t$) will be while we are still writing some questions $s$ so approximate this $R_t$ with the help of current reward $r$ and future reward that we might get from next question i.e $V(s+1)$ i.e</p>
<p>this becomes</p>
<p>$\delta_t =  r_t + V(s+1) - V(s)$</p>
<p>so this will still give us our advantage at a point t.</p>
<p>i.e how good/bad we did at point t= immediate reward(marks) after writing answer to t + expected future reward from new question - expected future reward from previous question t.</p>
<p>This will give us our advantage.</p>
<p>PPO is done using two phases.</p>
<ol>
<li>Rollout phase</li>
<li>Weight update phase.</li>
</ol>
<h4 id="rollout-phase">Rollout phase<a hidden class="anchor" aria-hidden="true" href="#rollout-phase">#</a></h4>
<p>We write a lot of exam paper in this phase in parallel.
for each exam paper and for each question in the exam paper we calculate $r_t$, $V(s_t)$ and $R_t$ $A_t$ and use it in our equation.
$$
L^{PPO}(\theta) = \mathbb{E}_t \left[
\min \left( r_t(\theta) \hat{A}_t, \text{clip}\left(r_t(\theta), 1 - \epsilon, 1 + \epsilon\right) \hat{A}_t \right) - c_1 \left( V_\theta(s_t) - V_t^\text{target} \right)^2 + c_2 \mathcal{H}\left<a href="s_t">\pi_\theta\right</a>\right]
$$
$$
r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_\text{old}}(a_t | s_t)}
$$</p>
<h4 id="weight-update-phase">Weight update phase<a hidden class="anchor" aria-hidden="true" href="#weight-update-phase">#</a></h4>
<p>We find this loss and try to maximize this clipped loss and entropy loss, but minimize the value function loss.</p>
<p>The PPO clipped surrogate objective is given as:</p>
<p>$$
L^{\text{CLIP}}(\theta) = \mathbb{E}_t \Big[ \min \big( r_t(\theta) \hat{A}_t, ; \mathrm{clip}(r_t(\theta), 1-\varepsilon, 1+\varepsilon) \hat{A}_t \big) \Big]
$$</p>
<p>The gradient ascent update rule is:</p>
<p>$$
\theta \gets \theta + \alpha \nabla_\theta L^{\text{CLIP}}(\theta)
$$
The most important part to understand here is</p>
<p>$r_t(\theta) \hat{A}_t$</p>
<p>so when this term gets clipped to either $1 + \varepsilon$ or $1-\varepsilon$ the gradient of this loss $\nabla_{\theta}r_t(\theta) \hat{A}_t$ is 0. So no update to the weights. But when this gradient is $\nabla_{\theta}r_t(\theta) \hat{A}_t$ the update depends on whether $\hat{A}_t$ is &gt;0 or &lt;0,</p>
<p>If $\hat{A}_t &gt; 0$, the gradient update will be in the direction that increases ${\pi_{\theta}}$.
If $\hat{A}_t &gt; 0$, the gradient update will be in the direction that decreases ${\pi_{\theta}}$.</p>
<h3 id="integrating-reward-model-to-ppo">Integrating reward model to PPO<a hidden class="anchor" aria-hidden="true" href="#integrating-reward-model-to-ppo">#</a></h3>
<p>As you might have noticed, the PPO loss comes from the token level, meaning we need loprobs, value, reward, return and advantage for each token. Buuuuut, this reward function we just trained is trained to output only one reward for last token so how does that work?</p>
<p>Answer: the reward is propagated.</p>
<p>The advantage is calculated reverse recursively i.e advantage at token n is also passed to the n-1 token. This means that we are looking ahead and telling our token n-1 that we already ended up a good state because of the state we are in. Lets look at this from the lens of advantage formula.</p>
<p>$\delta_t = R_t - V(s)$ and  $A_t = \delta_t + \lambda\cdot\gamma\cdot A_{t+1}$</p>
<p>Lets consider we are at 100th token which is the ending token of our sequence</p>
<p>$$R_t = r_t + V_{t+1}(s) - V_t(s)$$
reward model assigns $r_{100}$ 100 and lets ignore both the value function assuming they cancel out as we are already at the end.</p>
<p>$$
\begin{gather}
R_t= 100\\A_{100}=100, considering A_{101}=0
\end{gather}
$$
At 100th token we are at an advantage, now lets calculate $A_{99}$</p>
<p>$$
A_{99} = \delta_{99} + \lambda\cdot\gamma\cdot A_{100}
$$
as you can see the reward 100 is propagated to the 99th token, considering $\delta_{99}$ is positive here, it tells us that token 99 is still at an advantage because the we can already see the future here i.e 100th token which was at an advantage, so taking action 99 is still an advantage to us.</p>
<h3 id="reward-hacking">Reward Hacking<a hidden class="anchor" aria-hidden="true" href="#reward-hacking">#</a></h3>
<p>our models can find a loophole to maximize their return by generating high reward tokens but no-so-good answer. Example: If we are trying to make our model positive, model may find a way to output tokens such as &ldquo;thank you&rdquo; and add it our answer, which will provide a high reward to it, but it is meaningless to us. So we don&rsquo;t want our new (trained via PPO) model to deviate significantly from the model that we started from (SFT model), so we add KL divergence as a penalty to our each token&rsquo;s reward.</p>
<p>As described earlier, reward is provided to only the ending token, and the reward for other token becomes this KL penalty.</p>
<p>i.e if we have token_1, token_2, and token_3</p>
<p>r(token_3) = some reward from the reward model
r(token_2) = KL penalty i.e (logprobs_for_token_2_from_model_being_trained - logprobs_for_token_2_from_SFT_model)* -KL_penalty_coefficient.
and so on&hellip;</p>
<p>Some key points</p>
<ul>
<li>Human preferences that are used to train LLMs are multi-dimensional but the reward is a single score. LLMs being complex and &ldquo;intelligent&rdquo; will always find a way to exploit these rewards thus the reward hacking, so in large scale RLHF, reward models get saturated very fast, and we might need to train a new reward model.</li>
</ul>
<h3 id="grpo">GRPO<a hidden class="anchor" aria-hidden="true" href="#grpo">#</a></h3>
<p>The per token loss for GRPO is likewise..</p>
<p>Key difference between GRPO and PPO.</p>
<p>GRPO completely removes values function.
as value function is removed the advantage calculate is simplified.</p>
<p>$$
A_i = \frac{r_i - \text{mean}({r_1, r_2, \cdots, r_G})}{\text{std}({r_1,r_2, \cdots,r_G})}
$$</p>
<p>For each question/prompt different G samples are generated, and for each advantage for each question the reward is normalized to form the advantage.</p>
<p>Previously, in PPO we added KL penalty to the rewards themselves, but in GRPO we add it to the loss function directly.</p>
<p>$$
J(\theta) =
\frac{1}{G}\sum_{i=1}^G  \frac{1}{|a_i|} \sum_{t=1}^{|a_i|} \left(
\min\left(\frac{\pi_\theta(a_{i,t}|s_{i,t})}{\pi_{\theta_{old}}(a_{i,t}|s_{i,t})}A_{i,t},
\text{clip} \left(
\frac{\pi_\theta(a_{i,t}|s_{i,t})}{\pi_{\theta_{old}}(a_{i,t}|s_{i,t})},
1-\varepsilon, 1+\varepsilon \right) A_{i,t} \right) - \beta
D_{KL}(\pi_\theta(\cdot|s_{i,t})||\pi_{ref}(\cdot|s_{i,t}))
\right)
$$</p>
<p>connect to ssh via vscode,
transfer notebook using scp source destination
open that notebook in vscode
install jupyter from extension
select from top right&gt; kernels&gt; python kernals &gt; install python environments.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://cohlem.github.io/sub-notes/interpretability/">
    <span class="title">« Prev</span>
    <br>
    <span>Interpretability</span>
  </a>
  <a class="next" href="https://cohlem.github.io/sub-notes/flops-calculation/">
    <span class="title">Next »</span>
    <br>
    <span>Flops calculation</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://cohlem.github.io">CohleM</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
