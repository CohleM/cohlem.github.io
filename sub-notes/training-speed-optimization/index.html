<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Training Speed Optimization | CohleM</title>
<meta name="keywords" content="">
<meta name="description" content="Precision The more the precision point the less operation (TFLOPS) is performed.
FP64 used for scientific research purposes, where precision is a must. TF32 and BFLOAT16 are mostly used in NN training. INT8 is used for inference. Picture below shows specifications of A100 GPU.
Using these precision points may have some difference in code. See pytorch&rsquo;s docs
torch.compile It works in a similar fashion like the GCC compiler. It works by reducing overheads introduced by the python interpreter and optimizing the GPU read and writes.">
<meta name="author" content="CohleM">
<link rel="canonical" href="https://cohlem.github.io/sub-notes/training-speed-optimization/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css" integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe&#43;FVUFzPh7U=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://cohlem.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://cohlem.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://cohlem.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://cohlem.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://cohlem.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script type="text/javascript">
  MathJax = {
    tex: {
      inlineMath: [["\\(", "\\)"], ["$", "$"]],
      displayMath: [["\\[", "\\]"], ["$$", "$$"]]
    },
    options: {
      skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"]
    }
  };
</script>



<script async src="https://www.googletagmanager.com/gtag/js?id=G-X6LV4QY2G2"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-X6LV4QY2G2', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Training Speed Optimization" />
<meta property="og:description" content="Precision The more the precision point the less operation (TFLOPS) is performed.
FP64 used for scientific research purposes, where precision is a must. TF32 and BFLOAT16 are mostly used in NN training. INT8 is used for inference. Picture below shows specifications of A100 GPU.
Using these precision points may have some difference in code. See pytorch&rsquo;s docs
torch.compile It works in a similar fashion like the GCC compiler. It works by reducing overheads introduced by the python interpreter and optimizing the GPU read and writes." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cohlem.github.io/sub-notes/training-speed-optimization/" /><meta property="article:section" content="sub-notes" />
<meta property="article:published_time" content="2025-01-02T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-01-02T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Training Speed Optimization"/>
<meta name="twitter:description" content="Precision The more the precision point the less operation (TFLOPS) is performed.
FP64 used for scientific research purposes, where precision is a must. TF32 and BFLOAT16 are mostly used in NN training. INT8 is used for inference. Picture below shows specifications of A100 GPU.
Using these precision points may have some difference in code. See pytorch&rsquo;s docs
torch.compile It works in a similar fashion like the GCC compiler. It works by reducing overheads introduced by the python interpreter and optimizing the GPU read and writes."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Sub-notes",
      "item": "https://cohlem.github.io/sub-notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Training Speed Optimization",
      "item": "https://cohlem.github.io/sub-notes/training-speed-optimization/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Training Speed Optimization",
  "name": "Training Speed Optimization",
  "description": "Precision The more the precision point the less operation (TFLOPS) is performed.\nFP64 used for scientific research purposes, where precision is a must. TF32 and BFLOAT16 are mostly used in NN training. INT8 is used for inference. Picture below shows specifications of A100 GPU.\nUsing these precision points may have some difference in code. See pytorch\u0026rsquo;s docs\ntorch.compile It works in a similar fashion like the GCC compiler. It works by reducing overheads introduced by the python interpreter and optimizing the GPU read and writes.",
  "keywords": [
    
  ],
  "articleBody": "Precision The more the precision point the less operation (TFLOPS) is performed.\nFP64 used for scientific research purposes, where precision is a must. TF32 and BFLOAT16 are mostly used in NN training. INT8 is used for inference. Picture below shows specifications of A100 GPU.\nUsing these precision points may have some difference in code. See pytorch’s docs\ntorch.compile It works in a similar fashion like the GCC compiler. It works by reducing overheads introduced by the python interpreter and optimizing the GPU read and writes.\nFor instance\ndef gelu(x): \"\"\" Applies the GELU activation function to the input. \"\"\" return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) First this operation resides in GPU’s HBM memory, and this part of calculation “torch.pow(x, 3)” is passed to GPU and it performs the operations, one by one, the instructions are sent from HBM to GPU cores and transferred back to HBM one by one. But torch.compiles evaluates that the code is simply operation on input x and some +,* and transfers the code to GPU once and does all the operation and send it back to HBM, in this way it optimizes the training process.\nFlash attention It is somewhat similar to torch.compile’s process but torch.compile itself cannot comprehend our code(shown below) to perform the optimization.\naw = (Q @ torch.transpose(K, -2,-1)) # for matmul dim of q should be B,T,C and k should be B,C,T aw = aw/(K.shape[-1] **0.5) mask = self.tril[:,:,:T,:T] == 0 # generate mask aw = aw.masked_fill_(mask, float('-inf')) # apply mask i.e fill true values with -inf aw = torch.softmax(aw,dim=-1) # -inf values are converted to 0 and then each row is normalized cv = aw @ V # context vector We have to call torch.nn.functional.scaled_dot_product_attention combined with torch.compile to use FlashAttention in our code.\nRemove ugly numbers. Always include numbers in our code that have powers of 2 in it.\nfor instance 16,32,64 work best.\nImprovements\nfor instance, while training GPT-2 our vocab_size is 50257\nif we factorize it has divisors\n1 | 29 | 1733 50257 None of it have powers of 2, so the GPU performs operation on that matrix by truncating til the last powers of 2 and then doing the operation on the remaining parts, which is inefficient. We can simply increase that number to be a closed number that has powers of 2 such as 50304 = 2^7 × 3 x 131 which has high number of power of 2.\nWe can simply increase the training speed by making our numbers in our code have more powers of 2.\n",
  "wordCount" : "435",
  "inLanguage": "en",
  "datePublished": "2025-01-02T00:00:00Z",
  "dateModified": "2025-01-02T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "CohleM"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://cohlem.github.io/sub-notes/training-speed-optimization/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "CohleM",
    "logo": {
      "@type": "ImageObject",
      "url": "https://cohlem.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://cohlem.github.io" accesskey="h" title="CohleM (Alt + H)">CohleM</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://cohlem.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/random/" title="Random">
                    <span>Random</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://cohlem.github.io">Home</a>&nbsp;»&nbsp;<a href="https://cohlem.github.io/sub-notes/">Sub-notes</a></div>
    <h1 class="post-title">
      Training Speed Optimization
    </h1>
    <div class="post-meta"><span title='2025-01-02 00:00:00 +0000 UTC'>January 2, 2025</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;CohleM

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#precision" aria-label="Precision">Precision</a></li>
                <li>
                    <a href="#torchcompile" aria-label="torch.compile">torch.compile</a></li>
                <li>
                    <a href="#flash-attention" aria-label="Flash attention">Flash attention</a></li>
                <li>
                    <a href="#remove-ugly-numbers" aria-label="Remove ugly numbers.">Remove ugly numbers.</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="precision">Precision<a hidden class="anchor" aria-hidden="true" href="#precision">#</a></h3>
<p>The more the precision point the less operation (TFLOPS) is performed.</p>
<ul>
<li>FP64 used for scientific research purposes, where precision is a must.</li>
<li>TF32 and BFLOAT16 are mostly used in NN training.</li>
<li>INT8 is used for inference.</li>
</ul>
<p>Picture below shows specifications of A100 GPU.</p>
<p><img loading="lazy" src="sub-notes/training-speed-optimization/fig1.png" alt="GPU precision"  />
</p>
<p>Using these precision points may have some difference in code.
See pytorch&rsquo;s docs</p>
<h3 id="torchcompile">torch.compile<a hidden class="anchor" aria-hidden="true" href="#torchcompile">#</a></h3>
<p>It works in a similar fashion like the GCC compiler. It works by reducing overheads introduced by the python interpreter and optimizing the GPU read and writes.</p>
<p>For instance</p>
<p><img loading="lazy" src="sub-notes/training-speed-optimization/fig2.png" alt="gpu memory"  />
</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gelu</span>(x):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Applies the GELU activation function to the input.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> x <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> torch<span style="color:#f92672">.</span>tanh(math<span style="color:#f92672">.</span>sqrt(<span style="color:#ae81ff">2</span> <span style="color:#f92672">/</span> math<span style="color:#f92672">.</span>pi) <span style="color:#f92672">*</span> (x <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.044715</span> <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>pow(x, <span style="color:#ae81ff">3</span>))))
</span></span></code></pre></div><p>First this operation resides in GPU&rsquo;s HBM memory, and this part of calculation &ldquo;torch.pow(x, 3)&rdquo; is passed to GPU and it performs the operations, one by one, the instructions are sent from HBM to GPU cores and transferred back to HBM one by one. But torch.compiles evaluates that the code is simply operation on input x and some +,*  and transfers the code to GPU once and does all the operation and send it back to HBM, in this way it optimizes the training process.</p>
<h3 id="flash-attention">Flash attention<a hidden class="anchor" aria-hidden="true" href="#flash-attention">#</a></h3>
<p>It is somewhat similar to torch.compile&rsquo;s process but torch.compile itself cannot comprehend our code(shown below) to perform the optimization.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>aw <span style="color:#f92672">=</span> (Q <span style="color:#f92672">@</span> torch<span style="color:#f92672">.</span>transpose(K, <span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)) <span style="color:#75715e"># for matmul dim of q should be B,T,C and k should be B,C,T</span>
</span></span><span style="display:flex;"><span>aw <span style="color:#f92672">=</span> aw<span style="color:#f92672">/</span>(K<span style="color:#f92672">.</span>shape[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">**</span><span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span>mask <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>tril[:,:,:T,:T] <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> <span style="color:#75715e"># generate mask</span>
</span></span><span style="display:flex;"><span>aw <span style="color:#f92672">=</span> aw<span style="color:#f92672">.</span>masked_fill_(mask, float(<span style="color:#e6db74">&#39;-inf&#39;</span>)) <span style="color:#75715e"># apply mask i.e fill true values with -inf </span>
</span></span><span style="display:flex;"><span>aw <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>softmax(aw,dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># -inf values are converted to 0 and then each row is normalized</span>
</span></span><span style="display:flex;"><span>cv <span style="color:#f92672">=</span> aw <span style="color:#f92672">@</span> V <span style="color:#75715e"># context vector</span>
</span></span></code></pre></div><p>We have to call <strong><code>torch.nn.functional.scaled_dot_product_attention</code></strong> combined with torch.compile to use FlashAttention in our code.</p>
<h3 id="remove-ugly-numbers">Remove ugly numbers.<a hidden class="anchor" aria-hidden="true" href="#remove-ugly-numbers">#</a></h3>
<p>Always include numbers in our code that have powers of 2 in it.</p>
<p>for instance 16,32,64 work best.</p>
<p><strong>Improvements</strong></p>
<p>for instance, while training GPT-2 our vocab_size is 50257</p>
<p>if we factorize it has divisors</p>
<pre tabindex="0"><code>1 | 29 | 1733 50257
</code></pre><p>None of it have powers of 2, so the GPU performs operation on that matrix by truncating til the last powers of 2 and then doing the operation on the remaining parts, which is inefficient. We can simply increase that number to be a closed number that has powers of 2 such as 50304 = 2^7 × 3 x 131 which has high number of power of 2.</p>
<p>We can simply increase the training speed by making our numbers in our code have more powers of 2.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://cohlem.github.io/sub-notes/gradient-accumulation/">
    <span class="title">« Prev</span>
    <br>
    <span>Gradient Accumulation</span>
  </a>
  <a class="next" href="https://cohlem.github.io/sub-notes/skip-connections/">
    <span class="title">Next »</span>
    <br>
    <span>skip-connections</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://cohlem.github.io">CohleM</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
