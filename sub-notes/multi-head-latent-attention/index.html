<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Multi-head latent attention | CohleM</title>
<meta name="keywords" content="">
<meta name="description" content="Scaled-dot product Attention Q1 Given the attention equation $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{(xWq)(xWk)^\top}{\sqrt{d_k}}\right)(xWv)W_O $$ Why don&rsquo;t we train by combining $WqWk^\top$ and $WvWo$? because mathematically they seem equivalent $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{x(WqWk^\top)x^\top}{\sqrt{d_k}}\right)x(WvW_O) $$ I initially thought if we could combine those weights, we don&rsquo;t need to calculate $Q,K,V$ meaning there will be less number of matrix multiplication.
Answer We lose the objective of $Q,K,V,O$, they are meant to operate independently.">
<meta name="author" content="CohleM">
<link rel="canonical" href="https://cohlem.github.io/sub-notes/multi-head-latent-attention/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css" integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe&#43;FVUFzPh7U=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://cohlem.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://cohlem.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://cohlem.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://cohlem.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://cohlem.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script type="text/javascript">
  MathJax = {
    tex: {
      inlineMath: [["\\(", "\\)"], ["$", "$"]],
      displayMath: [["\\[", "\\]"], ["$$", "$$"]]
    },
    options: {
      skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"]
    }
  };
</script>



<script async src="https://www.googletagmanager.com/gtag/js?id=G-X6LV4QY2G2"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-X6LV4QY2G2', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Multi-head latent attention" />
<meta property="og:description" content="Scaled-dot product Attention Q1 Given the attention equation $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{(xWq)(xWk)^\top}{\sqrt{d_k}}\right)(xWv)W_O $$ Why don&rsquo;t we train by combining $WqWk^\top$ and $WvWo$? because mathematically they seem equivalent $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{x(WqWk^\top)x^\top}{\sqrt{d_k}}\right)x(WvW_O) $$ I initially thought if we could combine those weights, we don&rsquo;t need to calculate $Q,K,V$ meaning there will be less number of matrix multiplication.
Answer We lose the objective of $Q,K,V,O$, they are meant to operate independently." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cohlem.github.io/sub-notes/multi-head-latent-attention/" /><meta property="article:section" content="sub-notes" />
<meta property="article:published_time" content="2025-04-28T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-04-28T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Multi-head latent attention"/>
<meta name="twitter:description" content="Scaled-dot product Attention Q1 Given the attention equation $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{(xWq)(xWk)^\top}{\sqrt{d_k}}\right)(xWv)W_O $$ Why don&rsquo;t we train by combining $WqWk^\top$ and $WvWo$? because mathematically they seem equivalent $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{x(WqWk^\top)x^\top}{\sqrt{d_k}}\right)x(WvW_O) $$ I initially thought if we could combine those weights, we don&rsquo;t need to calculate $Q,K,V$ meaning there will be less number of matrix multiplication.
Answer We lose the objective of $Q,K,V,O$, they are meant to operate independently."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Sub-notes",
      "item": "https://cohlem.github.io/sub-notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Multi-head latent attention",
      "item": "https://cohlem.github.io/sub-notes/multi-head-latent-attention/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Multi-head latent attention",
  "name": "Multi-head latent attention",
  "description": "Scaled-dot product Attention Q1 Given the attention equation $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{(xWq)(xWk)^\\top}{\\sqrt{d_k}}\\right)(xWv)W_O $$ Why don\u0026rsquo;t we train by combining $WqWk^\\top$ and $WvWo$? because mathematically they seem equivalent $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{x(WqWk^\\top)x^\\top}{\\sqrt{d_k}}\\right)x(WvW_O) $$ I initially thought if we could combine those weights, we don\u0026rsquo;t need to calculate $Q,K,V$ meaning there will be less number of matrix multiplication.\nAnswer We lose the objective of $Q,K,V,O$, they are meant to operate independently.",
  "keywords": [
    
  ],
  "articleBody": "Scaled-dot product Attention Q1 Given the attention equation $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{(xWq)(xWk)^\\top}{\\sqrt{d_k}}\\right)(xWv)W_O $$ Why don’t we train by combining $WqWk^\\top$ and $WvWo$? because mathematically they seem equivalent $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{x(WqWk^\\top)x^\\top}{\\sqrt{d_k}}\\right)x(WvW_O) $$ I initially thought if we could combine those weights, we don’t need to calculate $Q,K,V$ meaning there will be less number of matrix multiplication.\nAnswer We lose the objective of $Q,K,V,O$, they are meant to operate independently. In analogy, $Q$ represents the notion of “what we have”, $K$ whats available to us and so on. BUT, if we combine them into a giant matrix $WqWk^\\top$ during backpropagation weight updates are mixed. We no longer update them separately so we end up loosing their significance.\nQ2 So, if we can’t use them during training, can we still mix those weights during inference now that we aren’t updating weights? since the mathematical equivalence is the same, can we use it to optimize inference performance?\nAnswer We decrease the number of matrix multiplication, BUT we end up increasing the actual element wise multiplications inside those matrix multiplication.\nWe end up decreasing the speed rather than increasing it.\nLet’s see this by comparison.\nNOTE: Given matrix A with size (2,3) and B with size (3,4), the total number of element wise matrix multiplication between is (2 * 3 * 4).\n$n$ = number of tokens $d_{\\text{model}}$: embedding dimension $nh$: number of heads $hd$: number of head_dimension $d_{\\text{k}}$: $nh$ x $hd$\nCASE I: Original Attention Compute $Q = X W_Q$: $\\mathcal{O}(n \\cdot d_{\\text{model}} \\cdot d_k)$\nCompute $K = X W_K$: $\\mathcal{O}(n \\cdot d_{\\text{model}} \\cdot d_k)$\nCompute $QK^T$: $\\mathcal{O}(n^2 \\cdot d_k)$\nCASE II: Combined Compute $X W_{QK}$: $\\mathcal{O}(n \\cdot d_{\\text{model}}^2)$\nCompute $(X W_{QK}) X^T$: $\\mathcal{O}(n^2 \\cdot d_{\\text{model}})$\nIf $d_k \\ll d_{\\text{model}}$ (e.g., $d_k = 128$, $d_{\\text{model}} = 512$):\nOriginal: $\\mathcal{O}( n \\cdot 512 \\cdot 128)+ \\mathcal{O}( n \\cdot 512 \\cdot 128) + \\mathcal{O}(n^2 \\cdot 128)$\nCombined: $\\mathcal{O}(n \\cdot 512^2) + \\mathcal{O}(n^2 \\cdot 512)$\nAs you can see the number of matrix multiplication is 3, but the total elementwise multiplication is very large.\nMulti-head Latent Attention The main reason behind using the variants of Attention is that we always to increase our inference speed and we are always bottlenecked by KV cache The KV cache needed in original Multi-head attention is $2\\cdot nh\\cdot hd\\cdot l$ for one token, as the tokens get large during inference, the memory needed for storing this case also increases.\nDeepseek propose the use of latent dimension to compress the dimension.\nAs we know $K,V$ both come from the same x i.e $K=xWk$ and $V=xWv$ but the different weights $Wk, Wv$\nhow about we make an intermediate compressed version of x, from which we can decompress it into K and V, and only store that compressed version of x. This is what they use for multi-head latent attention.\n$W_{\\text{dkv}}$: compression matrix of size ($d_{\\text{model}}$, $d_{\\text{c}}$) $L_{\\text{kv}}$= $xW_{\\text{dkv}}$ which is the compressed version of x\nWe decompress $L_{\\text{kv}}$ into K, V using $Wuk, Wuv$ i.e\n$Q=xWq$\n$Kc=L_{\\text{kv}} \\cdot Wuk$ ($Wuk$ size = ($d_{\\text{c}}, nh \\cdot hd$))\n$Vc=L_{\\text{kv}} \\cdot Wuv$ ($Wuv$ size = ($d_{\\text{c}}, nh \\cdot hd$))\nSimilarly, we can substitute those in our original attention equation $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{(Q)(Kc)^\\top}{\\sqrt{d_k}}\\right)(Vc)W_O $$ As you can see, we’ve increased the number of matrix multiplication (i.e the computation of $L_{\\text{kv}}$= $xW_{\\text{dkv}}$), but the total number of elementwise multiplication can be made comparable with the right choice of compression dimension $d_{\\text{c}}$\nBut, our main goal was to reduce the number of KV cache, but if we store only the $L_{\\text{kv}}$ only, we still would would need to perform $Kc=L_{\\text{kv}} \\cdot Wuk$ and $Vc=L_{\\text{kv}} \\cdot Wuv$ to calculate attention. So whats the point of this compression?\nWell there’s a trick to still store only the $L_{\\text{kv}}$ and use it without calculating Kc and Vc, we do weight combination like in our Q2 but still end up with less number of elementwise matrix multiplication. The equation above can also we written as\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{(xWq)(L_{\\text{kv}} \\cdot Wuk)^\\top}{\\sqrt{d_k}}\\right)(L_{\\text{kv}} \\cdot Wuv)W_O $$\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{(x(Wq Wuk^\\top) (L_{\\text{kv}})^\\top}{\\sqrt{d_k}}\\right)L_{\\text{kv}}(WuvW_O) $$\nAfter combining $(Wq Wuk^\\top)$ and $(WuvW_O)$ once, we can save $L_{\\text{kv}}$ in our cache and then directly multiply with $(Wq Wuk^\\top)$ to get the attention, without needing to calculate $Kc$ and $Vc$. Remember the issue we had while combining weights in Q2, this fades away because of the compression dimension because it strictly has to be less than $nh \\cdot hd$ i.e ($d_{\\text{c}} \\ll nh \\cdot hd$)\nDecoupled RoPE There are still some parts I feel like I don’t understand completely. But, here’s what I’ve understood till now.\nFirst thing to keep in mind, its the clever weight absorption design and caching only $L_{\\text{kv}}$ that helps MLA to retain its fast speed. But, we have yet to apply positional information to our Q and K i.e\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{R1(xWq)(R2(L_{\\text{kv}} \\cdot Wuk))^\\top}{\\sqrt{d_k}}\\right)(L_{\\text{kv}} \\cdot Wuv)W_O $$\nIn the paper, they say the RoPE matrix gets in the middle but I don’t know what i am missing here cause $({R1(xWq)Wuk^\\top)L_{\\text{kv}}^\\top R2^\\top}$ so weights can still be combined? I think there something that I don’t understand here. I’ll correct my understand in future.\nlets resume from the point that RoPE matrix gets in the middle. They again design a clever thing here. Add two new matrices $W_{QR}\\in \\mathbb{R}^{(d,nh \\cdot d^R)}$ and $W_{KR} \\in \\mathbb{R}^{(d,d^R)}$\n$Q^{R}=RoPE(xW_{QR})$ and $K^{R}=RoPE(xW_{KR})$\nand also cache the $K^R$\nadd then concatenate the new two matrices to the original Q and K\n$Q = [Q, Q^R]$\n$K = [K, K^R]$\nand then perform our original attention. $$ Q \\cdot K^\\top = [Q, Q^R] \\cdot [K, K^R]^\\top $$\n$$ Q \\cdot K^\\top = [Q \\cdot K + Q^R \\cdot K^R] $$\nas you can see the original Q and K are still preserved meaning we can still absorb the weights.\nTotal cached elements will be $K^R$ and $L_{kv}$ so the total saved cache will be $(d^R + dc)l$\nQuestion Why is second dimension of $W_{KR}$ is $d^R$ and not $(nh \\cdot d^R)$ Meaning $d^R$ will be broadcasted across all the heads.\nMy guess is that they found that keeping only $d^R$ would produce decent result and would also save the cache memory requirement.\nReferences https://arxiv.org/abs/2405.04434 https://liorsinai.github.io/machine-learning/2025/02/22/mla.html#multi-head-latent-attention\n",
  "wordCount" : "1037",
  "inLanguage": "en",
  "datePublished": "2025-04-28T00:00:00Z",
  "dateModified": "2025-04-28T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "CohleM"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://cohlem.github.io/sub-notes/multi-head-latent-attention/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "CohleM",
    "logo": {
      "@type": "ImageObject",
      "url": "https://cohlem.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://cohlem.github.io" accesskey="h" title="CohleM (Alt + H)">CohleM</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://cohlem.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/random/" title="Random">
                    <span>Random</span>
                </a>
            </li>
            <li>
                <a href="https://cohlem.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://cohlem.github.io">Home</a>&nbsp;»&nbsp;<a href="https://cohlem.github.io/sub-notes/">Sub-notes</a></div>
    <h1 class="post-title">
      Multi-head latent attention
    </h1>
    <div class="post-meta"><span title='2025-04-28 00:00:00 +0000 UTC'>April 28, 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;CohleM

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#scaled-dot-product-attention" aria-label="Scaled-dot product Attention">Scaled-dot product Attention</a><ul>
                        
                <li>
                    <a href="#q1" aria-label="Q1">Q1</a><ul>
                        
                <li>
                    <a href="#answer" aria-label="Answer">Answer</a></li></ul>
                </li>
                <li>
                    <a href="#q2" aria-label="Q2">Q2</a><ul>
                        
                <li>
                    <a href="#answer-1" aria-label="Answer">Answer</a></li>
                <li>
                    <a href="#case-i-original-attention" aria-label="CASE I: Original Attention">CASE I: Original Attention</a></li>
                <li>
                    <a href="#case-ii-combined" aria-label="CASE II: Combined">CASE II: Combined</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#multi-head-latent-attention" aria-label="Multi-head Latent Attention">Multi-head Latent Attention</a><ul>
                        
                <li>
                    <a href="#decoupled-rope" aria-label="Decoupled RoPE">Decoupled RoPE</a><ul>
                        
                <li>
                    <a href="#question" aria-label="Question">Question</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
            </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="scaled-dot-product-attention">Scaled-dot product Attention<a hidden class="anchor" aria-hidden="true" href="#scaled-dot-product-attention">#</a></h2>
<h3 id="q1">Q1<a hidden class="anchor" aria-hidden="true" href="#q1">#</a></h3>
<p>Given the attention equation
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{(xWq)(xWk)^\top}{\sqrt{d_k}}\right)(xWv)W_O
$$
Why don&rsquo;t we train by combining $WqWk^\top$ and $WvWo$? because mathematically they seem equivalent
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{x(WqWk^\top)x^\top}{\sqrt{d_k}}\right)x(WvW_O)
$$
I initially thought if we could combine those weights, we don&rsquo;t need to calculate $Q,K,V$ meaning there will be less number of matrix multiplication.</p>
<h4 id="answer">Answer<a hidden class="anchor" aria-hidden="true" href="#answer">#</a></h4>
<p>We lose the objective of $Q,K,V,O$, they are meant to operate independently. In analogy,
$Q$ represents the notion of &ldquo;what we have&rdquo;, $K$ whats available to us and so on. BUT, if we combine them into a giant matrix $WqWk^\top$ during backpropagation weight updates are mixed. We no longer update them separately so we end up loosing their significance.</p>
<h3 id="q2">Q2<a hidden class="anchor" aria-hidden="true" href="#q2">#</a></h3>
<p>So, if we can&rsquo;t use them during training, can we still mix those weights during inference now that we aren&rsquo;t updating weights? since the mathematical equivalence is the same, can we use it to optimize inference performance?</p>
<h4 id="answer-1">Answer<a hidden class="anchor" aria-hidden="true" href="#answer-1">#</a></h4>
<p>We decrease the number of matrix multiplication, BUT we end up increasing the actual element wise multiplications inside those matrix multiplication.</p>
<p>We end up decreasing the speed rather than increasing it.</p>
<p>Let&rsquo;s see this by comparison.</p>
<p><strong>NOTE: Given matrix A with size (2,3) and B with size (3,4), the total number of element wise matrix multiplication between is (2 * 3 * 4).</strong></p>
<p>$n$ = number of tokens
$d_{\text{model}}$: embedding dimension
$nh$: number of heads
$hd$: number of head_dimension
$d_{\text{k}}$: $nh$ x $hd$</p>
<h4 id="case-i-original-attention">CASE I: Original Attention<a hidden class="anchor" aria-hidden="true" href="#case-i-original-attention">#</a></h4>
<p>Compute $Q = X W_Q$: $\mathcal{O}(n \cdot d_{\text{model}} \cdot d_k)$</p>
<p>Compute $K = X W_K$: $\mathcal{O}(n \cdot d_{\text{model}} \cdot d_k)$</p>
<p>Compute $QK^T$: $\mathcal{O}(n^2 \cdot d_k)$</p>
<h4 id="case-ii-combined">CASE II: Combined<a hidden class="anchor" aria-hidden="true" href="#case-ii-combined">#</a></h4>
<p>Compute $X W_{QK}$: $\mathcal{O}(n \cdot d_{\text{model}}^2)$</p>
<p>Compute $(X W_{QK}) X^T$: $\mathcal{O}(n^2 \cdot d_{\text{model}})$</p>
<p>If $d_k \ll d_{\text{model}}$ (e.g., $d_k = 128$, $d_{\text{model}} = 512$):</p>
<p>Original: $\mathcal{O}( n \cdot 512 \cdot 128)+ \mathcal{O}( n \cdot 512 \cdot 128) + \mathcal{O}(n^2 \cdot 128)$</p>
<p>Combined: $\mathcal{O}(n \cdot 512^2) + \mathcal{O}(n^2 \cdot 512)$</p>
<p>As you can see the number of matrix multiplication is 3, but the total elementwise multiplication is very large.</p>
<h2 id="multi-head-latent-attention">Multi-head Latent Attention<a hidden class="anchor" aria-hidden="true" href="#multi-head-latent-attention">#</a></h2>
<p>The main reason behind using the variants of <strong>Attention</strong> is that we always to increase our inference speed and we are always bottlenecked by <a href="https://cohlem.github.io/sub-notes/kv-cache-gqa/">KV cache</a> The KV cache needed in original Multi-head attention is   $2\cdot nh\cdot hd\cdot l$
for one token, as the tokens get large during inference, the memory needed for storing this case also increases.</p>
<p>Deepseek propose the use of latent dimension to compress the dimension.</p>
<p>As we know $K,V$ both come from the same x i.e $K=xWk$ and $V=xWv$ but the different weights $Wk, Wv$</p>
<p>how about we make an intermediate compressed version of x, from which we can decompress it into K and V, and only store that compressed version of x. This is what they use for multi-head latent attention.</p>
<p>$W_{\text{dkv}}$: compression matrix of size ($d_{\text{model}}$, $d_{\text{c}}$)
$L_{\text{kv}}$= $xW_{\text{dkv}}$ which is the compressed version of x</p>
<p>We decompress $L_{\text{kv}}$ into K, V using $Wuk, Wuv$ i.e</p>
<p>$Q=xWq$</p>
<p>$Kc=L_{\text{kv}} \cdot Wuk$ ($Wuk$ size = ($d_{\text{c}}, nh \cdot hd$))</p>
<p>$Vc=L_{\text{kv}} \cdot Wuv$ ($Wuv$ size = ($d_{\text{c}}, nh \cdot hd$))</p>
<p>Similarly, we can  substitute those in our original attention equation
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{(Q)(Kc)^\top}{\sqrt{d_k}}\right)(Vc)W_O
$$
As you can see, we&rsquo;ve increased the number of matrix multiplication (i.e the computation of $L_{\text{kv}}$= $xW_{\text{dkv}}$), but the total number of elementwise multiplication can be made comparable with the right choice of compression dimension $d_{\text{c}}$</p>
<p>But, our main goal was to reduce the number of KV cache, but if we store only the $L_{\text{kv}}$ only, we still would would need to perform $Kc=L_{\text{kv}} \cdot Wuk$ and $Vc=L_{\text{kv}} \cdot Wuv$  to calculate attention. So whats the point of this compression?</p>
<p>Well there&rsquo;s a trick to still store only the $L_{\text{kv}}$ and use it without calculating Kc and Vc, we do weight combination like in our Q2 but still end up with less number of elementwise matrix multiplication. The equation above can also we written as</p>
<p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{(xWq)(L_{\text{kv}} \cdot Wuk)^\top}{\sqrt{d_k}}\right)(L_{\text{kv}} \cdot Wuv)W_O
$$</p>
<p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{(x(Wq Wuk^\top) (L_{\text{kv}})^\top}{\sqrt{d_k}}\right)L_{\text{kv}}(WuvW_O)
$$</p>
<p>After combining $(Wq Wuk^\top)$ and $(WuvW_O)$ once, we can save $L_{\text{kv}}$ in our cache and then directly multiply with $(Wq Wuk^\top)$ to get the attention, without needing to calculate $Kc$ and $Vc$. Remember the issue we had while combining weights in Q2, this fades away because of the compression dimension because it strictly has to be less than $nh \cdot hd$  i.e ($d_{\text{c}} \ll nh \cdot hd$)</p>
<h3 id="decoupled-rope">Decoupled RoPE<a hidden class="anchor" aria-hidden="true" href="#decoupled-rope">#</a></h3>
<p>There are still some parts I feel like I don&rsquo;t understand completely. But, here&rsquo;s what I&rsquo;ve understood till now.</p>
<p>First thing to keep in mind, its the clever weight absorption design and caching only $L_{\text{kv}}$ that helps MLA to retain its fast speed. But, we have yet to apply positional information to our Q and K i.e</p>
<p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{R1(xWq)(R2(L_{\text{kv}} \cdot Wuk))^\top}{\sqrt{d_k}}\right)(L_{\text{kv}} \cdot Wuv)W_O
$$</p>
<p>In the paper, they say the RoPE matrix gets in the middle but I don&rsquo;t know what i am missing here cause $({R1(xWq)Wuk^\top)L_{\text{kv}}^\top R2^\top}$ so weights can still be combined? I think there something that I don&rsquo;t understand here. I&rsquo;ll correct my understand in future.</p>
<p>lets resume from the point that RoPE matrix gets in the middle. They again design a clever thing here. Add two new matrices $W_{QR}\in \mathbb{R}^{(d,nh \cdot d^R)}$  and $W_{KR} \in \mathbb{R}^{(d,d^R)}$</p>
<p>$Q^{R}=RoPE(xW_{QR})$ and $K^{R}=RoPE(xW_{KR})$</p>
<p>and also cache the $K^R$</p>
<p>add then concatenate the new two matrices to the original Q and K</p>
<p>$Q = [Q, Q^R]$</p>
<p>$K = [K, K^R]$</p>
<p>and then perform our original attention.
$$
Q \cdot K^\top = [Q, Q^R] \cdot [K, K^R]^\top
$$</p>
<p>$$
Q \cdot K^\top = [Q \cdot K + Q^R \cdot K^R]
$$</p>
<p>as you can see the original Q and K are still preserved meaning we can still absorb the weights.</p>
<p>Total cached elements will be $K^R$ and $L_{kv}$ so the total saved cache will be $(d^R + dc)l$</p>
<h4 id="question">Question<a hidden class="anchor" aria-hidden="true" href="#question">#</a></h4>
<p>Why is second dimension of $W_{KR}$ is $d^R$ and not $(nh \cdot d^R)$
Meaning $d^R$ will be broadcasted across all the heads.</p>
<p>My guess is that they found that keeping only $d^R$ would produce decent result and would also save the cache memory requirement.</p>
<h4 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h4>
<p><a href="https://arxiv.org/abs/2405.04434">https://arxiv.org/abs/2405.04434</a>
<a href="https://liorsinai.github.io/machine-learning/2025/02/22/mla.html#multi-head-latent-attention">https://liorsinai.github.io/machine-learning/2025/02/22/mla.html#multi-head-latent-attention</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://cohlem.github.io/sub-notes/mixture-of-experts/">
    <span class="title">« Prev</span>
    <br>
    <span>Mixture of Experts</span>
  </a>
  <a class="next" href="https://cohlem.github.io/sub-notes/lora/">
    <span class="title">Next »</span>
    <br>
    <span>LoRA</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://cohlem.github.io">CohleM</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
